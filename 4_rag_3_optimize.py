# ==============================================================================
# è¯´æ˜ï¼šæœ¬æ–‡ä»¶ç”¨äºæ¼”ç¤º RAG ç³»ç»Ÿçš„å…¨é“¾è·¯æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
# ==============================================================================
# ä»æ•°æ®å‡†å¤‡åˆ°ç”Ÿæˆè¾“å‡ºï¼Œç³»ç»ŸåŒ–åœ°å±•ç¤º RAG ä¼˜åŒ–çš„ 8 ä¸ªå…³é”®ç¯èŠ‚ã€‚
#
# ã€æ ¸å¿ƒç›®æ ‡ã€‘
# é€šè¿‡å…·ä½“ä¼˜åŒ–æ‰‹æ®µï¼Œè§£å†³ RAG ç³»ç»Ÿä¸­å¸¸è§çš„"æ£€ç´¢ä¸å‡†"ã€"å›ç­”ä¸ä½³"ç­‰é—®é¢˜ï¼Œ
# å¹¶ä½¿ç”¨ Ragas è¿›è¡Œé‡åŒ–éªŒè¯ã€‚
#
# ã€ä¼˜åŒ–è·¯çº¿å›¾ã€‘
# 1. æ•°æ®å±‚ï¼šæ‰©å¤§ Top-Kã€é‡å»ºç´¢å¼•ï¼ˆStep 1-2ï¼‰
# 2. è§£æå±‚ï¼šPDF â†’ Markdown ç»“æ„åŒ–ï¼ˆStep 3ï¼‰
# 3. åˆ‡ç‰‡å±‚ï¼š5 ç§åˆ‡ç‰‡ç­–ç•¥å¯¹æ¯”ï¼ˆStep 4ï¼‰
# 4. åµŒå…¥å±‚ï¼šEmbedding æ¨¡å‹å¯¹æ¯”ï¼ˆStep 5ï¼‰
# 5. å­˜å‚¨å±‚ï¼šå‘é‡æ•°æ®åº“é€‰å‹ï¼ˆStep 6ï¼Œç†è®ºï¼‰
# 6. æ£€ç´¢å±‚ï¼šé—®é¢˜æ”¹å†™ã€å¤šæ­¥æŸ¥è¯¢ã€HyDEã€Rerankï¼ˆStep 7ï¼‰
# 7. ç”Ÿæˆå±‚ï¼šTemperatureã€Penaltyã€Seed è°ƒä¼˜ï¼ˆStep 8ï¼‰
#
# ã€å…³é”®åŸç†è¯´æ˜ã€‘
# 
# 1. æ–‡æ¡£å‡†å¤‡é˜¶æ®µåŸç†
#    - æ„å›¾ç©ºé—´ï¼ˆç”¨æˆ·é—®çš„é—®é¢˜ï¼‰vs çŸ¥è¯†ç©ºé—´ï¼ˆå·²æœ‰æ–‡æ¡£å†…å®¹ï¼‰
#    - é‡å éƒ¨åˆ†ï¼šå¯ä»¥ä¾é çŸ¥è¯†åº“å›ç­”é—®é¢˜ï¼Œéœ€æŒç»­æå‡å†…å®¹è´¨é‡
#    - æœªè¦†ç›–çš„æ„å›¾ç©ºé—´ï¼šå¹»è§‰å±…å¤šï¼Œéœ€è¦è¡¥å……ç¼ºæ¼çŸ¥è¯†
#    - æœªè¢«åˆ©ç”¨çš„çŸ¥è¯†ç©ºé—´ï¼šå¯èƒ½äº§ç”Ÿæ£€ç´¢å¹²æ‰°ï¼Œéœ€ä¼˜åŒ–å¬å›ç®—æ³•æˆ–å‰”é™¤å™ªéŸ³
#
# 2. æ–‡æ¡£è§£æä¸åˆ‡ç‰‡åŸç†
#    - æ–‡æ¡£è§£æï¼šä¼ ç»Ÿ PDF è§£æå¸¸ä¸¢å¤±è¡¨æ ¼ã€æ ‡é¢˜å±‚çº§ã€‚ä½¿ç”¨ PyMuPDF4LLM æˆ– 
#      DashScopeParse è½¬ä¸º Markdown å¯ä¿ç•™ç»“æ„
#    - æ–‡æ¡£åˆ‡ç‰‡ï¼šä¸»é¢˜æ¥è¿‘çš„å†…å®¹åº”èšåˆã€‚Token åˆ‡ç‰‡å®¹æ˜“åˆ‡æ–­è¯­ä¹‰ï¼›Markdown åˆ‡ç‰‡
#      èƒ½æŒ‰ç« èŠ‚ä¿æŒè¯­ä¹‰å®Œæ•´ï¼›Semantic åˆ‡ç‰‡èƒ½æ™ºèƒ½æ£€æµ‹è¯­ä¹‰è¾¹ç•Œ
#
# 3. åˆ‡ç‰‡ç­–ç•¥å¯¹æ¯”
#    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#    â”‚ åˆ‡ç‰‡å™¨             â”‚ åˆ‡åˆ†ä¾æ®     â”‚ ä¼˜åŠ¿         â”‚ åŠ£åŠ¿         â”‚
#    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
#    â”‚ TokenTextSplitter  â”‚ Token æ•°é‡   â”‚ ç®€å•å¿«é€Ÿ     â”‚ æ˜“åˆ‡æ–­è¯­ä¹‰   â”‚
#    â”‚ SentenceSplitter   â”‚ å¥å­è¾¹ç•Œ     â”‚ ä¿æŒå¥å®Œæ•´   â”‚ å¿½ç•¥ç« èŠ‚ç»“æ„ â”‚
#    â”‚ MarkdownNodeParser â”‚ Markdownç»“æ„ â”‚ ä¿ç•™å±‚çº§å…³ç³» â”‚ ä¾èµ–æ ¼å¼è´¨é‡ â”‚
#    â”‚ SentenceWindow     â”‚ æ»‘åŠ¨çª—å£     â”‚ ä¸Šä¸‹æ–‡ä¸°å¯Œ   â”‚ å†—ä½™ä¿¡æ¯å¤š   â”‚
#    â”‚ SemanticSplitter   â”‚ è¯­ä¹‰ç›¸ä¼¼åº¦   â”‚ æ™ºèƒ½è¾¹ç•Œæ£€æµ‹ â”‚ è®¡ç®—æˆæœ¬é«˜   â”‚
#    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# 4. å‘é‡æ•°æ®åº“é€‰å‹åŸç†
#    - æ ¸å¿ƒé€»è¾‘ï¼šå‘é‡æ•°æ®åº“æ˜¯ RAG ç³»ç»Ÿçš„"è®°å¿†ä¸­æ¢"ã€‚ä¸åŒæ•°æ®åº“åœ¨"å­˜å‚¨æ•ˆç‡"ã€
#      "æŸ¥è¯¢æ€§èƒ½"ã€"æ‰©å±•æ€§"ç­‰æ–¹é¢å„æœ‰ä¼˜åŠ£
#    - æœ¬ä¾‹ä½¿ç”¨ LlamaIndex é»˜è®¤çš„å†…å­˜ VectorStoreï¼Œå®é™…ç”Ÿäº§ä¸­åº”æ ¹æ®è´Ÿè½½é€‰æ‹©
#      Milvus/DashVector/Pgvector
#
# 5. æ£€ç´¢å±‚ä¼˜åŒ–åŸç†
#    - æ£€ç´¢å‰ï¼šé€šè¿‡é—®é¢˜æ”¹å†™ã€å¤šæ­¥æŸ¥è¯¢ã€HyDE ç­‰æ‰‹æ®µæå‡å¬å›ç‡
#    - æ£€ç´¢ä¸­ï¼šé€šè¿‡æ ‡ç­¾å¢å¼ºã€æ··åˆæ£€ç´¢ç­‰æ‰‹æ®µæå‡å‡†ç¡®ç‡
#    - æ£€ç´¢åï¼šé€šè¿‡é‡æ’åºï¼ˆRerankï¼‰æå‡ Top-N è´¨é‡
#    - Rerank åŸç†ï¼šå‘é‡æ£€ç´¢ï¼ˆBi-Encoderï¼‰é€Ÿåº¦å¿«ä½†ç²¾åº¦ä¸€èˆ¬ï¼Œé€‚åˆç²—æ’ï¼›
#      é‡æ’åºï¼ˆCross-Encoderï¼‰è®¡ç®—é‡å¤§ä½†ç²¾åº¦æé«˜ï¼Œé€‚åˆç²¾æ’
#
# 6. ç”Ÿæˆå±‚ä¼˜åŒ–åŸç†
#    - Temperatureï¼ˆæ¸©åº¦ï¼‰ï¼šæ§åˆ¶è¾“å‡ºéšæœºæ€§ã€‚ä½æ¸©ï¼ˆ0.1ï¼‰é€‚åˆäº‹å®é—®ç­”ï¼Œ
#      é«˜æ¸©ï¼ˆ0.7+ï¼‰é€‚åˆåˆ›æ„ç”Ÿæˆ
#    - Presence Penaltyï¼ˆå­˜åœ¨æƒ©ç½šï¼‰ï¼šæ­£å€¼é¼“åŠ±æ¨¡å‹è°ˆè®ºæ–°è¯é¢˜ï¼Œé˜²æ­¢é‡å¤
#    - Seedï¼ˆéšæœºç§å­ï¼‰ï¼šå›ºå®š Seed å¯è®©æ¨¡å‹è¾“å‡ºå…·æœ‰å¯å¤ç°æ€§ï¼ˆDeterministicï¼‰
# ==============================================================================

from tqdm.cli import tqdm as tqdm_cli
import tqdm.auto
tqdm.auto.tqdm = tqdm_cli

# å¯¼å…¥æ‰€éœ€çš„ä¾èµ–åŒ…
import os
import logging
import pandas as pd
import numpy as np
from openai import OpenAI
from IPython.display import display

# LlamaIndex Core
from llama_index.core import (
    Settings, 
    SimpleDirectoryReader, 
    VectorStoreIndex, 
    Document, 
    PromptTemplate
)
from llama_index.core.node_parser import (
    SentenceSplitter,
    MarkdownNodeParser,
    TokenTextSplitter,
    SentenceWindowNodeParser,
    SemanticSplitterNodeParser
)
from llama_index.core.postprocessor import MetadataReplacementPostProcessor, SimilarityPostprocessor
from llama_index.core.indices.query.query_transform.base import StepDecomposeQueryTransform, HyDEQueryTransform
from llama_index.core.query_engine import MultiStepQueryEngine, TransformQueryEngine

# LlamaIndex Integrations
from llama_index.embeddings.dashscope import DashScopeEmbedding, DashScopeTextEmbeddingModels
from llama_index.llms.openai_like import OpenAILike
from llama_index.postprocessor.dashscope_rerank import DashScopeRerank

# Evaluation (æ ¸å¿ƒè¯„ä¼°åŠŸèƒ½å·²ç§»è‡³ chatbot/eval.py)

# Local Utils
from config.load_key import load_key
from chatbot import rag
from chatbot.eval import evaluate_result, show_evaluation_result
from chatbot.document import file_to_md_local, md_polisher
from chatbot.utils import cosine_similarity, compare_embeddings

# è®¾ç½® pandas æ˜¾ç¤ºé€‰é¡¹
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 2000)

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(level=logging.ERROR)
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

# åŠ è½½APIå¯†é’¥
load_key()
print(f'''ä½ é…ç½®çš„ API Key æ˜¯ï¼š{os.environ["DASHSCOPE_API_KEY"][:5]+"*"*5}''')

# é…ç½® LlamaIndex
Settings.llm = OpenAILike(
    model="qwen-plus",
    api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    is_chat_model=True
)

Settings.embed_model = DashScopeEmbedding(
    model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V3,
    embed_batch_size=6,
    embed_input_length=8192
)


# ==============================================================================
#                               å·¥å…·å‡½æ•°å®šä¹‰åŒº
# ==============================================================================

def ask(question, query_engine):
    """
    æ‰§è¡Œé—®ç­”å¹¶æ‰“å°ç»“æœã€‚
    
    ã€åŠŸèƒ½ã€‘
    1. å°è¯•æ›´æ–° Prompt æ¨¡æ¿ï¼ˆå¦‚æœæ”¯æŒï¼‰
    2. æ‰§è¡ŒæŸ¥è¯¢å¹¶æµå¼æ‰“å°å›ç­”
    3. å±•ç¤ºæ£€ç´¢åˆ°çš„å‚è€ƒæ–‡æ¡£åŠå…¶å¾—åˆ†
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        query_engine: LlamaIndex æŸ¥è¯¢å¼•æ“
        
    Returns:
        response: æŸ¥è¯¢å“åº”å¯¹è±¡
    """
    # å°è¯•æ›´æ–° prompt templateï¼Œå¦‚æœä¸é€‚ç”¨åˆ™å¿½ç•¥
    try:
        rag.update_prompt_template(query_engine=query_engine)
    except:
        pass

    print('=' * 50)
    print(f'ğŸ¤” é—®é¢˜ï¼š{question}')
    print('=' * 50 + '\n')

    response = query_engine.query(question)

    print('ğŸ¤– å›ç­”ï¼š')
    if hasattr(response, 'print_response_stream') and callable(response.print_response_stream):
        response.print_response_stream()
    else:
        print(str(response))

    print('\n' + '-' * 50)
    print('ğŸ“š å‚è€ƒæ–‡æ¡£ (Top æ£€ç´¢ç»“æœ)ï¼š\n')
    if hasattr(response, 'source_nodes'):
        for i, source_node in enumerate(response.source_nodes, start=1):
            score_str = f"{source_node.score:.4f}" if source_node.score is not None else "N/A"
            print(f'æ–‡æ¡£ {i} (Score: {score_str}):')
            # æˆªå–å‰200å­—ç¬¦é¿å…åˆ·å±
            content_preview = source_node.get_content()[:200].replace('\n', ' ')
            print(f"{content_preview}...")
            print()
    print('-' * 50)

    return response


# evaluate_result, show_evaluation_result: å·²ç§»è‡³ chatbot/eval.py
# file_to_md_local, md_polisher: å·²ç§»è‡³ chatbot/document.py


def evaluate_splitter(splitter, documents, question, ground_truth, splitter_name, node_postprocessors=None):
    """
    è¯„æµ‹ä¸åŒæ–‡æ¡£åˆ‡ç‰‡æ–¹æ³•çš„æ•ˆæœã€‚
    
    ã€è¯„æµ‹æµç¨‹ã€‘
    1. ä½¿ç”¨æŒ‡å®šåˆ‡ç‰‡å™¨å¤„ç†æ–‡æ¡£
    2. æ„å»ºå‘é‡ç´¢å¼•
    3. æ‰§è¡ŒæŸ¥è¯¢å¹¶æ‰“å°å›ç­”
    4. ä½¿ç”¨ Ragas è¯„ä¼°è´¨é‡
    
    Args:
        splitter: åˆ‡ç‰‡å™¨å¯¹è±¡
        documents: æ–‡æ¡£åˆ—è¡¨
        question: æµ‹è¯•é—®é¢˜
        ground_truth: æ ‡å‡†ç­”æ¡ˆ
        splitter_name: åˆ‡ç‰‡å™¨åç§°ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
        node_postprocessors: åå¤„ç†å™¨åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
    """
    print(f"\n{'='*50}")
    print(f"ğŸ” æ­£åœ¨ä½¿ç”¨ {splitter_name} æ–¹æ³•è¿›è¡Œåˆ‡ç‰‡æµ‹è¯•...")
    print(f"{'='*50}\n")

    print("ğŸ“‘ æ­£åœ¨å¤„ç†æ–‡æ¡£ä¸æ„å»ºç´¢å¼•...")
    nodes = splitter.get_nodes_from_documents(documents)
    index = VectorStoreIndex(nodes, embed_model=Settings.embed_model)

    query_engine = index.as_query_engine(
        similarity_top_k=5,
        streaming=True,
        node_postprocessors=node_postprocessors 
    )

    response = query_engine.query(question)
    
    print(f"\nğŸ¤– {splitter_name} æ¨¡å‹å›ç­”:")
    response.print_response_stream()
    
    show_evaluation_result(evaluate_result(question, response, ground_truth), f"{splitter_name} è¯„ä¼°ç»“æœ")


# cosine_similarity, compare_embeddings: å·²ç§»è‡³ chatbot/utils.py


def compare_embedding_models(documents, question, ground_truth, sentence_splitter):
    """
    æ¯”è¾ƒä¸åŒåµŒå…¥æ¨¡å‹åœ¨ RAG ä¸­çš„è¡¨ç°ã€‚
    
    ã€å¯¹æ¯”ç»´åº¦ã€‘
    - å¬å›çš„æ–‡æ¡£ç‰‡æ®µ
    - ç”Ÿæˆçš„å›ç­”è´¨é‡
    - Ragas è¯„ä¼°å¾—åˆ†
    
    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        question: æµ‹è¯•é—®é¢˜
        ground_truth: æ ‡å‡†ç­”æ¡ˆ
        sentence_splitter: åˆ‡ç‰‡å™¨
    """
    print("ğŸ“‘ æ­£åœ¨å¤„ç†æ–‡æ¡£...")
    nodes = sentence_splitter.get_nodes_from_documents(documents)

    embedding_models = {
        "text-embedding-v2": DashScopeEmbedding(model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2),
        "text-embedding-v3": DashScopeEmbedding(model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V3)
    }

    for model_name, embed_model in embedding_models.items():
        print(f"\n{'='*50}")
        print(f"ğŸ” æ­£åœ¨æµ‹è¯• {model_name}...")
        print(f"{'='*50}")

        index = VectorStoreIndex(nodes, embed_model=embed_model)
        query_engine = index.as_query_engine(streaming=True, similarity_top_k=5)

        print(f"\nâ“ æµ‹è¯•é—®é¢˜: {question}")
        print("\nğŸ¤– æ¨¡å‹å›ç­”:")
        response = query_engine.query(question)
        response.print_response_stream()

        print(f"\nğŸ“š å¬å›çš„å‚è€ƒç‰‡æ®µ:")
        for i, node in enumerate(response.source_nodes, 1):
            print(f"\næ–‡æ¡£ç‰‡æ®µ {i}:")
            print("-" * 40)
            print(node)

        show_evaluation_result(evaluate_result(question, response, ground_truth), f"{model_name} è¯„ä¼°ç»“æœ")


# --- Step 7 ä¸“ç”¨å·¥å…·å‡½æ•° ---

query_gen_str = """\
ç³»ç»Ÿè§’è‰²è®¾å®š:
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®é¢˜æ”¹å†™åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„åŸå§‹é—®é¢˜æ‰©å……ä¸ºä¸€ä¸ªæ›´å®Œæ•´ã€æ›´å…¨é¢çš„é—®é¢˜ã€‚

è§„åˆ™ï¼š
1. å°†å¯èƒ½çš„æ­§ä¹‰ã€ç›¸å…³æ¦‚å¿µå’Œä¸Šä¸‹æ–‡ä¿¡æ¯æ•´åˆåˆ°ä¸€ä¸ªå®Œæ•´çš„é—®é¢˜ä¸­
2. ä½¿ç”¨æ‹¬å·å¯¹æ­§ä¹‰æ¦‚å¿µè¿›è¡Œè¡¥å……è¯´æ˜
3. æ·»åŠ å…³é”®çš„é™å®šè¯å’Œä¿®é¥°è¯­
4. ç¡®ä¿æ”¹å†™åçš„é—®é¢˜æ¸…æ™°ä¸”è¯­ä¹‰å®Œæ•´
5. å¯¹äºæ¨¡ç³Šæ¦‚å¿µï¼Œåœ¨æ‹¬å·ä¸­åˆ—ä¸¾ä¸»è¦å¯èƒ½æ€§

åŸå§‹é—®é¢˜:
{query}

è¯·ç”Ÿæˆä¸€ä¸ªç»¼åˆçš„æ”¹å†™é—®é¢˜ï¼Œç¡®ä¿ï¼š
- åŒ…å«åŸå§‹é—®é¢˜çš„æ ¸å¿ƒæ„å›¾
- æ¶µç›–å¯èƒ½çš„æ­§ä¹‰è§£é‡Š
- ä½¿ç”¨æ¸…æ™°çš„é€»è¾‘å…³ç³»è¯è¿æ¥ä¸åŒæ–¹é¢
- å¿…è¦æ—¶ä½¿ç”¨æ‹¬å·è¡¥å……è¯´æ˜

è¾“å‡ºæ ¼å¼ï¼š
[ç»¼åˆæ”¹å†™] - æ”¹å†™åçš„é—®é¢˜
"""
query_gen_prompt = PromptTemplate(query_gen_str)


def generate_queries(query: str):
    """
    ä½¿ç”¨ LLM æ‰©å†™é—®é¢˜ã€‚
    
    ã€åº”ç”¨åœºæ™¯ã€‘
    ç”¨æˆ·é—®é¢˜å¾€å¾€è¿‡äºç®€çŸ­æˆ–æ¨¡ç³Šï¼Œé€šè¿‡ LLM æ‰©å†™å¯ä»¥ï¼š
    - è¡¥å……ä¸Šä¸‹æ–‡ä¿¡æ¯
    - æ¾„æ¸…æ­§ä¹‰
    - æå‡æ£€ç´¢å¬å›ç‡
    
    Args:
        query: åŸå§‹é—®é¢˜
        
    Returns:
        str: æ‰©å†™åçš„é—®é¢˜
    """
    response = Settings.llm.predict(
        query_gen_prompt, query=query
    )
    return response


def extract_tags(text):
    """
    æå–æ–‡æœ¬æ ‡ç­¾ï¼ˆä½¿ç”¨ OpenAI Compatible APIï¼‰ã€‚
    
    ã€æ”¯æŒçš„æ ‡ç­¾ç±»å‹ã€‘
    - äººå
    - éƒ¨é—¨åç§°
    - èŒä½åç§°
    - æŠ€æœ¯é¢†åŸŸ
    - äº§å“åç§°
    
    ã€åº”ç”¨åœºæ™¯ã€‘
    åœ¨æ··åˆæ£€ç´¢ä¸­ï¼Œæ ‡ç­¾å¯ä»¥ä½œä¸ºç²¾ç¡®åŒ¹é…çš„è¿‡æ»¤æ¡ä»¶ï¼Œ
    ä¸å‘é‡æ£€ç´¢ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰å½¢æˆäº’è¡¥ã€‚
    
    Args:
        text: å¾…æå–æ ‡ç­¾çš„æ–‡æœ¬
        
    Returns:
        str: JSON æ ¼å¼çš„æ ‡ç­¾åˆ—è¡¨
    """
    client = OpenAI(
        api_key=os.getenv("DASHSCOPE_API_KEY"), 
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
    system_message = """ä½ æ˜¯ä¸€ä¸ªæ ‡ç­¾æå–ä¸“å®¶ã€‚è¯·ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¹¶æŒ‰è¦æ±‚è¾“å‡ºæ ‡ç­¾ã€‚
---
ã€æ”¯æŒçš„æ ‡ç­¾ç±»å‹ã€‘
- äººå
- éƒ¨é—¨åç§°
- èŒä½åç§°
- æŠ€æœ¯é¢†åŸŸ
- äº§å“åç§°
---
ã€è¾“å‡ºè¦æ±‚ã€‘
1. è¯·ç”¨ JSON æ ¼å¼è¾“å‡ºï¼Œå¦‚ï¼š[{"key": "éƒ¨é—¨åç§°", "value": "æ•™ç ”éƒ¨"}]
2. å¦‚æœæŸç±»æ ‡ç­¾æœªè¯†åˆ«åˆ°ï¼Œåˆ™ä¸è¾“å‡ºè¯¥ç±»
"""
    completion = client.chat.completions.create(
        model="qwen-plus",
        messages=[
            {'role': 'system', 'content': system_message},
            {'role': 'user', 'content': text}
        ],
        response_format={"type": "json_object"}
    )
    return completion.choices[0].message.content


# ==============================================================================
#                               ä¸»ç¨‹åºæ‰§è¡ŒåŒº
# ==============================================================================

# å®šä¹‰æµ‹è¯•é—®é¢˜ä¸æ ‡å‡†ç­”æ¡ˆ
question = 'å¼ ä¼Ÿæ˜¯å“ªä¸ªéƒ¨é—¨çš„'
ground_truth = '''å…¬å¸æœ‰ä¸‰åå¼ ä¼Ÿï¼Œåˆ†åˆ«æ˜¯ï¼š
- æ•™ç ”éƒ¨çš„å¼ ä¼Ÿï¼šèŒä½æ˜¯æ•™ç ”ä¸“å‘˜ï¼Œé‚®ç®± zhangwei@educompany.comã€‚
- è¯¾ç¨‹å¼€å‘éƒ¨çš„å¼ ä¼Ÿï¼šèŒä½æ˜¯è¯¾ç¨‹å¼€å‘ä¸“å‘˜ï¼Œé‚®ç®± zhangwei01@educompany.comã€‚
- ITéƒ¨çš„å¼ ä¼Ÿï¼šèŒä½æ˜¯ITä¸“å‘˜ï¼Œé‚®ç®± zhangwei036@educompany.comã€‚
'''


def demo_step1_baseline():
    """
    Step 1: åŸºå‡†æµ‹è¯• (Baseline)ã€‚
    
    ã€ç›®çš„ã€‘
    ä½¿ç”¨æ—§ç´¢å¼• + æ‰©å¤§ Top-K è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå»ºç«‹ä¼˜åŒ–å‰çš„æ€§èƒ½åŸºçº¿ã€‚
    
    ã€æ ¸å¿ƒæ“ä½œã€‘
    - åŠ è½½å·²æœ‰ç´¢å¼•
    - è®¾ç½® similarity_top_k=10ï¼ˆæ‰©å¤§å¬å›çª—å£ï¼‰
    - æ‰§è¡ŒæŸ¥è¯¢å¹¶è¯„ä¼°
    """
    print("\n" + "#" * 30 + " Step 1: åŸºå‡†æµ‹è¯• (Top-K=10) " + "#" * 30 + "\n")
    try:
        index = rag.load_index()
        query_engine = index.as_query_engine(streaming=True, similarity_top_k=10)
        response = ask(question, query_engine)
        show_evaluation_result(evaluate_result(question, response, ground_truth), "Step 1 è¯„ä¼°ç»“æœ")
    except Exception as e:
        print(f"Step 1 è·³è¿‡ (ç´¢å¼•å¯èƒ½ä¸å­˜åœ¨): {e}")


def demo_step2_reindex():
    """
    Step 2: æ•°æ®å±‚ä¼˜åŒ– - é‡å»ºç´¢å¼•ã€‚
    
    ã€ç›®çš„ã€‘
    å½“çŸ¥è¯†åº“é™ˆæ—§æˆ–ç¼ºå¤±å…³é”®æ–‡æ¡£æ—¶ï¼Œéœ€è¦é‡æ–°åŠ è½½æ–‡æ¡£å¹¶é‡å»ºç´¢å¼•ã€‚
    
    ã€æ ¸å¿ƒæ“ä½œã€‘
    - åŠ è½½æ–°æ–‡æ¡£ç›®å½• (./docs/ragdoc1)
    - ä½¿ç”¨ VectorStoreIndex.from_documents() é‡å»ºç´¢å¼•
    - å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ•ˆæœ
    
    Returns:
        tuple: (æ–‡æ¡£åˆ—è¡¨, æ–°ç´¢å¼•)
    """
    print("\n" + "#" * 30 + " Step 2: æ•°æ®å±‚ä¼˜åŒ– (é‡å»ºç´¢å¼•) " + "#" * 30 + "\n")
    print('ğŸ“‚ æ­£åœ¨åŠ è½½æ–°æ–‡æ¡£ (./docs/ragdoc1)...')
    documents_step2 = SimpleDirectoryReader('./docs/ragdoc1').load_data()

    print('ğŸ› ï¸ æ­£åœ¨é‡å»ºç´¢å¼•...')
    index_step2 = VectorStoreIndex.from_documents(documents_step2)

    query_engine = index_step2.as_query_engine(streaming=True, similarity_top_k=5)
    response = ask(question, query_engine)
    show_evaluation_result(evaluate_result(question, response, ground_truth), "Step 2 è¯„ä¼°ç»“æœ")

    return documents_step2, index_step2


def demo_step3_parsing():
    """
    Step 3: è§£æå±‚ä¼˜åŒ– - PDF è½¬ Markdownã€‚
    
    ã€ç›®çš„ã€‘
    ä¼ ç»Ÿ PDF è§£æå¸¸ä¸¢å¤±è¡¨æ ¼ã€æ ‡é¢˜å±‚çº§ç­‰ç»“æ„ä¿¡æ¯ã€‚
    ä½¿ç”¨ PyMuPDF4LLM è½¬ä¸º Markdown å¯ä¿ç•™æ–‡æ¡£ç»“æ„ã€‚
    
    ã€åŸç†ã€‘
    PDF æ–‡ä»¶æœ¬è´¨æ˜¯"æ’ç‰ˆæŒ‡ä»¤"ï¼Œç›´æ¥æå–æ–‡æœ¬ä¼šä¸¢å¤±ç»“æ„ä¿¡æ¯ï¼ˆå¦‚è¡¨æ ¼è¾¹ç•Œã€æ ‡é¢˜å±‚çº§ï¼‰ã€‚
    ç»“æ„åŒ–è§£æç­–ç•¥ï¼š
    - PyMuPDF4LLM: æœ¬åœ°è§£æï¼Œè½»é‡çº§ï¼Œé€Ÿåº¦å¿«ï¼Œé€‚åˆç®€å•æ–‡æ¡£
    - DashScopeParse: äº‘ç«¯è§£æï¼Œé‡é‡çº§ï¼Œè´¨é‡é«˜ï¼Œé€‚åˆå¤æ‚æ–‡æ¡£ï¼ˆè¡¨æ ¼ã€å…¬å¼ï¼‰
    - MinerU: æœ¬åœ°è§£æï¼Œé‡é‡çº§ï¼Œéœ€è¦ GPUï¼Œè´¨é‡æœ€é«˜
    
    ã€ä¸ºä»€ä¹ˆéœ€è¦ Markdownï¼Ÿã€‘
    - ä¿ç•™æ ‡é¢˜å±‚çº§ï¼ˆ# ## ###ï¼‰ï¼Œä¾¿äºæŒ‰ç« èŠ‚åˆ‡åˆ†
    - ä¿ç•™è¡¨æ ¼ç»“æ„ï¼Œé˜²æ­¢ä¿¡æ¯ä¸¢å¤±
    - ä¿ç•™åˆ—è¡¨æ ¼å¼ï¼Œä¾¿äºä¿¡æ¯æå–
    
    ã€æ¶¦è‰²çš„ä½œç”¨ã€‘
    PDF è½¬ Markdown å¯èƒ½å­˜åœ¨çš„é—®é¢˜ï¼š
    - ç›®å½•å±‚çº§é”™ä¹±
    - è¡¨æ ¼ä¸Šä¸‹è¡Œä¸å¯¹é½
    - ä¸Šä¸‹æ–‡æ–­è£‚
    ä½¿ç”¨ LLM æ¶¦è‰²å¯ä»¥ä¿®å¤è¿™äº›æ ¼å¼é—®é¢˜ã€‚
    
    ã€æ ¸å¿ƒæ“ä½œã€‘
    1. ä½¿ç”¨ PyMuPDF4LLM æœ¬åœ°è§£æ PDF
    2. ä½¿ç”¨ LLM æ¶¦è‰² Markdown å†…å®¹ï¼ˆä¿®å¤æ ¼å¼é”™è¯¯ï¼‰
    3. ä¿å­˜æ¶¦è‰²åçš„æ–‡æ¡£åˆ°æœ¬åœ°
    """
    print("\n" + "#" * 30 + " Step 3: è§£æå±‚ä¼˜åŒ– (PDF -> Markdown) " + "#" * 30 + "\n")
    pdf_path = './docs/å†…å®¹å…¬å¸å„éƒ¨é—¨èŒè´£ä¸å…³é”®è§’è‰²è”ç³»ä¿¡æ¯æ±‡æ€».pdf'
    md_content = file_to_md_local(pdf_path)
    print(f"\nğŸ“„ è§£æç»“æœé¢„è§ˆ (å‰300å­—ç¬¦):\n{md_content[:300]}...")

    md_polished = md_polisher(md_content)
    if not md_polished: 
        md_polished = md_content 

    output_path = './docs/optimized_doc.md'
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(md_polished)
    print(f"\nğŸ’¾ æ¶¦è‰²åçš„æ–‡æ¡£å·²ä¿å­˜è‡³: {output_path}")
    print("-" * 50)


def demo_step4_chunking(documents):
    """
    Step 4: åˆ‡ç‰‡å±‚ä¼˜åŒ– - åˆ‡ç‰‡å™¨å¤§æ¯”æ‹¼ã€‚
    
    ã€ç›®çš„ã€‘
    å¯¹æ¯” 5 ç§åˆ‡ç‰‡ç­–ç•¥çš„æ•ˆæœã€‚åˆ‡ç‰‡è´¨é‡ç›´æ¥å½±å“æ£€ç´¢ç²¾åº¦ã€‚
    
    ã€æ ¸å¿ƒåŸç†ã€‘
    æ–‡æ¡£åˆ‡ç‰‡çš„æ ¸å¿ƒç›®æ ‡ï¼šåœ¨"ä¿¡æ¯å®Œæ•´æ€§"å’Œ"æ£€ç´¢ç²’åº¦"ä¹‹é—´å–å¾—å¹³è¡¡ã€‚
    - åˆ‡å¤ªå¤§ï¼šä¸€ä¸ª chunk åŒ…å«å¤šä¸ªä¸»é¢˜ï¼Œè¯­ä¹‰æ··æ‚ï¼Œæ£€ç´¢ä¸ç²¾å‡†
    - åˆ‡å¤ªå°ï¼šä¸Šä¸‹æ–‡ä¸è¶³ï¼Œä¿¡æ¯ç¢ç‰‡åŒ–ï¼Œç†è§£å›°éš¾
    
    ã€5 ç§åˆ‡ç‰‡ç­–ç•¥è¯¦è§£ã€‘
    
    1. TokenTextSplitterï¼ˆæŒ‰ Token æ•°åˆ‡åˆ†ï¼‰
       - åŸç†ï¼šæŒ‰å›ºå®š Token æ•°ï¼ˆå¦‚ 512ï¼‰å¼ºåˆ¶åˆ‡åˆ†
       - ä¼˜åŠ¿ï¼šç®€å•å¿«é€Ÿï¼Œé€‚åˆè‹±æ–‡
       - åŠ£åŠ¿ï¼šå¯èƒ½åœ¨å¥å­ä¸­é—´åˆ‡æ–­ï¼Œè¯­ä¹‰ä¸å®Œæ•´
       - é€‚ç”¨ï¼šåŸå‹éªŒè¯ã€å¯¹ç²¾åº¦è¦æ±‚ä¸é«˜çš„åœºæ™¯
    
    2. SentenceSplitterï¼ˆæŒ‰å¥å­åˆ‡åˆ†ï¼‰
       - åŸç†ï¼šæŒ‰å¥å­è¾¹ç•Œï¼ˆå¥å·ã€é—®å·ã€æ„Ÿå¹å·ï¼‰åˆ‡åˆ†ï¼Œç´¯ç§¯åˆ°æŒ‡å®š Token æ•°
       - ä¼˜åŠ¿ï¼šä¿æŒå¥å­å®Œæ•´æ€§ï¼Œè¯­ä¹‰è¿è´¯
       - åŠ£åŠ¿ï¼šå¿½ç•¥æ®µè½å’Œç« èŠ‚ç»“æ„ï¼Œå¯èƒ½å°†ä¸ç›¸å…³çš„å¥å­æ”¾åœ¨ä¸€èµ·
       - é€‚ç”¨ï¼šé€šç”¨åœºæ™¯ï¼Œå¹³è¡¡äº†é€Ÿåº¦å’Œè´¨é‡
    
    3. MarkdownNodeParserï¼ˆæŒ‰ Markdown ç»“æ„åˆ‡åˆ†ï¼‰
       - åŸç†ï¼šæŒ‰ Markdown æ ‡é¢˜å±‚çº§ï¼ˆ#ã€##ã€###ï¼‰åˆ‡åˆ†
       - ä¼˜åŠ¿ï¼šä¿ç•™æ–‡æ¡£ç»“æ„ï¼ŒåŒä¸€ç« èŠ‚çš„å†…å®¹èšåˆåœ¨ä¸€èµ·
       - åŠ£åŠ¿ï¼šä¾èµ–æ–‡æ¡£æ ¼å¼è´¨é‡ï¼Œçº¯æ–‡æœ¬æ–‡æ¡£æ— æ³•ä½¿ç”¨
       - é€‚ç”¨ï¼šç»“æ„åŒ–æ–‡æ¡£ï¼ˆæŠ€æœ¯æ–‡æ¡£ã€æŠ¥å‘Šï¼‰
    
    4. SentenceWindowNodeParserï¼ˆæ»‘åŠ¨çª—å£ï¼‰
       - åŸç†ï¼šä»¥ä¸€ä¸ªå¥å­ä¸ºæ ¸å¿ƒï¼Œå‰åå„æ‰©å±• N ä¸ªå¥å­ä½œä¸ºä¸Šä¸‹æ–‡çª—å£
       - ä¼˜åŠ¿ï¼šä¸Šä¸‹æ–‡ä¸°å¯Œï¼Œæ¯ä¸ª chunk éƒ½æœ‰è¶³å¤Ÿçš„å‰åæ–‡ä¿¡æ¯
       - åŠ£åŠ¿ï¼šå­˜å‚¨å†—ä½™ï¼ˆç›¸é‚» chunk é‡å åº¦é«˜ï¼‰ï¼Œæ£€ç´¢æ—¶éœ€è¦åå¤„ç†
       - é€‚ç”¨ï¼šéœ€è¦ä¸°å¯Œä¸Šä¸‹æ–‡çš„åœºæ™¯ï¼ˆå¦‚å¤šè·³æ¨ç†ï¼‰
    
    5. SemanticSplitterNodeParserï¼ˆè¯­ä¹‰åˆ‡åˆ†ï¼‰
       - åŸç†ï¼šè®¡ç®—ç›¸é‚»å¥å­çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œå½“ç›¸ä¼¼åº¦çªé™æ—¶åˆ‡åˆ†
       - ä¼˜åŠ¿ï¼šæ™ºèƒ½æ£€æµ‹ä¸»é¢˜è¾¹ç•Œï¼Œæ¯ä¸ª chunk å†…éƒ¨è¯­ä¹‰é«˜åº¦ä¸€è‡´
       - åŠ£åŠ¿ï¼šè®¡ç®—æˆæœ¬é«˜ï¼ˆéœ€è¦ Embeddingï¼‰ï¼Œé€Ÿåº¦æ…¢
       - é€‚ç”¨ï¼šå¯¹è´¨é‡è¦æ±‚æé«˜çš„åœºæ™¯ï¼ˆç²¾å‡†é—®ç­”ã€æ³•å¾‹æ–‡æ¡£ï¼‰
    
    ã€è¯„ä¼°ç»´åº¦ã€‘
    - Answer Correctness: å›ç­”æ­£ç¡®æ€§
    - Context Recall: æ£€ç´¢å¬å›ç‡
    - Context Precision: æ£€ç´¢ç²¾ç¡®åº¦ï¼ˆæ’åºè´¨é‡ï¼‰
    
    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
    """
    print("\n" + "#" * 30 + " Step 4: åˆ‡ç‰‡å±‚ä¼˜åŒ– (Splitter Comparison) " + "#" * 30 + "\n")

    # 1. TokenTextSplitter
    token_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=20)
    evaluate_splitter(token_splitter, documents, question, ground_truth, "TokenSplitter")

    # 2. SentenceSplitter
    sentence_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)
    evaluate_splitter(sentence_splitter, documents, question, ground_truth, "SentenceSplitter")

    # 3. MarkdownNodeParser
    markdown_splitter = MarkdownNodeParser()
    evaluate_splitter(markdown_splitter, documents, question, ground_truth, "MarkdownNodeParser")

    # 4. SentenceWindowNodeParser
    sentence_window_splitter = SentenceWindowNodeParser.from_defaults(
        window_size=3,
        window_metadata_key="window",
        original_text_metadata_key="original_text"
    )
    evaluate_splitter(
        sentence_window_splitter, 
        documents, 
        question, 
        ground_truth, 
        "SentenceWindow",
        node_postprocessors=[MetadataReplacementPostProcessor(target_metadata_key="window")]
    )

    # 5. SemanticSplitterNodeParser
    semantic_splitter = SemanticSplitterNodeParser(
        buffer_size=1,
        breakpoint_percentile_threshold=95,
        embed_model=Settings.embed_model
    )
    evaluate_splitter(semantic_splitter, documents, question, ground_truth, "SemanticSplitter")


def demo_step5_embedding(documents):
    """
    Step 5: åµŒå…¥å±‚ä¼˜åŒ– - Embedding æ¨¡å‹å¯¹æ¯”ã€‚
    
    ã€ç›®çš„ã€‘
    Embedding æ¨¡å‹æ˜¯ RAG ç³»ç»Ÿçš„"è¯­ä¹‰åŸºçŸ³"ï¼Œç›´æ¥å†³å®šäº†ç›¸ä¼¼åº¦è®¡ç®—çš„å‡†ç¡®æ€§ã€‚
    å¯¹æ¯” text-embedding-v2 ä¸ v3 åœ¨è¯­ä¹‰åŒºåˆ†åº¦ä¸Šçš„å·®å¼‚ã€‚
    
    ã€æ ¸å¿ƒåŸç†ã€‘
    Embedding æ¨¡å‹å°†æ–‡æœ¬æ˜ å°„åˆ°é«˜ç»´å‘é‡ç©ºé—´ï¼ˆå¦‚ 1536 ç»´ï¼‰ï¼Œè¯­ä¹‰ç›¸è¿‘çš„æ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­
    è·ç¦»ä¹Ÿè¿‘ã€‚å‘é‡æ£€ç´¢æœ¬è´¨æ˜¯åœ¨é«˜ç»´ç©ºé—´ä¸­å¯»æ‰¾"æœ€è¿‘é‚»"ã€‚
    
    ã€Embedding è´¨é‡çš„ä¸‰ä¸ªç»´åº¦ã€‘
    1. è¯­ä¹‰åŒºåˆ†åº¦ï¼šç›¸å…³æ–‡æœ¬è·ç¦»è¿‘ï¼Œæ— å…³æ–‡æœ¬è·ç¦»è¿œ
    2. å‘é‡ç»´åº¦ï¼šç»´åº¦è¶Šé«˜è¡¨è¾¾èƒ½åŠ›è¶Šå¼ºï¼Œä½†è®¡ç®—æˆæœ¬ä¹Ÿè¶Šé«˜
    3. é¢†åŸŸé€‚é…æ€§ï¼šé€šç”¨æ¨¡å‹ vs é¢†åŸŸæ¨¡å‹ï¼ˆå¦‚æ³•å¾‹ã€åŒ»ç–—ï¼‰
    
    ã€ä½™å¼¦ç›¸ä¼¼åº¦åŸç†ã€‘
    - å…¬å¼ï¼šcos(Î¸) = (A Â· B) / (||A|| * ||B||)
    - å–å€¼èŒƒå›´ï¼š[-1, 1]
      * 1.0ï¼šå®Œå…¨ç›¸åŒæ–¹å‘ï¼ˆè¯­ä¹‰å®Œå…¨ä¸€è‡´ï¼‰
      * 0.0ï¼šå‚ç›´ï¼ˆæ— å…³ï¼‰
      * -1.0ï¼šå®Œå…¨ç›¸åæ–¹å‘ï¼ˆè¯­ä¹‰ç›¸åï¼‰
    - ä¼˜åŠ¿ï¼šä¸å—å‘é‡é•¿åº¦å½±å“ï¼Œåªå…³æ³¨æ–¹å‘
    
    ã€V2 vs V3 å¯¹æ¯”ã€‘
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ç»´åº¦         â”‚ V2             â”‚ V3             â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ å‘é‡ç»´åº¦     â”‚ 1536           â”‚ 1536           â”‚
    â”‚ è®­ç»ƒæ•°æ®     â”‚ è¾ƒå°‘           â”‚ æ›´å¤š           â”‚
    â”‚ è¯­ä¹‰åŒºåˆ†åº¦   â”‚ ä¸­ç­‰           â”‚ æ›´é«˜           â”‚
    â”‚ å¤šè¯­è¨€èƒ½åŠ›   â”‚ åŸºç¡€           â”‚ å¢å¼º           â”‚
    â”‚ é•¿æ–‡æœ¬å¤„ç†   â”‚ åŸºç¡€           â”‚ å¢å¼º           â”‚
    â”‚ é€‚ç”¨åœºæ™¯     â”‚ é€šç”¨           â”‚ é«˜ç²¾åº¦åœºæ™¯     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    ã€ä¸ºä»€ä¹ˆ Embedding æ¨¡å‹å¾ˆé‡è¦ï¼Ÿã€‘
    RAG ç³»ç»Ÿçš„å¬å›ç‡ä¸Šé™ç”± Embedding æ¨¡å‹å†³å®šï¼š
    - å¥½çš„ Embeddingï¼šç›¸å…³æ–‡æ¡£æ’åœ¨å‰é¢ï¼ŒTop-K å°±èƒ½å¬å›
    - å·®çš„ Embeddingï¼šç›¸å…³æ–‡æ¡£æ’åœ¨åé¢ï¼Œå³ä½¿ Top-K å¾ˆå¤§ä¹Ÿå¬å›ä¸äº†
    
    ã€å¯¹æ¯”è§’åº¦ã€‘
    1. å‘é‡è¿ç®—åŸç†æ¼”ç¤ºï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    2. æ–‡æœ¬ç›¸ä¼¼åº¦å¯¹æ¯”ï¼ˆåŒä¸€æ–‡æœ¬åœ¨ä¸åŒæ¨¡å‹ä¸‹çš„è¡¨ç°ï¼‰
    3. RAG å®æˆ˜å¯¹æ¯”ï¼ˆå®Œæ•´çš„æ£€ç´¢ + ç”Ÿæˆæµç¨‹ï¼‰
    
    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
    """
    print("\n" + "#" * 30 + " Step 5: åµŒå…¥å±‚ä¼˜åŒ– (Embedding Comparison) " + "#" * 30 + "\n")

    # 1. å‘é‡è¿ç®—åŸç†æ¼”ç¤º
    print("\n>>> è§’åº¦ 1: å‘é‡è¿ç®—åŸç†æ¼”ç¤º")
    a = np.array([0.2, 0.8])
    b = np.array([0.3, 0.7])
    c = np.array([0.8, 0.2])
    print(f"å‘é‡ A: {a}")
    print(f"å‘é‡ B: {b}")
    print(f"å‘é‡ C: {c}")
    print(f"A ä¸ B çš„ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_similarity(a, b):.4f}")
    print(f"B ä¸ C çš„ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_similarity(b, c):.4f}")
    print("-" * 30)

    # 2. å®æˆ˜ï¼šä¸åŒæ¨¡å‹çš„æ–‡æœ¬ç›¸ä¼¼åº¦
    print("\n>>> è§’åº¦ 2: ä¸åŒæ¨¡å‹çš„æ–‡æœ¬ç›¸ä¼¼åº¦")
    query_text = "å¼ ä¼Ÿæ˜¯å“ªä¸ªéƒ¨é—¨çš„"
    chunks = [
        "æ ¸ï¼Œæä¾›â¾æ”¿ç®¡ç†ä¸åè°ƒâ½€æŒï¼Œä¼˜åŒ–â¾æ”¿â¼¯ä½œæµç¨‹ã€‚ â¾æ”¿éƒ¨ ç§¦â»œ è”¡é™ G705 034 â¾æ”¿ â¾æ”¿ä¸“å‘˜ 13800000034 qinf@educompany.com ç»´æŠ¤å…¬å¸æ¡£æ¡ˆä¸ä¿¡æ¯ç³»ç»Ÿï¼Œè´Ÿè´£å…¬å¸é€šçŸ¥åŠå…¬å‘Šçš„å‘å¸ƒ",
        "ç»„ç»‡å…¬å¸æ´»åŠ¨çš„å‰æœŸå‡†å¤‡ä¸åæœŸè¯„ä¼°ï¼Œç¡®ä¿å…¬å¸å„é¡¹â¼¯ä½œçš„é¡ºåˆ©è¿›â¾ã€‚ ITéƒ¨ å¼ ä¼Ÿ â»¢äº‘ H802 036 ITâ½€æ’‘ ITä¸“å‘˜ 13800000036 zhangwei036@educompany.com è¿›â¾å…¬å¸â½¹ç»œåŠç¡¬ä»¶è®¾å¤‡çš„é…ç½®"
    ]
    embedding_models_dict = {
        "text-embedding-v2": DashScopeEmbedding(model_name="text-embedding-v2"),
        "text-embedding-v3": DashScopeEmbedding(model_name="text-embedding-v3")
    }
    compare_embeddings(query_text, chunks, embedding_models_dict)

    # 3. å®æˆ˜ï¼šRAG æ•ˆæœå¯¹æ¯”
    sentence_splitter_for_embed = SentenceSplitter(chunk_size=1000, chunk_overlap=200)
    compare_embedding_models(
        documents=documents,
        question=question,
        ground_truth=ground_truth,
        sentence_splitter=sentence_splitter_for_embed
    )


def demo_step6_vector_db():
    """
    Step 6: å­˜å‚¨å±‚ä¼˜åŒ– - å‘é‡æ•°æ®åº“é€‰å‹ã€‚
    
    ã€ç›®çš„ã€‘
    å‘é‡æ•°æ®åº“æ˜¯ RAG ç³»ç»Ÿçš„"è®°å¿†ä¸­æ¢"ã€‚ä¸åŒæ•°æ®åº“åœ¨å­˜å‚¨æ•ˆç‡ã€æŸ¥è¯¢æ€§èƒ½ã€
    æ‰©å±•æ€§ç­‰æ–¹é¢å„æœ‰ä¼˜åŠ£ã€‚
    
    ã€é€‰å‹å»ºè®®ã€‘
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ æ•°æ®åº“ç±»å‹   â”‚ ä»£è¡¨äº§å“   â”‚ é€‚ç”¨åœºæ™¯     â”‚ æ ¸å¿ƒä¼˜åŠ¿     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ å†…å­˜å‹       â”‚ Faiss      â”‚ åŸå‹éªŒè¯     â”‚ é€Ÿåº¦æå¿«     â”‚
    â”‚ åµŒå…¥å¼       â”‚ Chroma     â”‚ å•æœºåº”ç”¨     â”‚ éƒ¨ç½²ç®€å•     â”‚
    â”‚ ä¸“ç”¨å‘é‡åº“   â”‚ Milvus     â”‚ ç”Ÿäº§ç¯å¢ƒ     â”‚ åŠŸèƒ½å®Œæ•´     â”‚
    â”‚ ä¼ ç»ŸDBæ‰©å±•   â”‚ Pgvector   â”‚ æ··åˆåœºæ™¯     â”‚ å…¼å®¹ç°æœ‰æ¶æ„ â”‚
    â”‚ äº‘åŸç”Ÿ       â”‚ DashVector â”‚ å¿«é€Ÿä¸Šçº¿     â”‚ å…è¿ç»´       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    ã€æœ¬ä¾‹ä½¿ç”¨ã€‘
    LlamaIndex é»˜è®¤çš„å†…å­˜ VectorStoreï¼ˆé€‚åˆå­¦ä¹ å’ŒåŸå‹ï¼‰ã€‚
    """
    print("\n" + "#" * 30 + " Step 6: å­˜å‚¨å±‚ä¼˜åŒ– (Vector DB Selection) " + "#" * 30 + "\n")
    print("ğŸ“š æœ¬æ­¥éª¤ä¸ºç†è®ºè¯´æ˜ï¼Œè¯¦è§å‡½æ•° docstring ä¸­çš„é€‰å‹å»ºè®®ã€‚\n")
    print("ğŸ’¡ ç”Ÿäº§ç¯å¢ƒæ¨èï¼š")
    print("  - å°è§„æ¨¡ï¼ˆ< 10ä¸‡æ–‡æ¡£ï¼‰ï¼šChromaã€LanceDB")
    print("  - ä¸­è§„æ¨¡ï¼ˆ10ä¸‡-1000ä¸‡ï¼‰ï¼šMilvusã€Qdrant")
    print("  - å¤§è§„æ¨¡ï¼ˆ> 1000ä¸‡ï¼‰ï¼šäº‘åŸç”Ÿæ–¹æ¡ˆï¼ˆDashVectorã€Pineconeï¼‰")
    print("  - å·²æœ‰ PostgreSQLï¼šPgvector æ‰©å±•")


def demo_step7_retrieval(base_index):
    """
    Step 7: æ£€ç´¢å±‚ä¼˜åŒ– - è¿›é˜¶æ£€ç´¢ç­–ç•¥ã€‚
    
    ã€ç›®çš„ã€‘
    å•çº¯çš„ Top-K å‘é‡æ£€ç´¢å¾€å¾€ä¸å¤Ÿç²¾å‡†ã€‚æœ¬æ­¥éª¤ä»‹ç» RAG æ£€ç´¢é“¾è·¯ä¸­
    "æ£€ç´¢å‰"ã€"æ£€ç´¢ä¸­"ã€"æ£€ç´¢å"çš„ä¸‰é˜¶æ®µä¼˜åŒ–ç­–ç•¥ã€‚
    
    ã€æ ¸å¿ƒåŸç†ã€‘
    RAG æ£€ç´¢é“¾è·¯å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰å¯¹åº”çš„ä¼˜åŒ–ç­–ç•¥ã€‚
    
    ã€æ£€ç´¢å‰ï¼ˆPre-Retrievalï¼‰- ä¼˜åŒ–æŸ¥è¯¢ã€‘
    
    1. é—®é¢˜æ”¹å†™ (Query Rewriting)
       - åŸç†ï¼šç”¨æˆ·é—®é¢˜å¾€å¾€è¿‡äºç®€çŸ­æˆ–æ¨¡ç³Šï¼ˆå¦‚"å¼ ä¼Ÿæ˜¯è°ï¼Ÿ"ï¼‰ï¼Œç›´æ¥æ£€ç´¢æ•ˆæœä¸ä½³ã€‚
         é€šè¿‡ LLM å°†é—®é¢˜æ‰©å†™ä¸ºæ›´å®Œæ•´ã€æ›´é€‚åˆæ£€ç´¢çš„å½¢å¼ã€‚
       - ç¤ºä¾‹ï¼š
         * åŸå§‹ï¼š"å¼ ä¼Ÿæ˜¯è°ï¼Ÿ"
         * æ”¹å†™ï¼š"å¼ ä¼Ÿæ˜¯å“ªä¸ªéƒ¨é—¨çš„å‘˜å·¥ï¼Ÿä»–çš„èŒä½æ˜¯ä»€ä¹ˆï¼Ÿè”ç³»æ–¹å¼æ˜¯ä»€ä¹ˆï¼Ÿ"
       - æ•ˆæœï¼šæ‰©å†™åçš„é—®é¢˜åŒ…å«æ›´å¤šå…³é”®è¯ï¼Œæå‡å¬å›ç‡
    
    2. å¤šæ­¥æŸ¥è¯¢ (Multi-step Query)
       - åŸç†ï¼šå¤æ‚é—®é¢˜ï¼ˆå¦‚"å¼ ä¼Ÿå’Œæå››åˆ†åˆ«æ˜¯å“ªä¸ªéƒ¨é—¨çš„ï¼Ÿ"ï¼‰åŒ…å«å¤šä¸ªå­é—®é¢˜ã€‚
         StepDecomposeQueryTransform ä¼šå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªç®€å•å­é—®é¢˜åºåˆ—ï¼Œ
         é€æ­¥æ£€ç´¢å¹¶æ±‡æ€»ç­”æ¡ˆã€‚
       - æµç¨‹ï¼š
         * æ­¥éª¤1ï¼šè¯†åˆ«é—®é¢˜ä¸­çš„å¤šä¸ªä¸»ä½“ï¼ˆå¼ ä¼Ÿã€æå››ï¼‰
         * æ­¥éª¤2ï¼šä¸ºæ¯ä¸ªä¸»ä½“ç”Ÿæˆå­é—®é¢˜
         * æ­¥éª¤3ï¼šé€æ­¥æ£€ç´¢å¹¶åˆå¹¶ç»“æœ
       - æ•ˆæœï¼šé¿å…é—æ¼ä¿¡æ¯ï¼Œæå‡å¤šè·³é—®é¢˜çš„å›ç­”è´¨é‡
    
    3. HyDE (Hypothetical Document Embeddings - å‡è®¾æ€§æ–‡æ¡£åµŒå…¥)
       - åŸç†ï¼šæœ‰æ—¶é—®é¢˜ï¼ˆQueryï¼‰å’Œæ–‡æ¡£ï¼ˆDocumentï¼‰åœ¨è¯­ä¹‰ç©ºé—´ä¸­è·ç¦»è¾ƒè¿œã€‚
         * é—®é¢˜å¾ˆçŸ­ï¼š"å¼ ä¼Ÿæ˜¯è°ï¼Ÿ"ï¼ˆå‡ ä¸ªå­—ï¼‰
         * æ–‡æ¡£å¾ˆé•¿ï¼š"å¼ ä¼Ÿï¼Œæ•™ç ”éƒ¨ä¸“å‘˜ï¼Œè´Ÿè´£è¯¾ç¨‹å¼€å‘..."ï¼ˆå‡ åå­—ï¼‰
         HyDE ç­–ç•¥æ˜¯å…ˆè®© LLM ç”Ÿæˆä¸€ä¸ª"å‡è®¾æ€§ç­”æ¡ˆ"ï¼Œè¿™ä¸ªç­”æ¡ˆè™½ç„¶å¯èƒ½åŒ…å«å¹»è§‰ï¼Œ
         ä½†åœ¨è¯­ä¹‰ä¸Šä¸çœŸå®æ–‡æ¡£éå¸¸æ¥è¿‘ã€‚ç„¶åç”¨è¿™ä¸ª"å‡è®¾ç­”æ¡ˆ"å»æ£€ç´¢ã€‚
       - ç¤ºä¾‹ï¼š
         * é—®é¢˜ï¼š"å¼ ä¼Ÿæ˜¯è°ï¼Ÿ"
         * å‡è®¾ç­”æ¡ˆï¼š"å¼ ä¼Ÿæ˜¯æ•™ç ”éƒ¨çš„ä¸€åä¸“å‘˜ï¼Œä¸»è¦è´Ÿè´£è¯¾ç¨‹å†…å®¹çš„å¼€å‘å’Œå®¡æ ¸å·¥ä½œ..."
         * ç”¨å‡è®¾ç­”æ¡ˆçš„ Embedding å»æ£€ç´¢ â†’ æ›´å®¹æ˜“å¬å›ç›¸å…³æ–‡æ¡£
       - æ•ˆæœï¼šç¼©å°é—®é¢˜-æ–‡æ¡£çš„è¯­ä¹‰è·ç¦»ï¼Œæå‡å¬å›ç‡
    
    ã€æ£€ç´¢ä¸­ï¼ˆRetrievalï¼‰- ä¼˜åŒ–å¬å›ã€‘
    
    4. æ ‡ç­¾å¢å¼º (Tag Extraction + Metadata Filtering)
       - åŸç†ï¼šçº¯å‘é‡æ£€ç´¢æ˜¯"æ¨¡ç³ŠåŒ¹é…"ï¼Œæ ‡ç­¾æ˜¯"ç²¾ç¡®åŒ¹é…"ã€‚
         æå–å®ä½“æ ‡ç­¾ï¼ˆå¦‚äººåã€éƒ¨é—¨åï¼‰ä½œä¸ºå…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶ï¼Œå¯ä»¥å®ç°"ç²¾ç¡®åˆ¶å¯¼"ã€‚
       - æµç¨‹ï¼š
         * æ­¥éª¤1ï¼šä»é—®é¢˜ä¸­æå–æ ‡ç­¾ï¼ˆå¦‚"å¼ ä¼Ÿ"ã€"æ•™ç ”éƒ¨"ï¼‰
         * æ­¥éª¤2ï¼šä»æ–‡æ¡£ä¸­æå–æ ‡ç­¾å¹¶å­˜å‚¨ä¸ºå…ƒæ•°æ®
         * æ­¥éª¤3ï¼šæ£€ç´¢æ—¶åŒæ—¶åŒ¹é…å‘é‡ç›¸ä¼¼åº¦å’Œæ ‡ç­¾ç²¾ç¡®åº¦
       - æ•ˆæœï¼šè¿‡æ»¤æ‰ä¸ç›¸å…³æ–‡æ¡£ï¼Œæå‡ç²¾ç¡®åº¦
    
    ã€æ£€ç´¢åï¼ˆPost-Retrievalï¼‰- ä¼˜åŒ–æ’åºã€‘
    
    5. é‡æ’åº (Rerank)
       - åŸç†ï¼š
         * Bi-Encoderï¼ˆå‘é‡æ£€ç´¢ï¼‰ï¼šå°† Query å’Œ Doc åˆ†åˆ«ç¼–ç ä¸ºå‘é‡ï¼Œè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ã€‚
           ä¼˜åŠ¿ï¼šé€Ÿåº¦å¿«ï¼ˆç™¾ä¸‡æ–‡æ¡£æ¯«ç§’çº§ï¼‰ï¼Œé€‚åˆç²—æ’ã€‚
           åŠ£åŠ¿ï¼šQuery å’Œ Doc ç‹¬ç«‹ç¼–ç ï¼Œæ— æ³•æ•æ‰æ·±å±‚äº¤äº’ã€‚
         * Cross-Encoderï¼ˆé‡æ’åºï¼‰ï¼šå°† Query å’Œ Doc æ‹¼æ¥åä¸€èµ·ç¼–ç ï¼Œæ•æ‰æ·±å±‚è¯­ä¹‰äº¤äº’ã€‚
           ä¼˜åŠ¿ï¼šç²¾åº¦æé«˜ï¼Œé€‚åˆç²¾æ’ã€‚
           åŠ£åŠ¿ï¼šè®¡ç®—é‡å¤§ï¼ˆæ¯å¯¹ Query-Doc éƒ½è¦é‡æ–°ç¼–ç ï¼‰ã€‚
       - æœ€ä½³å®è·µï¼š
         * ç¬¬ä¸€é˜¶æ®µï¼šç”¨ Bi-Encoder ç²—æ’ï¼Œä» 100ä¸‡ å¬å› Top-20
         * ç¬¬äºŒé˜¶æ®µï¼šç”¨ Cross-Encoder ç²¾æ’ï¼Œä» Top-20 ç­›é€‰ Top-3
       - æ•ˆæœï¼šåœ¨é€Ÿåº¦å’Œç²¾åº¦é—´å–å¾—æœ€ä½³å¹³è¡¡
    
    ã€ä¼˜åŒ–ç­–ç•¥æ€»ç»“ã€‘
    1. é—®é¢˜æ”¹å†™: æ‰©å†™æ¨¡ç³Šé—®é¢˜ï¼Œæå‡å¬å›ç‡
    2. å¤šæ­¥æŸ¥è¯¢: åˆ†è§£å¤æ‚é—®é¢˜ï¼Œé€æ­¥æ£€ç´¢
    3. HyDE: ç”Ÿæˆå‡è®¾æ€§ç­”æ¡ˆï¼Œç¼©å°é—®é¢˜-æ–‡æ¡£çš„è¯­ä¹‰è·ç¦»
    4. æ ‡ç­¾å¢å¼º: æå–å®ä½“æ ‡ç­¾ï¼Œç²¾ç¡®åˆ¶å¯¼
    5. é‡æ’åº: Cross-Encoder ç²¾æ’ï¼Œæå‡ Top-N è´¨é‡
    
    Args:
        base_index: å‘é‡ç´¢å¼•ï¼ˆStep 2 åˆ›å»ºçš„é«˜è´¨é‡ç´¢å¼•ï¼‰
    """
    print("\n" + "#" * 30 + " Step 7: æ£€ç´¢å±‚ä¼˜åŒ– (Advanced Retrieval) " + "#" * 30 + "\n")

    # --- 7.1 é—®é¢˜æ”¹å†™ (Query Rewriting) ---
    print("\n>>> 7.1 é—®é¢˜æ”¹å†™ (Query Rewriting)")
    print(f"åŸå§‹é—®é¢˜: {question}")
    rewritten_query = generate_queries(question)
    print(f"æ”¹å†™åé—®é¢˜: {rewritten_query}")

    query_engine = base_index.as_query_engine(similarity_top_k=5)
    print("\n[ä½¿ç”¨æ”¹å†™åçš„é—®é¢˜è¿›è¡Œæ£€ç´¢]")
    response = ask(rewritten_query, query_engine)
    show_evaluation_result(evaluate_result(rewritten_query, response, ground_truth), "7.1 é—®é¢˜æ”¹å†™è¯„ä¼°")

    # --- 7.2 å¤šæ­¥æŸ¥è¯¢ (Multi-step Query) ---
    print("\n>>> 7.2 å¤šæ­¥æŸ¥è¯¢ (Multi-step Query)")
    step_decompose_transform = StepDecomposeQueryTransform(verbose=True)
    query_engine_multistep = MultiStepQueryEngine(
        query_engine=base_index.as_query_engine(similarity_top_k=5),
        query_transform=step_decompose_transform,
        index_summary="å…¬å¸äººå‘˜ä¿¡æ¯ï¼ŒåŒ…å«å§“åã€éƒ¨é—¨ã€èŒä½ã€é‚®ç®±ç­‰"
    )
    print(f"ç”¨æˆ·é—®é¢˜: {question}")
    print("ğŸ¤– AIæ­£åœ¨è¿›è¡Œå¤šæ­¥æŸ¥è¯¢åˆ†è§£...")
    response = ask(question, query_engine_multistep)
    show_evaluation_result(evaluate_result(question, response, ground_truth), "7.2 å¤šæ­¥æŸ¥è¯¢è¯„ä¼°")

    # --- 7.3 HyDE (Hypothetical Document Embeddings) ---
    print("\n>>> 7.3 HyDE å‡è®¾æ€§æ–‡æ¡£æ£€ç´¢")
    hyde = HyDEQueryTransform(include_original=True)
    query_engine_hyde = TransformQueryEngine(
        query_engine=base_index.as_query_engine(similarity_top_k=5),
        query_transform=hyde
    )
    print("ğŸ¤– AIæ­£åœ¨ç”Ÿæˆå‡è®¾æ€§æ–‡æ¡£å¹¶æ£€ç´¢...")
    query_bundle = hyde(question)
    print(f"ğŸ‘» ç”Ÿæˆçš„å‡è®¾æ€§æ–‡æ¡£:\n{query_bundle.embedding_strs[0][:200]}...\n")

    response = ask(question, query_engine_hyde)
    show_evaluation_result(evaluate_result(question, response, ground_truth), "7.3 HyDE è¯„ä¼°")

    # --- 7.4 æ ‡ç­¾å¢å¼º (Tag Extraction) - Demo ---
    print("\n>>> 7.4 æ ‡ç­¾å¢å¼º (Tag Extraction Demo)")
    sample_text = "å¼ ä¼Ÿæ˜¯ITéƒ¨çš„æŠ€æœ¯éª¨å¹²ï¼Œè´Ÿè´£å…¬å¸ç½‘ç»œå®‰å…¨ã€‚"
    print(f"åˆ†ææ–‡æœ¬: {sample_text}")
    print(f"æå–æ ‡ç­¾: {extract_tags(sample_text)}")

    # --- 7.5 é‡æ’åº (Rerank) ---
    print("\n>>> 7.5 é‡æ’åº (Reranking)")
    query_engine_rerank = base_index.as_query_engine(
        similarity_top_k=20,  # æ‰©å¤§å¬å›
        node_postprocessors=[
            DashScopeRerank(top_n=3, model="gte-rerank"),
            SimilarityPostprocessor(similarity_cutoff=0.2)
        ]
    )
    response = ask(question, query_engine_rerank)
    show_evaluation_result(evaluate_result(question, response, ground_truth), "7.5 é‡æ’åºè¯„ä¼°")


def demo_step8_generation(base_index):
    """
    Step 8: ç”Ÿæˆå±‚ä¼˜åŒ– - LLM å‚æ•°ä¸è§’è‰²è°ƒä¼˜ã€‚
    
    ã€ç›®çš„ã€‘
    RAG çš„æœ€åä¸€å…¬é‡Œæ˜¯ç”Ÿæˆã€‚é’ˆå¯¹ä¸åŒåœºæ™¯ï¼Œéœ€è¦è°ƒæ•´ LLM çš„æ ¸å¿ƒå‚æ•°ã€‚
    
    ã€æ ¸å¿ƒåŸç†ã€‘
    LLM ç”Ÿæˆè¿‡ç¨‹æœ¬è´¨æ˜¯"æ¦‚ç‡é‡‡æ ·"ï¼šæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒï¼Œç„¶åæŒ‰æ¦‚ç‡é‡‡æ ·ã€‚
    ä¸åŒå‚æ•°æ§åˆ¶é‡‡æ ·ç­–ç•¥ï¼Œä»è€Œå½±å“è¾“å‡ºé£æ ¼ã€‚
    
    ã€æ ¸å¿ƒå‚æ•°è¯¦è§£ã€‘
    
    1. Temperature (æ¸©åº¦) - æ§åˆ¶è¾“å‡ºéšæœºæ€§
       - åŸç†ï¼šæ¸©åº¦è¶Šä½ï¼Œé«˜æ¦‚ç‡ token è¢«é€‰ä¸­çš„å¯èƒ½æ€§è¶Šå¤§ï¼›æ¸©åº¦è¶Šé«˜ï¼Œä½æ¦‚ç‡ token
         ä¹Ÿæœ‰æœºä¼šè¢«é€‰ä¸­ã€‚
       - å…¬å¼ï¼šP_i = exp(logit_i / T) / Î£ exp(logit_j / T)
       - æ•ˆæœï¼š
         * T = 0.1 (ä½æ¸©)ï¼šå‡ ä¹æ€»æ˜¯é€‰æ‹©æœ€é«˜æ¦‚ç‡çš„ token â†’ è¾“å‡ºç¨³å®šã€ä¸¥è°¨ã€å¯é¢„æµ‹
         * T = 0.7 (ä¸­æ¸©)ï¼šå¹³è¡¡äº†ç¨³å®šæ€§å’Œåˆ›é€ æ€§
         * T = 1.5 (é«˜æ¸©)ï¼šä½æ¦‚ç‡ token ä¹Ÿæœ‰æœºä¼š â†’ è¾“å‡ºå‘æ•£ã€æœ‰åˆ›æ„ã€ä¸å¯é¢„æµ‹
       - é€‚ç”¨åœºæ™¯ï¼š
         * äº‹å®é—®ç­”ã€å®¢æœå¯¹è¯ã€ä»£ç ç”Ÿæˆ â†’ ä½æ¸©ï¼ˆ0.1-0.3ï¼‰
         * æ–‡æ¡ˆåˆ›ä½œã€å¤´è„‘é£æš´ã€æ•…äº‹ç»­å†™ â†’ é«˜æ¸©ï¼ˆ0.7-1.0ï¼‰
    
    2. Presence Penalty (å­˜åœ¨æƒ©ç½š) - é˜²æ­¢é‡å¤
       - åŸç†ï¼šå¯¹å·²ç»å‡ºç°è¿‡çš„ token æ–½åŠ æƒ©ç½šï¼Œé™ä½å…¶è¢«å†æ¬¡é€‰ä¸­çš„æ¦‚ç‡ã€‚
       - å–å€¼èŒƒå›´ï¼š[-2.0, 2.0]
         * æ­£å€¼ï¼šé¼“åŠ±æ¨¡å‹è°ˆè®ºæ–°è¯é¢˜ï¼Œé¿å…é‡å¤ï¼ˆå¦‚ 0.5-1.0ï¼‰
         * è´Ÿå€¼ï¼šå…è®¸æ¨¡å‹é‡å¤ä½¿ç”¨ç›¸åŒçš„è¯ï¼ˆå¦‚å¼•ç”¨åŸæ–‡ï¼‰
         * 0ï¼šä¸æ–½åŠ æƒ©ç½š
       - æ•ˆæœï¼š
         * Penalty = 0.0ï¼šå…è®¸é‡å¤ï¼Œé€‚åˆéœ€è¦ä¸¥æ ¼å¼•ç”¨åŸæ–‡çš„åœºæ™¯
         * Penalty = 0.5ï¼šé€‚åº¦æƒ©ç½šï¼Œé¿å…é™ˆè¯æ»¥è°ƒ
         * Penalty = 1.0ï¼šå¼ºåŠ›æƒ©ç½šï¼Œè¾“å‡ºæ›´å…·å¤šæ ·æ€§ä½†å¯èƒ½ä¸å¤Ÿè¿è´¯
       - é€‚ç”¨åœºæ™¯ï¼š
         * äº‹å®é—®ç­”ã€æ–‡æ¡£æ‘˜è¦ â†’ ä½æƒ©ç½šï¼ˆ0.0ï¼‰ï¼Œå…è®¸å¼•ç”¨åŸæ–‡
         * åˆ›æ„å†™ä½œã€æ–‡æ¡ˆç”Ÿæˆ â†’ é«˜æƒ©ç½šï¼ˆ0.5-1.0ï¼‰ï¼Œé¿å…é‡å¤
    
    3. Seed (éšæœºç§å­) - æ§åˆ¶å¯å¤ç°æ€§
       - åŸç†ï¼šå›ºå®šéšæœºç§å­åï¼Œç›¸åŒçš„è¾“å…¥ä¼šäº§ç”Ÿç›¸åŒçš„è¾“å‡ºï¼ˆDeterministicï¼‰ã€‚
       - æ•ˆæœï¼š
         * è®¾ç½® Seed = 42ï¼šæ¯æ¬¡è¿è¡Œç»“æœå®Œå…¨ä¸€è‡´ï¼Œä¾¿äºè°ƒè¯•å’Œè¯„ä¼°
         * ä¸è®¾ç½® Seedï¼šæ¯æ¬¡è¿è¡Œç»“æœä¸åŒï¼Œæ›´å…·éšæœºæ€§
       - é€‚ç”¨åœºæ™¯ï¼š
         * A/B æµ‹è¯•ã€æ•ˆæœè¯„ä¼° â†’ è®¾ç½®å›ºå®š Seed
         * ç”Ÿäº§ç¯å¢ƒã€éœ€è¦å¤šæ ·æ€§ â†’ ä¸è®¾ç½® Seed
    
    4. Max Tokens (æœ€å¤§è¾“å‡ºé•¿åº¦)
       - åŸç†ï¼šé™åˆ¶ç”Ÿæˆçš„æœ€å¤§ token æ•°ã€‚
       - æ•ˆæœï¼š
         * 512 tokensï¼šé€‚åˆç®€çŸ­é—®ç­”
         * 1024-2048 tokensï¼šé€‚åˆé•¿æ–‡æœ¬ç”Ÿæˆ
       - æ³¨æ„ï¼šè¿‡çŸ­å¯èƒ½å¯¼è‡´å›ç­”ä¸å®Œæ•´ï¼Œè¿‡é•¿æ¶ˆè€—æ›´å¤šèµ„æº
    
    ã€åœºæ™¯å¯¹æ¯”ã€‘
    - åœºæ™¯ 1: ä¸¥è°¨æ¨¡å¼ï¼ˆäº‹å®é—®ç­”ï¼‰
      * Temperature = 0.1 (ä½æ¸©ï¼Œç¡®ä¿äº‹å®å‡†ç¡®)
      * Presence Penalty = 0.0 (å…è®¸å¼•ç”¨åŸæ–‡)
      * Seed = 42 (å¯å¤ç°)
      * Max Tokens = 512 (ç®€çŸ­å›ç­”)
    
    - åœºæ™¯ 2: åˆ›æ„æ¨¡å¼ï¼ˆæ–‡æ¡ˆç”Ÿæˆï¼‰
      * Temperature = 0.8 (é«˜æ¸©ï¼Œæ¿€å‘åˆ›é€ åŠ›)
      * Presence Penalty = 0.5 (é¿å…é™ˆè¯æ»¥è°ƒ)
      * Seed = None (å…è®¸éšæœºå‘æŒ¥)
      * Max Tokens = 1024 (é•¿æ–‡æœ¬)
    
    Args:
        base_index: å‘é‡ç´¢å¼•
    """
    print("\n" + "#" * 30 + " Step 8: ç”Ÿæˆå±‚ä¼˜åŒ– (LLM Tuning) " + "#" * 30 + "\n")

    # åœºæ™¯ 1: ä¸¥è°¨çš„äº‹å®é—®ç­”
    print(">>> åœºæ™¯ 1: ä¸¥è°¨æ¨¡å¼ (Temp=0.1, Seed=42, Penalty=0.0)")
    llm_factual = OpenAILike(
        model="qwen-plus",
        api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
        api_key=os.getenv("DASHSCOPE_API_KEY"),
        is_chat_model=True,
        temperature=0.1,      # ä½æ¸©åº¦ï¼Œç¡®ä¿äº‹å®å‡†ç¡®
        max_tokens=512,
        presence_penalty=0.0, # ä¸å¼ºè¡Œæƒ©ç½šé‡å¤ï¼Œå…è®¸å¼•ç”¨åŸæ–‡
        seed=42               # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
    )
    # ä¸´æ—¶æ›¿æ¢ Settings
    original_llm = Settings.llm
    Settings.llm = llm_factual
    ask(question, base_index.as_query_engine(similarity_top_k=3))

    # åœºæ™¯ 2: å‘æ•£çš„åˆ›æ„ç”Ÿæˆ
    print("\n>>> åœºæ™¯ 2: åˆ›æ„æ¨¡å¼ (Temp=0.8, Penalty=0.5)")
    llm_creative = OpenAILike(
        model="qwen-plus",
        api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
        api_key=os.getenv("DASHSCOPE_API_KEY"),
        is_chat_model=True,
        temperature=0.8,      # é«˜æ¸©åº¦ï¼Œæ¿€å‘åˆ›é€ åŠ›
        max_tokens=1024,
        presence_penalty=0.5, # é¼“åŠ±å°è¯•æ–°è¯æ±‡ï¼Œé¿å…é™ˆè¯æ»¥è°ƒ
        # seed ä¸è®¾ç½®ï¼Œå…è®¸éšæœºå‘æŒ¥
    )
    Settings.llm = llm_creative
    ask("è¯·ä¸ºå…¬å¸å¹´ä¼šå†™ä¸€ä¸ªåˆ›æ„å¼€åœºç™½", base_index.as_query_engine(similarity_top_k=3))

    # æ¢å¤é»˜è®¤
    Settings.llm = original_llm


def main():
    """ä¸»ç¨‹åºï¼šæ‰§è¡Œæ‰€æœ‰ RAG ä¼˜åŒ–æ­¥éª¤"""
    
    print("\n" + "="*60)
    print("  RAG ç³»ç»Ÿå…¨é“¾è·¯æ€§èƒ½ä¼˜åŒ–å®éªŒ")
    print("="*60)
    
    try:
        # Step 1: åŸºå‡†æµ‹è¯•
        demo_step1_baseline()
        
        # Step 2: æ•°æ®å±‚ä¼˜åŒ–ï¼ˆé‡å»ºç´¢å¼•ï¼‰
        documents_step2, index_step2 = demo_step2_reindex()
        
        # Step 3: è§£æå±‚ä¼˜åŒ–ï¼ˆPDF -> Markdownï¼‰
        demo_step3_parsing()
        
        # Step 4: åˆ‡ç‰‡å±‚ä¼˜åŒ–ï¼ˆåˆ‡ç‰‡å™¨å¯¹æ¯”ï¼‰
        demo_step4_chunking(documents_step2)
        
        # Step 5: åµŒå…¥å±‚ä¼˜åŒ–ï¼ˆEmbedding æ¨¡å‹å¯¹æ¯”ï¼‰
        demo_step5_embedding(documents_step2)
        
        # Step 6: å­˜å‚¨å±‚ä¼˜åŒ–ï¼ˆç†è®ºè¯´æ˜ï¼‰
        demo_step6_vector_db()
        
        # Step 7: æ£€ç´¢å±‚ä¼˜åŒ–ï¼ˆè¿›é˜¶æ£€ç´¢ç­–ç•¥ï¼‰
        demo_step7_retrieval(index_step2)
        
        # Step 8: ç”Ÿæˆå±‚ä¼˜åŒ–ï¼ˆLLM å‚æ•°è°ƒä¼˜ï¼‰
        demo_step8_generation(index_step2)
        
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "="*60)
    print("  æ‰€æœ‰ä¼˜åŒ–æ­¥éª¤æ¼”ç¤ºå®Œæˆï¼")
    print("="*60)
    print("\nã€æ€»ç»“ã€‘")
    print("é€šè¿‡ 8 ä¸ªä¼˜åŒ–æ­¥éª¤ï¼Œæˆ‘ä»¬ç³»ç»ŸåŒ–åœ°æå‡äº† RAG ç³»ç»Ÿçš„è´¨é‡ã€‚")
    print("\nã€å…³é”®è¦ç‚¹ã€‘")
    print("  âœ“ æ•°æ®å±‚: æ‰©å¤§ Top-Kã€é‡å»ºç´¢å¼•")
    print("  âœ“ è§£æå±‚: PDF â†’ Markdown ä¿ç•™ç»“æ„")
    print("  âœ“ åˆ‡ç‰‡å±‚: è¯­ä¹‰åˆ‡ç‰‡ > Token åˆ‡ç‰‡")
    print("  âœ“ åµŒå…¥å±‚: V3 æ¨¡å‹ > V2 æ¨¡å‹")
    print("  âœ“ æ£€ç´¢å±‚: é—®é¢˜æ”¹å†™ + Rerank æå‡ç²¾åº¦")
    print("  âœ“ ç”Ÿæˆå±‚: äº‹å®é—®ç­”ç”¨ä½æ¸©åº¦ï¼Œåˆ›æ„ç”Ÿæˆç”¨é«˜æ¸©åº¦")
    print("\nã€ä¸‹ä¸€æ­¥ã€‘")
    print("  â†’ æ ¹æ®å…·ä½“ä¸šåŠ¡åœºæ™¯ï¼Œé€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç­–ç•¥")
    print("  â†’ å»ºç«‹è¯„ä¼°æ•°æ®é›†ï¼ŒæŒç»­ç›‘æ§ä¼˜åŒ–æ•ˆæœ")
    print("  â†’ å°†ä¼˜åŒ–ç­–ç•¥é›†æˆåˆ°ç”Ÿäº§ç¯å¢ƒ\n")


if __name__ == "__main__":
    main()
