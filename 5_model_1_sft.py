# ==============================================================================
# è¯´æ˜ï¼šæœ¬åœ°æ¨¡å‹ç›‘ç£å¾®è°ƒï¼ˆSFT - Supervised Fine-Tuningï¼‰
# ==============================================================================
# åŸºäº PEFT + Transformers æ¡†æ¶å¯¹ Qwen2.5-1.5B-Instruct æ¨¡å‹è¿›è¡Œ LoRA å¾®è°ƒï¼Œ
# é’ˆå¯¹ç‰¹å®šæ•°å­¦é—®ç­”ä»»åŠ¡ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚
#
# ã€æ ¸å¿ƒæ¦‚å¿µã€‘
# 1. **é¢„è®­ç»ƒï¼ˆPre-trainingï¼‰**ï¼š
#    - ç›®çš„ï¼šåœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ•°æ®ä¸Šå­¦ä¹ è¯­è¨€çš„é€šç”¨è¡¨å¾èƒ½åŠ›
#    - æ–¹æ³•ï¼šè‡ªç›‘ç£å­¦ä¹ ï¼ˆå¦‚ Next Token Predictionï¼‰
#    - ç»“æœï¼šå¾—åˆ°å…·å¤‡åŸºç¡€è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›çš„åŸºåº§æ¨¡å‹
#    - ç‰¹ç‚¹ï¼šè®¡ç®—é‡å¤§ï¼ˆéœ€è¦æ•°åƒå¼  GPU å¡ã€æ•°å‘¨è‡³æ•°æœˆè®­ç»ƒï¼‰
#
# 2. **å¾®è°ƒï¼ˆFine-Tuningï¼‰**ï¼š
#    - ç›®çš„ï¼šåœ¨ç‰¹å®šä»»åŠ¡çš„æ ‡æ³¨æ•°æ®ä¸Šä¼˜åŒ–æ¨¡å‹ï¼Œä½¿å…¶é€‚åº”ç‰¹å®šé¢†åŸŸæˆ–ä»»åŠ¡
#    - æ–¹æ³•ï¼šç›‘ç£å­¦ä¹ ï¼ˆä½¿ç”¨æ ‡æ³¨çš„è¾“å…¥-è¾“å‡ºå¯¹ï¼‰
#    - ç»“æœï¼šå¾—åˆ°é’ˆå¯¹ç‰¹å®šä»»åŠ¡ä¼˜åŒ–çš„æ¨¡å‹
#    - ç‰¹ç‚¹ï¼šè®¡ç®—é‡å°ï¼ˆå•å¡æˆ–å°‘é‡å¡ã€æ•°å°æ—¶è‡³æ•°å¤©ï¼‰
#
# 3. **å…¨å‚å¾®è°ƒï¼ˆFull Fine-Tuningï¼‰**ï¼š
#    - å®šä¹‰ï¼šæ›´æ–°æ¨¡å‹çš„æ‰€æœ‰å‚æ•°
#    - ä¼˜ç‚¹ï¼šæ€§èƒ½å¤©èŠ±æ¿é«˜ï¼Œå¯ä»¥å……åˆ†é€‚åº”æ–°ä»»åŠ¡
#    - ç¼ºç‚¹ï¼šæ˜¾å­˜å ç”¨å¤§ï¼ˆéœ€è¦å­˜å‚¨æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ï¼‰ï¼Œè®­ç»ƒæ…¢ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ
#
# 4. **å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFT - Parameter-Efficient Fine-Tuningï¼‰**ï¼š
#    - å®šä¹‰ï¼šåªæ›´æ–°æ¨¡å‹çš„ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œå†»ç»“å¤§éƒ¨åˆ†é¢„è®­ç»ƒæƒé‡
#    - ä»£è¡¨æ–¹æ³•ï¼š**LoRAï¼ˆLow-Rank Adaptationï¼‰**
#    - ä¼˜ç‚¹ï¼šæ˜¾å­˜å ç”¨å°ï¼Œè®­ç»ƒå¿«ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¤šä»»åŠ¡éƒ¨ç½²æ–¹ä¾¿
#    - ç¼ºç‚¹ï¼šæ€§èƒ½å¤©èŠ±æ¿ç•¥ä½äºå…¨å‚å¾®è°ƒ
#
# ã€LoRA åŸç†ã€‘
# LoRA çš„æ ¸å¿ƒæ€æƒ³ï¼šæ¨¡å‹é€‚åº”æ–°ä»»åŠ¡æ—¶ï¼Œæƒé‡æ›´æ–°çŸ©é˜µæ˜¯"ä½ç§©"çš„ï¼ˆå¯ä»¥ç”¨ä¸¤ä¸ªå°çŸ©é˜µçš„ä¹˜ç§¯è¡¨ç¤ºï¼‰ã€‚
#
# - å‡è®¾åŸå§‹æƒé‡çŸ©é˜µä¸º W âˆˆ R^(dÃ—k)ï¼ŒLoRA ä¸ç›´æ¥æ›´æ–° Wï¼Œè€Œæ˜¯å­¦ä¹ ä¸¤ä¸ªå°çŸ©é˜µï¼š
#   - A âˆˆ R^(dÃ—r)  ï¼ˆä¸‹æŠ•å½±çŸ©é˜µï¼‰
#   - B âˆˆ R^(rÃ—k)  ï¼ˆä¸ŠæŠ•å½±çŸ©é˜µï¼‰
#   å…¶ä¸­ r << min(d, k) æ˜¯ LoRA ç§©ï¼ˆrankï¼‰ï¼Œé€šå¸¸å– 4ã€8ã€16 ç­‰
#
# - å¾®è°ƒåçš„æƒé‡ä¸ºï¼šW' = W + Î± * (B Â· A)
#   - W ä¿æŒå†»ç»“ï¼ˆä¸æ›´æ–°ï¼‰
#   - åªè®­ç»ƒ A å’Œ Bï¼ˆå‚æ•°é‡ä¸º r*(d+k)ï¼Œè¿œå°äº d*kï¼‰
#   - Î± æ˜¯ç¼©æ”¾å› å­ï¼Œæ§åˆ¶ LoRA æ›´æ–°çš„å¼ºåº¦
#
# - å‚æ•°é‡å¯¹æ¯”ï¼ˆä»¥ d=4096, k=4096, r=8 ä¸ºä¾‹ï¼‰ï¼š
#   - å…¨å‚ï¼š4096 * 4096 = 16,777,216 å‚æ•°
#   - LoRAï¼š8 * (4096 + 4096) = 65,536 å‚æ•°ï¼ˆä»… 0.4%ï¼‰
#
# ã€é¢„è®¾é…ç½®è¯´æ˜ã€‘
# æœ¬è„šæœ¬å†…ç½®5ç§é¢„è®¾é…ç½®ï¼Œé€šè¿‡ --config-id å‚æ•°é€‰æ‹©ï¼š
#
# â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ ID â”‚ åç§°                â”‚ å­¦ä¹ ç‡  â”‚ Rank â”‚ Epoch â”‚ æ•°æ®é›†      â”‚ Batch â”‚ ç´¯ç§¯æ­¥æ•°â”‚æœ‰æ•ˆBatchâ”‚ä¿å­˜/è¯„ä¼°â”‚ è¯´æ˜                     â”‚
# â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ 0  â”‚ è¿‡å¤§å­¦ä¹ ç‡æµ‹è¯•      â”‚ 0.1     â”‚ 4    â”‚ 1     â”‚ train_100   â”‚ 8     â”‚ 2      â”‚ 16      â”‚ 1æ­¥    â”‚ è§‚å¯Ÿè®­ç»ƒä¸ç¨³å®š/å‘æ•£ç°è±¡  â”‚
# â”‚ 1  â”‚ å¿«é€ŸéªŒè¯ï¼ˆæ¨èï¼‰âœ¨  â”‚ 5e-5    â”‚ 4    â”‚ 1     â”‚ train_100   â”‚ 2     â”‚ 8      â”‚ 16      â”‚ 1æ­¥    â”‚ å†…å­˜ä¼˜åŒ–ï¼Œ3-5åˆ†é’Ÿå®Œæˆ    â”‚
# â”‚ 2  â”‚ å°æ•°æ®é›†é•¿è®­ç»ƒ      â”‚ 5e-5    â”‚ 4    â”‚ 50    â”‚ train_100   â”‚ 4     â”‚ 4      â”‚ 16      â”‚ 20æ­¥   â”‚ è§‚å¯Ÿè¿‡æ‹Ÿåˆç°è±¡           â”‚
# â”‚ 3  â”‚ å¤§æ•°æ®é›†æ ‡å‡†è®­ç»ƒ    â”‚ 5e-5    â”‚ 8    â”‚ 3     â”‚ train_1k    â”‚ 4     â”‚ 4      â”‚ 16      â”‚ 20æ­¥   â”‚ æ€§èƒ½/æ—¶é—´å¹³è¡¡ï¼Œæ¨è      â”‚
# â”‚ 4  â”‚ å¤§æ•°æ®é›†é•¿è®­ç»ƒ      â”‚ 5e-5    â”‚ 8    â”‚ 15    â”‚ train_1k    â”‚ 4     â”‚ 4      â”‚ 16      â”‚ 20æ­¥   â”‚ è¿½æ±‚æœ€ä½³æ€§èƒ½ï¼Œè€—æ—¶è¾ƒé•¿   â”‚
# â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# ã€é‡è¦è¯´æ˜ã€‘
# - âš¡ **å·²å¯ç”¨å†…å­˜ä¼˜åŒ–**ï¼š
#   1. Gradient Checkpointingï¼ˆèŠ‚çœ70%æ¿€æ´»å€¼å†…å­˜ï¼‰
#   2. ç¦ç”¨è®­ç»ƒä¸­è¯„ä¼°ï¼ˆèŠ‚çœ2-3GBå³°å€¼å†…å­˜ï¼‰
#   3. åªä¿ç•™æœ€ç»ˆcheckpointï¼ˆèŠ‚çœç£ç›˜ç©ºé—´ï¼‰
# - Batchï¼šå•æ¬¡å‰å‘ä¼ æ’­çš„æ ·æœ¬æ•°ï¼ˆper_device_batch_sizeï¼‰
# - ç´¯ç§¯æ­¥æ•°ï¼šæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆgradient_accumulation_stepsï¼‰
# - æœ‰æ•ˆBatchï¼šçœŸå®æ‰¹æ¬¡å¤§å° = Batch Ã— ç´¯ç§¯æ­¥æ•°
# - ğŸ’¡ é™ä½Batchã€å¢åŠ ç´¯ç§¯æ­¥æ•° = é™ä½å³°å€¼å†…å­˜å ç”¨ï¼Œä½†ä¿æŒè®­ç»ƒæ•ˆæœä¸å˜
# - ä¿å­˜ï¼šæ¯ save_steps ä¿å­˜ä¸€æ¬¡checkpoint
# - è¯„ä¼°ï¼šè®­ç»ƒå®Œæˆåç”¨ `5_model_2_eval.py` å•ç‹¬è¯„ä¼°ï¼ˆä¸å½±å“è®­ç»ƒæ•ˆæœï¼‰
# - æœ‰æ•ˆBatchè¶Šå¤§ï¼Œæ¢¯åº¦ä¼°è®¡è¶Šå‡†ç¡®ï¼Œè®­ç»ƒè¶Šç¨³å®š
# - é…ç½®å‚æ•°ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œè„šæœ¬ä¼šä¸¥æ ¼æŒ‰é…ç½®æ‰§è¡Œï¼Œä¸ä¼šåŠ¨æ€è°ƒæ•´
# - å¦‚æœé‡åˆ°å†…å­˜ä¸è¶³ï¼Œä½¿ç”¨ --force-cpu å‚æ•°å¼ºåˆ¶CPUè®­ç»ƒ
# - âœ… æ”¯æŒMPSï¼ˆApple Siliconï¼Œ18GBå¯ç”¨ï¼‰ã€CUDAï¼ˆNVIDIAï¼‰ã€CPUè®¾å¤‡
#
# ã€ä½¿ç”¨æ–¹æ³•ã€‘
# ```bash
# # åŸºæœ¬è®­ç»ƒ
# python 5_model_1_sft.py --config-id 1
#
# # è®­ç»ƒå¹¶åˆå¹¶æ¨¡å‹
# python 5_model_1_sft.py --config-id 3 --merge
# ```
#
# ã€è¾“å‡ºæ–‡ä»¶ç»“æ„ã€‘
# ```
# output/
# â”œâ”€â”€ config_0/
# â”‚   â””â”€â”€ checkpoints/             # LoRAæƒé‡checkpoints
# â”‚       â”œâ”€â”€ checkpoint-1/        # ä¿å­˜ç‚¹1
# â”‚       â”œâ”€â”€ checkpoint-2/        # ä¿å­˜ç‚¹2ï¼ˆä¿ç•™æœ€è¿‘2ä¸ªï¼‰
# â”‚       â””â”€â”€ trainer_state.json   # è®­ç»ƒçŠ¶æ€
# â”œâ”€â”€ config_1/
# â”‚   â””â”€â”€ checkpoints/
# â”œâ”€â”€ config_0_training_loss.png   # Lossæ›²çº¿
# â””â”€â”€ config_1_training_loss.png
# ```
#
# ã€ç›¸å…³è„šæœ¬ã€‘
# - `5_model_2_eval.py`: è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹æ€§èƒ½ï¼ˆå«åŸºå‡†æµ‹è¯•ï¼‰
# - `chatbot/sft_utils.py`: å…±ç”¨çš„å·¥å…·å‡½æ•°æ¨¡å—
# ==============================================================================

import os
import argparse
import torch
import glob
import shutil
import json

# æ¶ˆé™¤è®­ç»ƒè¿‡ç¨‹ä¸­çš„è­¦å‘Šä¿¡æ¯
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

from datasets import load_dataset as hf_load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
)
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from chatbot.sft_utils import plot_training_curves


# ==============================================================================
# é¢„è®¾é…ç½®ï¼š5ç§å†…ç½®é…ç½®æ–¹æ¡ˆï¼ˆå·²ä¼˜åŒ–ï¼Œå¯¹é½åŸå§‹swifté…ç½®ï¼‰
# ==============================================================================
# ã€ä¼˜åŒ–è¦ç‚¹ã€‘
# - warmup_steps=100ï¼ˆå›ºå®šæ­¥æ•°ï¼Œè€Œéæ¯”ä¾‹ï¼‰â†’ Config 3é¢„çƒ­å æ¯”å¤§ï¼Œè®­ç»ƒç›¸å¯¹ä¸è¶³
# - weight_decay=0.1ï¼ˆå¢å¼ºæ­£åˆ™åŒ–ï¼‰â†’ é˜²æ­¢å°æ•°æ®é›†è¿‡æ‹Ÿåˆ
# - lr_scheduler_type="cosine"ï¼ˆä½™å¼¦è¡°å‡ï¼‰â†’ å­¦ä¹ ç‡æ›´å¹³æ»‘ä¼˜é›…
# ==============================================================================
PRESET_CONFIGS = {
    0: {
        "name": "è¿‡å¤§å­¦ä¹ ç‡æµ‹è¯•",
        "description": "æ•…æ„è®¾ç½®è¿‡å¤§çš„å­¦ä¹ ç‡ï¼ˆ0.1ï¼‰ï¼Œè§‚å¯Ÿè®­ç»ƒä¸ç¨³å®šã€losséœ‡è¡æˆ–å‘æ•£ç°è±¡",
        "learning_rate": 0.1,  # è¿‡å¤§çš„å­¦ä¹ ç‡ï¼ˆæ­£å¸¸å€¼çš„2000å€ï¼‰
        "lora_rank": 4,
        "num_train_epochs": 1,
        "train_file": "./resources/train_100.jsonl",
        "batch_size": 8,
        "gradient_accumulation_steps": 2,
        "max_length": 512,
        "save_steps": 1,  # æ¯æ­¥éƒ½ä¿å­˜ï¼Œè§‚å¯Ÿè®­ç»ƒåˆæœŸçš„ç»†èŠ‚å˜åŒ–
        "split_ratio": 0.1,
    },
    1: {
        "name": "å¿«é€ŸéªŒè¯ï¼ˆæ¨èï¼‰",
        "description": "ä½¿ç”¨æ ‡å‡†é…ç½®å¿«é€ŸéªŒè¯æµç¨‹ï¼ˆ1è½®è®­ç»ƒï¼‰",
        "learning_rate": 5e-5,  # æ ‡å‡†å­¦ä¹ ç‡
        "lora_rank": 4,  # å°ç§©ï¼Œé€‚åˆ100æ ·æœ¬çš„å°æ•°æ®é›†
        "num_train_epochs": 1,  # ä»…1è½®ï¼Œç”¨äºå¿«é€ŸéªŒè¯æµç¨‹å¯è¡Œæ€§
        "train_file": "./resources/train_100.jsonl",
        "batch_size": 8,
        "gradient_accumulation_steps": 2,  # æœ‰æ•ˆæ‰¹æ¬¡=16
        "max_length": 512,
        "save_steps": 1,  # æ¯æ­¥éƒ½ä¿å­˜ï¼Œä¾¿äºè§‚å¯Ÿè®­ç»ƒåˆæœŸ
        "split_ratio": 0.1,  # 90è®­ç»ƒ+10éªŒè¯
    },
    2: {
        "name": "å°æ•°æ®é›†é•¿è®­ç»ƒ",
        "description": "åœ¨å°æ•°æ®é›†ä¸Šè¿›è¡Œé•¿æ—¶é—´è®­ç»ƒï¼ˆ50è½®ï¼‰ï¼Œè§‚å¯Ÿè¿‡æ‹Ÿåˆç°è±¡",
        "learning_rate": 5e-5,
        "lora_rank": 4,
        "num_train_epochs": 50,  # å……åˆ†è®­ç»ƒï¼Œä½¿æ¨¡å‹åœ¨å°æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆ
        "train_file": "./resources/train_100.jsonl",
        "batch_size": 8,
        "gradient_accumulation_steps": 2,
        "max_length": 512,
        "save_steps": 20,  # æ¯20æ­¥ä¿å­˜/è¯„ä¼°ï¼ˆæ€»300æ­¥ï¼Œä¿å­˜15æ¬¡ï¼‰
        "split_ratio": 0.1,  # ç”¨äºè§‚å¯Ÿè®­ç»ƒLossä¸éªŒè¯Lossçš„åˆ†ç¦»
    },
    3: {
        "name": "å¤§æ•°æ®é›†æ ‡å‡†è®­ç»ƒ",
        "description": "ä½¿ç”¨1kæ ·æœ¬è¿›è¡Œ3è½®è®­ç»ƒï¼Œç”±äºwarmupå æ¯”å¤§ï¼ˆ100/178æ­¥=56%ï¼‰ï¼Œé¢„æœŸè®­ç»ƒä¸å¤Ÿå……åˆ†ï¼ˆæ¬ æ‹Ÿåˆç¤ºä¾‹ï¼‰",
        "learning_rate": 5e-5,
        "lora_rank": 8,  # è¾ƒå¤§çš„ç§©ï¼ŒåŒ¹é…æ›´å¤šçš„è®­ç»ƒæ•°æ®
        "num_train_epochs": 3,  # æ ‡å‡†è®­ç»ƒè½®æ•°ï¼Œå¹³è¡¡æ•ˆæœä¸é€Ÿåº¦
        "train_file": "./resources/train_1k.jsonl",
        "batch_size": 8,
        "gradient_accumulation_steps": 2,
        "max_length": 512,
        "save_steps": 20,
        "split_ratio": 0.05,  # 950è®­ç»ƒ+50éªŒè¯ï¼ŒéªŒè¯é›†æ¯”ä¾‹é€‚ä¸­
    },
    4: {
        "name": "å¤§æ•°æ®é›†é•¿è®­ç»ƒ",
        "description": "ä½¿ç”¨1kæ ·æœ¬è¿›è¡Œ15è½®è®­ç»ƒï¼Œwarmupå æ¯”åˆç†ï¼ˆ100/890æ­¥=11%ï¼‰ï¼Œå……åˆ†è®­ç»ƒè¾¾åˆ°æœ€ä½³æ•ˆæœ",
        "learning_rate": 5e-5,
        "lora_rank": 8,
        "num_train_epochs": 15,  # å……åˆ†è®­ç»ƒï¼Œè¿½æ±‚æœ€ä½³æ•ˆæœ
        "train_file": "./resources/train_1k.jsonl",
        "batch_size": 8,
        "gradient_accumulation_steps": 2,
        "max_length": 512,
        "save_steps": 20,  # æ€»1000æ­¥ï¼Œä¿å­˜50æ¬¡
        "split_ratio": 0.05,
    },
}


def load_and_prepare_dataset(train_file, split_ratio=0.1):
    """
    åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†ï¼ˆGPUè®­ç»ƒï¼Œå§‹ç»ˆä½¿ç”¨éªŒè¯é›†ï¼‰ã€‚
    
    ã€å‚æ•°ã€‘
    - train_file: è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSONLæ ¼å¼ï¼‰
    - split_ratio: éªŒè¯é›†åˆ†å‰²æ¯”ä¾‹ï¼ˆ>0ï¼Œæ‰€æœ‰é…ç½®éƒ½æœ‰éªŒè¯é›†ï¼‰
    
    ã€è¿”å›ã€‘
    - train_dataset: è®­ç»ƒæ•°æ®é›†
    - eval_dataset: éªŒè¯æ•°æ®é›†
    """
    # è¯»å–JSONLæ–‡ä»¶
    data = []
    with open(train_file, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    
    # è½¬æ¢ä¸ºHuggingFace Datasetæ ¼å¼
    dataset = hf_load_dataset('json', data_files={'train': train_file}, split='train')
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆæ‰€æœ‰é…ç½®éƒ½ä½¿ç”¨éªŒè¯é›†ï¼‰
    split_dataset = dataset.train_test_split(test_size=split_ratio, seed=42)
    train_dataset = split_dataset['train']
    eval_dataset = split_dataset['test']
    
    return train_dataset, eval_dataset


def tokenize_function(examples, tokenizer, max_length=512):
    """
    æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼šå°†å¯¹è¯æ ¼å¼è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥ã€‚
    
    ã€å‚æ•°ã€‘
    - examples: æ•°æ®æ ·æœ¬
    - tokenizer: tokenizerå®ä¾‹
    - max_length: æœ€å¤§åºåˆ—é•¿åº¦
    
    ã€è¿”å›ã€‘
    - å¤„ç†åçš„æ•°æ®å­—å…¸
    """
    # æå–messageså¹¶æ ¼å¼åŒ–
    model_inputs = {"input_ids": [], "attention_mask": [], "labels": []}
    
    for messages in examples['messages']:
        # æ„å»ºå®Œæ•´å¯¹è¯
        # messagesæ ¼å¼ï¼š[{role: system, content: ...}, {role: user, content: ...}, {role: assistant, content: ...}]
        
        # è·³è¿‡systemæ¶ˆæ¯ï¼Œåªå¤„ç†userå’Œassistant
        user_msg = next((m['content'] for m in messages if m['role'] == 'user'), '')
        assistant_msg = next((m['content'] for m in messages if m['role'] == 'assistant'), '')
        
        # ä½¿ç”¨chat templateæ ¼å¼åŒ–ï¼ˆå¦‚æœtokenizeræ”¯æŒï¼‰
        if hasattr(tokenizer, 'apply_chat_template'):
            # ä½¿ç”¨æ ‡å‡†chat template
            formatted_messages = [
                {"role": "user", "content": user_msg},
                {"role": "assistant", "content": assistant_msg}
            ]
            text = tokenizer.apply_chat_template(
                formatted_messages,
                tokenize=False,
                add_generation_prompt=False
            )
        else:
            # ç®€å•æ‹¼æ¥
            text = f"User: {user_msg}\nAssistant: {assistant_msg}"
        
        # Tokenize
        tokenized = tokenizer(
            text,
            max_length=max_length,
            truncation=True,
            padding=False,  # åŠ¨æ€paddingç”±DataCollatorå¤„ç†
        )
        
        model_inputs["input_ids"].append(tokenized["input_ids"])
        model_inputs["attention_mask"].append(tokenized["attention_mask"])
        model_inputs["labels"].append(tokenized["input_ids"].copy())  # labelsä¸input_idsç›¸åŒ
    
    return model_inputs


def run_fine_tune(config_id, model_path="./model"):
    """
    è¿è¡Œ LoRA å¾®è°ƒå®éªŒã€‚
    
    ã€åŠŸèƒ½ã€‘
    æ ¹æ®é…ç½®IDè‡ªåŠ¨é€‰æ‹©é¢„è®¾å‚æ•°è¿›è¡Œå¾®è°ƒï¼Œè¾“å‡ºæ–‡ä»¶æŒ‰é…ç½®IDåŒºåˆ†ã€‚
    
    ã€å‚æ•°ã€‘
    - config_id: é…ç½®IDï¼ˆ0-4ï¼‰
    - model_path: åŸºåº§æ¨¡å‹è·¯å¾„
    
    ã€è¿”å›ã€‘
    - checkpoint_path: å¾®è°ƒåçš„ checkpoint è·¯å¾„
    - loss_curve_path: Loss æ›²çº¿å›¾ç‰‡è·¯å¾„
    """
    if config_id not in PRESET_CONFIGS:
        print(f"âŒ é”™è¯¯ï¼šæ— æ•ˆçš„é…ç½®ID {config_id}ï¼Œè¯·é€‰æ‹© 0-4 ä¹‹é—´çš„æ•´æ•°")
        return None, None
    
    config = PRESET_CONFIGS[config_id]
    
    print("\n" + "ğŸ“ " + "="*76 + " ğŸ“")
    print("    æœ¬åœ°æ¨¡å‹ç›‘ç£å¾®è°ƒï¼ˆSFT - GPUï¼‰")
    print(f"    é…ç½®ID: {config_id} - {config['name']}")
    print("ğŸ“ " + "="*76 + " ğŸ“")
    
    if not os.path.exists(config['train_file']):
        print(f"âŒ é”™è¯¯ï¼šè®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨äº {config['train_file']}")
        return None, None
    
    # æ£€æµ‹GPUè®¾å¤‡
    if not torch.cuda.is_available():
        print("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°CUDA GPUï¼Œæœ¬è„šæœ¬ä»…æ”¯æŒGPUè®­ç»ƒ")
        return None, None
    
    device = torch.device("cuda")
    device_name = f"CUDA ({torch.cuda.get_device_name(0)})"
    
    # ä¸¥æ ¼ä½¿ç”¨é…ç½®ä¸­çš„ batch_size å’Œ gradient_accumulation_steps
    batch_size = config['batch_size']
    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 4)
    effective_batch_size = batch_size * gradient_accumulation_steps
    
    print(f"\nğŸ“ é…ç½®è¯´æ˜: {config['description']}")
    print(f"\nğŸ“Š è®­ç»ƒå‚æ•°:")
    print(f"   - GPU: {device_name}")
    print(f"   - å­¦ä¹ ç‡: {config['learning_rate']}")
    print(f"   - Warmupæ­¥æ•°: 100æ­¥ï¼ˆå›ºå®šï¼Œå¯¹é½swifté…ç½®ï¼‰")
    print(f"   - å­¦ä¹ ç‡è¡°å‡: Cosineï¼ˆä½™å¼¦è¡°å‡ï¼‰")
    print(f"   - æ­£åˆ™åŒ–å¼ºåº¦: Weight Decay = 0.1")
    print(f"   - LoRA Rank: {config['lora_rank']}")
    print(f"   - è®­ç»ƒè½®æ•°: {config['num_train_epochs']}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
    print(f"   - æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}")
    print(f"   - éªŒè¯é›†æ¯”ä¾‹: {config.get('split_ratio', 0.1)*100:.0f}%")
    print(f"   - è®­ç»ƒæ•°æ®: {config['train_file']}")
    
    # é…ç½®è¾“å‡ºç›®å½•
    config_dir = f"./output/config_{config_id}"
    output_dir = os.path.join(config_dir, "checkpoints")
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {config_dir}")
    print("="*80)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    print("="*80)
    
    try:
        # ========== æ­¥éª¤1ï¼šåŠ è½½æ¨¡å‹å’Œtokenizer ==========
        print("\n[1/5] åŠ è½½æ¨¡å‹å’Œtokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        # è®¾ç½®pad_tokenï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹åˆ°GPUï¼ˆä½¿ç”¨fp16åŠ é€Ÿï¼‰
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map={"": device},
            trust_remote_code=True,
        )
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° {device_name}")
        
        # ğŸ”¥ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰ä»¥èŠ‚çœå†…å­˜
        # åŸç†ï¼šä¸ä¿å­˜æ‰€æœ‰ä¸­é—´æ¿€æ´»å€¼ï¼Œåå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—
        # æ•ˆæœï¼šèŠ‚çœ ~70% æ¿€æ´»å€¼å†…å­˜ï¼Œè®­ç»ƒé€Ÿåº¦ä¸‹é™ 10-20%
        # ä¸å½±å“è®­ç»ƒæ•ˆæœï¼Œåªæ˜¯ç”¨æ—¶é—´æ¢ç©ºé—´
        model.gradient_checkpointing_enable()
        print("âš¡ å·²å¯ç”¨ Gradient Checkpointingï¼ˆå†…å­˜ä¼˜åŒ–ï¼Œä¸å½±å“è®­ç»ƒæ•ˆæœï¼‰")
        
        # ========== æ­¥éª¤2ï¼šé…ç½®LoRA ==========
        print("\n[2/5] é…ç½®LoRA...")
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=config['lora_rank'],  # LoRAç§©ï¼šæ§åˆ¶å¯è®­ç»ƒå‚æ•°é‡
            lora_alpha=config['lora_rank'] * 2,  # LoRAç¼©æ”¾ç³»æ•°ï¼šé€šå¸¸ä¸ºrankçš„2å€
            lora_dropout=0.05,  # Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],  # åº”ç”¨åˆ°æ‰€æœ‰çº¿æ€§å±‚
            bias="none",
        )
        
        # åº”ç”¨LoRA
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()  # æ‰“å°å¯è®­ç»ƒå‚æ•°æ•°é‡
        
        # ========== æ­¥éª¤3ï¼šåŠ è½½å’Œå‡†å¤‡æ•°æ®é›† ==========
        print("\n[3/5] åŠ è½½æ•°æ®é›†...")
        split_ratio = config.get('split_ratio', 0.1)
        train_dataset, eval_dataset = load_and_prepare_dataset(
            config['train_file'],
            split_ratio=split_ratio
        )
        
        print(f"âœ… è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        print(f"âœ… éªŒè¯æ ·æœ¬: {len(eval_dataset)}")
        
        # æ•°æ®é¢„å¤„ç†
        print("   é¢„å¤„ç†æ•°æ®...")
        tokenize_fn = lambda examples: tokenize_function(examples, tokenizer, config['max_length'])
        
        train_dataset = train_dataset.map(
            tokenize_fn,
            batched=True,
            remove_columns=train_dataset.column_names,
            desc="Tokenizing train dataset"
        )
        
        eval_dataset = eval_dataset.map(
            tokenize_fn,
            batched=True,
            remove_columns=eval_dataset.column_names,
            desc="Tokenizing eval dataset"
        )
        
        # ========== æ­¥éª¤4ï¼šé…ç½®Trainer ==========
        print("\n[4/5] é…ç½®è®­ç»ƒå™¨...")
        
        # å¯¹äºçŸ­è®­ç»ƒï¼ˆ1è½®ï¼‰ï¼Œä¿ç•™æ‰€æœ‰checkpointä»¥å®Œæ•´è®°å½•è®­ç»ƒè¿‡ç¨‹
        # å¯¹äºé•¿è®­ç»ƒï¼ˆå¤šè½®ï¼‰ï¼Œåªä¿ç•™æœ€è¿‘1ä¸ªcheckpointèŠ‚çœç£ç›˜
        save_total_limit = None if config['num_train_epochs'] == 1 else 1
        
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=config['num_train_epochs'],
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=config['learning_rate'],
            warmup_steps=100,  # å›ºå®š100æ­¥é¢„çƒ­ï¼Œä¸åŸå§‹swifté…ç½®å¯¹é½
            weight_decay=0.1,  # å¢å¼ºL2æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            lr_scheduler_type="cosine",  # ä½™å¼¦è¡°å‡ï¼Œå­¦ä¹ ç‡æ›´å¹³æ»‘
            logging_steps=config['save_steps'],
            save_steps=config['save_steps'],
            save_strategy="steps",
            save_total_limit=save_total_limit,  # çŸ­è®­ç»ƒä¿ç•™å…¨éƒ¨ï¼Œé•¿è®­ç»ƒåªä¿ç•™æœ€è¿‘1ä¸ª
            # è®­ç»ƒä¸­è¯„ä¼°é…ç½®ï¼ˆæ‰€æœ‰é…ç½®éƒ½æœ‰éªŒè¯é›†ï¼‰
            eval_strategy="steps",
            eval_steps=config['save_steps'],
            load_best_model_at_end=True,  # è®­ç»ƒç»“æŸæ—¶è‡ªåŠ¨åŠ è½½éªŒè¯Lossæœ€ä½çš„æ¨¡å‹
            metric_for_best_model="loss",
            fp16=True,  # åŠç²¾åº¦è®­ç»ƒï¼šèŠ‚çœæ˜¾å­˜ï¼ŒåŠ é€Ÿè®¡ç®—ï¼ˆA10æ”¯æŒï¼‰
            report_to=[],
            remove_unused_columns=False,
            # GPUä¼˜åŒ–å‚æ•°
            dataloader_num_workers=4,  # å¤šè¿›ç¨‹å¹¶è¡ŒåŠ è½½æ•°æ®
            dataloader_pin_memory=True,  # å›ºå®šå†…å­˜é¡µï¼ŒåŠ é€ŸCPUâ†’GPUæ•°æ®ä¼ è¾“
            gradient_checkpointing=True,  # æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼šèŠ‚çœ70%æ¿€æ´»å€¼æ˜¾å­˜ï¼Œè®­ç»ƒé€Ÿåº¦é™ä½10-20%
        )
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=tokenizer,
            model=model,
            padding=True,
        )
        
        # åˆ›å»ºTrainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
        )
        
        # ========== æ­¥éª¤5ï¼šå¼€å§‹è®­ç»ƒ ==========
        print("\n[5/5] å¼€å§‹è®­ç»ƒ...")
        print("="*80)
        
        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        
        train_result = trainer.train()
        
        print("\nâœ… å¾®è°ƒå®Œæˆï¼")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆå·²ç”±load_best_model_at_endè‡ªåŠ¨åŠ è½½éªŒè¯Lossæœ€ä½çš„æ¨¡å‹ï¼‰
        best_model_dir = os.path.join(config_dir, "best_model")
        print(f"   ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆéªŒè¯Lossæœ€ä½ï¼‰åˆ°: {best_model_dir}")
        trainer.save_model(best_model_dir)
        trainer.save_state()
        checkpoint_path = best_model_dir
        
        # æç¤ºï¼šè®­ç»ƒè¿‡ç¨‹ä¸­çš„checkpointsä»ä¿ç•™åœ¨checkpointsç›®å½•
        print(f"   è®­ç»ƒè¿‡ç¨‹checkpoints: {output_dir}")
        print(f"   è¯„ä¼°ç”¨æœ€ä½³æ¨¡å‹: {checkpoint_path}")
        
        # ç»˜åˆ¶ loss æ›²çº¿
        loss_curve_path = f"./output/config_{config_id}_training_loss.png"
        plot_training_curves(checkpoint_path, output_dir="./output", 
                            output_filename=f"config_{config_id}_training_loss.png")
        print(f"   Loss æ›²çº¿: {loss_curve_path}")
        
        return checkpoint_path, loss_curve_path
            
    except Exception as e:
        print(f"\nâŒ å¾®è°ƒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None


def main():
    """
    ä¸»å‡½æ•°ï¼šè§£æå‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œå¾®è°ƒã€‚
    """
    parser = argparse.ArgumentParser(
        description="æœ¬åœ°æ¨¡å‹ç›‘ç£å¾®è°ƒï¼ˆSFT - GPUï¼‰- åŸºäº PEFT + LoRA",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
é¢„è®¾é…ç½®è¯´æ˜ï¼ˆå·²ä¼˜åŒ–ï¼Œå¯¹é½åŸå§‹swifté…ç½®ï¼‰ï¼š
  ID 0: è¿‡å¤§å­¦ä¹ ç‡æµ‹è¯• - è§‚å¯Ÿè®­ç»ƒä¸ç¨³å®š/å‘æ•£ç°è±¡
  ID 1: å¿«é€ŸéªŒè¯ï¼ˆæ¨èï¼‰- é¦–æ¬¡è¿è¡Œï¼Œ1åˆ†é’Ÿå®Œæˆ
  ID 2: å°æ•°æ®é›†é•¿è®­ç»ƒ - è§‚å¯Ÿè¿‡æ‹Ÿåˆç°è±¡ï¼ˆ50è½®ï¼‰
  ID 3: å¤§æ•°æ®é›†æ ‡å‡†è®­ç»ƒ - 3è½®è®­ç»ƒï¼Œé¢„æœŸæ¬ æ‹Ÿåˆï¼ˆæ•™å­¦ç”¨ï¼‰
  ID 4: å¤§æ•°æ®é›†é•¿è®­ç»ƒ - 15è½®è®­ç»ƒï¼Œå……åˆ†è®­ç»ƒè¾¾åˆ°æœ€ä½³æ•ˆæœ

å…³é”®ä¼˜åŒ–ï¼ˆå¯¹é½swiftï¼‰ï¼š
  - Warmup: å›ºå®š100æ­¥ï¼ˆè€Œéæ¯”ä¾‹ï¼‰ï¼ŒConfig 3é¢„çƒ­å æ¯”å¤§
  - å­¦ä¹ ç‡è¡°å‡: Cosineä½™å¼¦è¡°å‡ï¼ˆæ›´å¹³æ»‘ï¼‰
  - æ­£åˆ™åŒ–: Weight Decay = 0.1ï¼ˆå¢å¼ºç‰ˆï¼‰

ä½¿ç”¨ç¤ºä¾‹ï¼š
  python 5_model_1_sft.py --config-id 1  # å¿«é€ŸéªŒè¯
  python 5_model_1_sft.py --config-id 3  # æ ‡å‡†è®­ç»ƒï¼ˆæ¬ æ‹Ÿåˆç¤ºä¾‹ï¼‰
  python 5_model_1_sft.py --config-id 4  # å……åˆ†è®­ç»ƒï¼ˆæœ€ä½³æ•ˆæœï¼‰
        """
    )
    
    # å‚æ•°
    parser.add_argument("--config-id", type=int, required=True,
                       help="é…ç½®IDï¼ˆ0-4ï¼‰ï¼Œæ¯ä¸ªIDå¯¹åº”ä¸€ç»„é¢„è®¾çš„è®­ç»ƒå‚æ•°")
    parser.add_argument("--model-path", type=str, default="./model",
                       help="åŸºåº§æ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤: ./modelï¼‰")
    
    args = parser.parse_args()
    
    # éªŒè¯é…ç½®ID
    if args.config_id not in PRESET_CONFIGS:
        print(f"\nâŒ é”™è¯¯ï¼šæ— æ•ˆçš„é…ç½®ID {args.config_id}")
        print("è¯·é€‰æ‹© 0-4 ä¹‹é—´çš„æ•´æ•°ï¼š")
        for cid, cfg in PRESET_CONFIGS.items():
            print(f"  {cid}: {cfg['name']}")
        return
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.model_path):
        print(f"\nâŒ é”™è¯¯ï¼šæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨äº {args.model_path}")
        print("è¯·å…ˆä¸‹è½½æ¨¡å‹ï¼š")
        print(f"  mkdir -p {args.model_path}")
        print(f"  modelscope download --model qwen/Qwen2.5-1.5B-Instruct --local_dir '{args.model_path}'")
        return
    
    # è¿è¡Œå¾®è°ƒ
    checkpoint_path, loss_curve_path = run_fine_tune(
        config_id=args.config_id,
        model_path=args.model_path
    )
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "ğŸ† " + "="*76 + " ğŸ†")
    print("    è®­ç»ƒå®Œæˆ")
    print("ğŸ† " + "="*76 + " ğŸ†")
    
    if checkpoint_path:
        print(f"\nâœ… é…ç½® {args.config_id} è®­ç»ƒæˆåŠŸï¼")
        print(f"ğŸ“ LoRA Checkpoint: {checkpoint_path}")
        if loss_curve_path:
            print(f"ğŸ“ˆ Loss æ›²çº¿: {loss_curve_path}")
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œï¼š")
        print(f"   # è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹")
        print(f"   python 5_model_2_eval.py --config-id {args.config_id}")
        print(f"\n   # åˆå¹¶ LoRA æƒé‡åˆ°åŸºåº§æ¨¡å‹")
        print(f"   python 5_model_3_merge.py --config-id {args.config_id}")
    else:
        print(f"\nâŒ é…ç½® {args.config_id} è®­ç»ƒå¤±è´¥")
    
    print("\n" + "="*80)
    print("ğŸ‰ æµç¨‹ç»“æŸ")
    print("="*80)


if __name__ == "__main__":
    main()
