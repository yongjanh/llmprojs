# ==============================================================================
# è¯´æ˜ï¼šæœ¬åœ°æ¨¡å‹ç›‘ç£å¾®è°ƒï¼ˆSFT - Supervised Fine-Tuningï¼‰
# ==============================================================================
# åŸºäº PEFT + Transformers æ¡†æ¶å¯¹ Qwen2.5-1.5B-Instruct æ¨¡å‹è¿›è¡Œ LoRA å¾®è°ƒï¼Œ
# é’ˆå¯¹ç‰¹å®šæ•°å­¦é—®ç­”ä»»åŠ¡ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚
#
# ã€æ ¸å¿ƒæ¦‚å¿µã€‘
# 1. **é¢„è®­ç»ƒï¼ˆPre-trainingï¼‰**ï¼š
#    - ç›®çš„ï¼šåœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ•°æ®ä¸Šå­¦ä¹ è¯­è¨€çš„é€šç”¨è¡¨å¾èƒ½åŠ›
#    - æ–¹æ³•ï¼šè‡ªç›‘ç£å­¦ä¹ ï¼ˆå¦‚ Next Token Predictionï¼‰
#    - ç»“æœï¼šå¾—åˆ°å…·å¤‡åŸºç¡€è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›çš„åŸºåº§æ¨¡å‹
#    - ç‰¹ç‚¹ï¼šè®¡ç®—é‡å¤§ï¼ˆéœ€è¦æ•°åƒå¼  GPU å¡ã€æ•°å‘¨è‡³æ•°æœˆè®­ç»ƒï¼‰
#
# 2. **å¾®è°ƒï¼ˆFine-Tuningï¼‰**ï¼š
#    - ç›®çš„ï¼šåœ¨ç‰¹å®šä»»åŠ¡çš„æ ‡æ³¨æ•°æ®ä¸Šä¼˜åŒ–æ¨¡å‹ï¼Œä½¿å…¶é€‚åº”ç‰¹å®šé¢†åŸŸæˆ–ä»»åŠ¡
#    - æ–¹æ³•ï¼šç›‘ç£å­¦ä¹ ï¼ˆä½¿ç”¨æ ‡æ³¨çš„è¾“å…¥-è¾“å‡ºå¯¹ï¼‰
#    - ç»“æœï¼šå¾—åˆ°é’ˆå¯¹ç‰¹å®šä»»åŠ¡ä¼˜åŒ–çš„æ¨¡å‹
#    - ç‰¹ç‚¹ï¼šè®¡ç®—é‡å°ï¼ˆå•å¡æˆ–å°‘é‡å¡ã€æ•°å°æ—¶è‡³æ•°å¤©ï¼‰
#
# 3. **å…¨å‚å¾®è°ƒï¼ˆFull Fine-Tuningï¼‰**ï¼š
#    - å®šä¹‰ï¼šæ›´æ–°æ¨¡å‹çš„æ‰€æœ‰å‚æ•°
#    - ä¼˜ç‚¹ï¼šæ€§èƒ½å¤©èŠ±æ¿é«˜ï¼Œå¯ä»¥å……åˆ†é€‚åº”æ–°ä»»åŠ¡
#    - ç¼ºç‚¹ï¼šæ˜¾å­˜å ç”¨å¤§ï¼ˆéœ€è¦å­˜å‚¨æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ï¼‰ï¼Œè®­ç»ƒæ…¢ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ
#
# 4. **å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFT - Parameter-Efficient Fine-Tuningï¼‰**ï¼š
#    - å®šä¹‰ï¼šåªæ›´æ–°æ¨¡å‹çš„ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œå†»ç»“å¤§éƒ¨åˆ†é¢„è®­ç»ƒæƒé‡
#    - ä»£è¡¨æ–¹æ³•ï¼š**LoRAï¼ˆLow-Rank Adaptationï¼‰**
#    - ä¼˜ç‚¹ï¼šæ˜¾å­˜å ç”¨å°ï¼Œè®­ç»ƒå¿«ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¤šä»»åŠ¡éƒ¨ç½²æ–¹ä¾¿
#    - ç¼ºç‚¹ï¼šæ€§èƒ½å¤©èŠ±æ¿ç•¥ä½äºå…¨å‚å¾®è°ƒ
#
# ã€LoRA åŸç†ã€‘
# LoRA çš„æ ¸å¿ƒæ€æƒ³ï¼šæ¨¡å‹é€‚åº”æ–°ä»»åŠ¡æ—¶ï¼Œæƒé‡æ›´æ–°çŸ©é˜µæ˜¯"ä½ç§©"çš„ï¼ˆå¯ä»¥ç”¨ä¸¤ä¸ªå°çŸ©é˜µçš„ä¹˜ç§¯è¡¨ç¤ºï¼‰ã€‚
#
# - å‡è®¾åŸå§‹æƒé‡çŸ©é˜µä¸º W âˆˆ R^(dÃ—k)ï¼ŒLoRA ä¸ç›´æ¥æ›´æ–° Wï¼Œè€Œæ˜¯å­¦ä¹ ä¸¤ä¸ªå°çŸ©é˜µï¼š
#   - A âˆˆ R^(dÃ—r)  ï¼ˆä¸‹æŠ•å½±çŸ©é˜µï¼‰
#   - B âˆˆ R^(rÃ—k)  ï¼ˆä¸ŠæŠ•å½±çŸ©é˜µï¼‰
#   å…¶ä¸­ r << min(d, k) æ˜¯ LoRA ç§©ï¼ˆrankï¼‰ï¼Œé€šå¸¸å– 4ã€8ã€16 ç­‰
#
# - å¾®è°ƒåçš„æƒé‡ä¸ºï¼šW' = W + Î± * (B Â· A)
#   - W ä¿æŒå†»ç»“ï¼ˆä¸æ›´æ–°ï¼‰
#   - åªè®­ç»ƒ A å’Œ Bï¼ˆå‚æ•°é‡ä¸º r*(d+k)ï¼Œè¿œå°äº d*kï¼‰
#   - Î± æ˜¯ç¼©æ”¾å› å­ï¼Œæ§åˆ¶ LoRA æ›´æ–°çš„å¼ºåº¦
#
# - å‚æ•°é‡å¯¹æ¯”ï¼ˆä»¥ d=4096, k=4096, r=8 ä¸ºä¾‹ï¼‰ï¼š
#   - å…¨å‚ï¼š4096 * 4096 = 16,777,216 å‚æ•°
#   - LoRAï¼š8 * (4096 + 4096) = 65,536 å‚æ•°ï¼ˆä»… 0.4%ï¼‰
#
# ã€é¢„è®¾é…ç½®è¯´æ˜ã€‘
# æœ¬è„šæœ¬å†…ç½®5ç§é¢„è®¾é…ç½®ï¼Œé€šè¿‡ --config-id å‚æ•°é€‰æ‹©ï¼š
#
# â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ ID â”‚ åç§°                â”‚ å­¦ä¹ ç‡  â”‚ Rank â”‚ Epoch â”‚ æ•°æ®é›†      â”‚ Batch â”‚ ç´¯ç§¯æ­¥æ•°â”‚æœ‰æ•ˆBatchâ”‚ä¿å­˜/è¯„ä¼°â”‚ è¯´æ˜                     â”‚
# â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ 0  â”‚ è¿‡å¤§å­¦ä¹ ç‡æµ‹è¯•      â”‚ 0.1     â”‚ 4    â”‚ 1     â”‚ train_100   â”‚ 8     â”‚ 2      â”‚ 16      â”‚ 1æ­¥    â”‚ è§‚å¯Ÿè®­ç»ƒä¸ç¨³å®š/å‘æ•£ç°è±¡  â”‚
# â”‚ 1  â”‚ å¿«é€ŸéªŒè¯ï¼ˆæ¨èï¼‰âœ¨  â”‚ 5e-5    â”‚ 4    â”‚ 1     â”‚ train_100   â”‚ 2     â”‚ 8      â”‚ 16      â”‚ 1æ­¥    â”‚ å†…å­˜ä¼˜åŒ–ï¼Œ3-5åˆ†é’Ÿå®Œæˆ    â”‚
# â”‚ 2  â”‚ å°æ•°æ®é›†é•¿è®­ç»ƒ      â”‚ 5e-5    â”‚ 4    â”‚ 50    â”‚ train_100   â”‚ 4     â”‚ 4      â”‚ 16      â”‚ 20æ­¥   â”‚ è§‚å¯Ÿè¿‡æ‹Ÿåˆç°è±¡           â”‚
# â”‚ 3  â”‚ å¤§æ•°æ®é›†æ ‡å‡†è®­ç»ƒ    â”‚ 5e-5    â”‚ 8    â”‚ 3     â”‚ train_1k    â”‚ 4     â”‚ 4      â”‚ 16      â”‚ 20æ­¥   â”‚ æ€§èƒ½/æ—¶é—´å¹³è¡¡ï¼Œæ¨è      â”‚
# â”‚ 4  â”‚ å¤§æ•°æ®é›†é•¿è®­ç»ƒ      â”‚ 5e-5    â”‚ 8    â”‚ 15    â”‚ train_1k    â”‚ 4     â”‚ 4      â”‚ 16      â”‚ 20æ­¥   â”‚ è¿½æ±‚æœ€ä½³æ€§èƒ½ï¼Œè€—æ—¶è¾ƒé•¿   â”‚
# â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# ã€é‡è¦è¯´æ˜ã€‘
# - âš¡ **å·²å¯ç”¨å†…å­˜ä¼˜åŒ–**ï¼š
#   1. Gradient Checkpointingï¼ˆèŠ‚çœ70%æ¿€æ´»å€¼å†…å­˜ï¼‰
#   2. ç¦ç”¨è®­ç»ƒä¸­è¯„ä¼°ï¼ˆèŠ‚çœ2-3GBå³°å€¼å†…å­˜ï¼‰
#   3. åªä¿ç•™æœ€ç»ˆcheckpointï¼ˆèŠ‚çœç£ç›˜ç©ºé—´ï¼‰
# - Batchï¼šå•æ¬¡å‰å‘ä¼ æ’­çš„æ ·æœ¬æ•°ï¼ˆper_device_batch_sizeï¼‰
# - ç´¯ç§¯æ­¥æ•°ï¼šæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆgradient_accumulation_stepsï¼‰
# - æœ‰æ•ˆBatchï¼šçœŸå®æ‰¹æ¬¡å¤§å° = Batch Ã— ç´¯ç§¯æ­¥æ•°
# - ğŸ’¡ é™ä½Batchã€å¢åŠ ç´¯ç§¯æ­¥æ•° = é™ä½å³°å€¼å†…å­˜å ç”¨ï¼Œä½†ä¿æŒè®­ç»ƒæ•ˆæœä¸å˜
# - ä¿å­˜ï¼šæ¯ save_steps ä¿å­˜ä¸€æ¬¡checkpoint
# - è¯„ä¼°ï¼šè®­ç»ƒå®Œæˆåç”¨ `5_model_2_eval.py` å•ç‹¬è¯„ä¼°ï¼ˆä¸å½±å“è®­ç»ƒæ•ˆæœï¼‰
# - æœ‰æ•ˆBatchè¶Šå¤§ï¼Œæ¢¯åº¦ä¼°è®¡è¶Šå‡†ç¡®ï¼Œè®­ç»ƒè¶Šç¨³å®š
# - é…ç½®å‚æ•°ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œè„šæœ¬ä¼šä¸¥æ ¼æŒ‰é…ç½®æ‰§è¡Œï¼Œä¸ä¼šåŠ¨æ€è°ƒæ•´
# - å¦‚æœé‡åˆ°å†…å­˜ä¸è¶³ï¼Œä½¿ç”¨ --force-cpu å‚æ•°å¼ºåˆ¶CPUè®­ç»ƒ
# - âœ… æ”¯æŒMPSï¼ˆApple Siliconï¼Œ18GBå¯ç”¨ï¼‰ã€CUDAï¼ˆNVIDIAï¼‰ã€CPUè®¾å¤‡
#
# ã€ä½¿ç”¨æ–¹æ³•ã€‘
# ```bash
# # åŸºæœ¬è®­ç»ƒ
# python 5_model_1_sft.py --config-id 1
#
# # è®­ç»ƒå¹¶åˆå¹¶æ¨¡å‹
# python 5_model_1_sft.py --config-id 3 --merge
# ```
#
# ã€è¾“å‡ºæ–‡ä»¶ç»“æ„ã€‘
# ```
# output/
# â”œâ”€â”€ config_0/
# â”‚   â””â”€â”€ checkpoints/             # LoRAæƒé‡checkpoints
# â”‚       â”œâ”€â”€ checkpoint-1/        # ä¿å­˜ç‚¹1
# â”‚       â”œâ”€â”€ checkpoint-2/        # ä¿å­˜ç‚¹2ï¼ˆä¿ç•™æœ€è¿‘2ä¸ªï¼‰
# â”‚       â””â”€â”€ trainer_state.json   # è®­ç»ƒçŠ¶æ€
# â”œâ”€â”€ config_1/
# â”‚   â””â”€â”€ checkpoints/
# â”œâ”€â”€ config_0_training_loss.png   # Lossæ›²çº¿
# â””â”€â”€ config_1_training_loss.png
# ```
#
# ã€ç›¸å…³è„šæœ¬ã€‘
# - `5_model_2_eval.py`: è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹æ€§èƒ½ï¼ˆå«åŸºå‡†æµ‹è¯•ï¼‰
# - `chatbot/sft_utils.py`: å…±ç”¨çš„å·¥å…·å‡½æ•°æ¨¡å—
# ==============================================================================

import os
import argparse
import torch
import glob
import shutil
import json
from datasets import load_dataset as hf_load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
)
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from chatbot.sft_utils import detect_device, plot_training_curves


def disable_mps_if_needed():
    """åœ¨force_cpuæ¨¡å¼ä¸‹å®Œå…¨ç¦ç”¨MPS"""
    if torch.backends.mps.is_available():
        # è®¾ç½®ç¯å¢ƒå˜é‡ç¦ç”¨MPS
        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
        # ç›´æ¥è®¾ç½®torchä¸ä½¿ç”¨MPS
        torch.set_default_device('cpu')
        print("   â†’ MPSå·²ç¦ç”¨ï¼Œæ‰€æœ‰æ“ä½œå°†åœ¨CPUä¸Šè¿›è¡Œ")


# ==============================================================================
# é¢„è®¾é…ç½®ï¼š5ç§å†…ç½®é…ç½®æ–¹æ¡ˆ
# ==============================================================================
PRESET_CONFIGS = {
    0: {
        "name": "è¿‡å¤§å­¦ä¹ ç‡æµ‹è¯•",
        "description": "æ•…æ„è®¾ç½®è¿‡å¤§çš„å­¦ä¹ ç‡ï¼ˆ0.1ï¼‰ï¼Œè§‚å¯Ÿè®­ç»ƒä¸ç¨³å®šã€losséœ‡è¡æˆ–å‘æ•£ç°è±¡",
        "learning_rate": 0.1,
        "lora_rank": 4,
        "num_train_epochs": 1,
        "train_file": "./resources/train_100.jsonl",
        "batch_size": 8,
        "gradient_accumulation_steps": 2,
        "max_length": 512,
        "save_steps": 1,
    },
    1: {
        "name": "å¿«é€ŸéªŒè¯ï¼ˆæ¨èï¼‰",
        "description": "ä½¿ç”¨æ ‡å‡†é…ç½®å¿«é€ŸéªŒè¯æµç¨‹ï¼Œå·²å¯ç”¨å†…å­˜ä¼˜åŒ–ï¼ˆGradient Checkpointingï¼‰",
        "learning_rate": 5e-5,
        "lora_rank": 4,
        "num_train_epochs": 1,
        "train_file": "./resources/train_100.jsonl",
        "batch_size": 2,  # é™ä½batch_sizeä»¥é€‚åº”å†…å­˜é™åˆ¶
        "gradient_accumulation_steps": 8,  # å¢åŠ ç´¯ç§¯æ­¥æ•°ï¼Œä¿æŒæœ‰æ•ˆbatch_size=16
        "max_length": 512,
        "save_steps": 1,
    },
    2: {
        "name": "å°æ•°æ®é›†é•¿è®­ç»ƒ",
        "description": "åœ¨å°æ•°æ®é›†ä¸Šè¿›è¡Œé•¿æ—¶é—´è®­ç»ƒï¼ˆ50è½®ï¼‰ï¼Œè§‚å¯Ÿè¿‡æ‹Ÿåˆç°è±¡",
        "learning_rate": 5e-5,
        "lora_rank": 4,
        "num_train_epochs": 50,
        "train_file": "./resources/train_100.jsonl",
        "batch_size": 4,
        "gradient_accumulation_steps": 4,
        "max_length": 512,
        "save_steps": 20,
    },
    3: {
        "name": "å¤§æ•°æ®é›†æ ‡å‡†è®­ç»ƒ",
        "description": "ä½¿ç”¨1kæ ·æœ¬è¿›è¡Œæ ‡å‡†3è½®è®­ç»ƒï¼Œè¾ƒå¥½çš„æ€§èƒ½/æ—¶é—´å¹³è¡¡",
        "learning_rate": 5e-5,
        "lora_rank": 8,
        "num_train_epochs": 3,
        "train_file": "./resources/train_1k.jsonl",
        "batch_size": 4,
        "gradient_accumulation_steps": 4,
        "max_length": 512,
        "save_steps": 20,
    },
    4: {
        "name": "å¤§æ•°æ®é›†é•¿è®­ç»ƒ",
        "description": "ä½¿ç”¨1kæ ·æœ¬è¿›è¡Œ15è½®é•¿è®­ç»ƒï¼Œè¿½æ±‚æœ€ä½³æ€§èƒ½ï¼ˆè€—æ—¶è¾ƒé•¿ï¼‰",
        "learning_rate": 5e-5,
        "lora_rank": 8,
        "num_train_epochs": 15,
        "train_file": "./resources/train_1k.jsonl",
        "batch_size": 4,
        "gradient_accumulation_steps": 4,
        "max_length": 512,
        "save_steps": 20,
    },
}


def load_and_prepare_dataset(train_file, split_ratio=0.01):
    """
    åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†ã€‚
    
    ã€å‚æ•°ã€‘
    - train_file: è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSONLæ ¼å¼ï¼‰
    - split_ratio: éªŒè¯é›†åˆ†å‰²æ¯”ä¾‹
    
    ã€è¿”å›ã€‘
    - train_dataset: è®­ç»ƒæ•°æ®é›†
    - eval_dataset: éªŒè¯æ•°æ®é›†
    """
    # è¯»å–JSONLæ–‡ä»¶
    data = []
    with open(train_file, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    
    # è½¬æ¢ä¸ºHuggingFace Datasetæ ¼å¼
    dataset = hf_load_dataset('json', data_files={'train': train_file}, split='train')
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    if split_ratio > 0:
        split_dataset = dataset.train_test_split(test_size=split_ratio, seed=42)
        train_dataset = split_dataset['train']
        eval_dataset = split_dataset['test']
    else:
        train_dataset = dataset
        eval_dataset = None
    
    return train_dataset, eval_dataset


def tokenize_function(examples, tokenizer, max_length=512):
    """
    æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼šå°†å¯¹è¯æ ¼å¼è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥ã€‚
    
    ã€å‚æ•°ã€‘
    - examples: æ•°æ®æ ·æœ¬
    - tokenizer: tokenizerå®ä¾‹
    - max_length: æœ€å¤§åºåˆ—é•¿åº¦
    
    ã€è¿”å›ã€‘
    - å¤„ç†åçš„æ•°æ®å­—å…¸
    """
    # æå–messageså¹¶æ ¼å¼åŒ–
    model_inputs = {"input_ids": [], "attention_mask": [], "labels": []}
    
    for messages in examples['messages']:
        # æ„å»ºå®Œæ•´å¯¹è¯
        # messagesæ ¼å¼ï¼š[{role: system, content: ...}, {role: user, content: ...}, {role: assistant, content: ...}]
        
        # è·³è¿‡systemæ¶ˆæ¯ï¼Œåªå¤„ç†userå’Œassistant
        user_msg = next((m['content'] for m in messages if m['role'] == 'user'), '')
        assistant_msg = next((m['content'] for m in messages if m['role'] == 'assistant'), '')
        
        # ä½¿ç”¨chat templateæ ¼å¼åŒ–ï¼ˆå¦‚æœtokenizeræ”¯æŒï¼‰
        if hasattr(tokenizer, 'apply_chat_template'):
            # ä½¿ç”¨æ ‡å‡†chat template
            formatted_messages = [
                {"role": "user", "content": user_msg},
                {"role": "assistant", "content": assistant_msg}
            ]
            text = tokenizer.apply_chat_template(
                formatted_messages,
                tokenize=False,
                add_generation_prompt=False
            )
        else:
            # ç®€å•æ‹¼æ¥
            text = f"User: {user_msg}\nAssistant: {assistant_msg}"
        
        # Tokenize
        tokenized = tokenizer(
            text,
            max_length=max_length,
            truncation=True,
            padding=False,  # åŠ¨æ€paddingç”±DataCollatorå¤„ç†
        )
        
        model_inputs["input_ids"].append(tokenized["input_ids"])
        model_inputs["attention_mask"].append(tokenized["attention_mask"])
        model_inputs["labels"].append(tokenized["input_ids"].copy())  # labelsä¸input_idsç›¸åŒ
    
    return model_inputs


def run_fine_tune(config_id, model_path="./model", device=None):
    """
    è¿è¡Œ LoRA å¾®è°ƒå®éªŒã€‚
    
    ã€åŠŸèƒ½ã€‘
    æ ¹æ®é…ç½®IDè‡ªåŠ¨é€‰æ‹©é¢„è®¾å‚æ•°è¿›è¡Œå¾®è°ƒï¼Œè¾“å‡ºæ–‡ä»¶æŒ‰é…ç½®IDåŒºåˆ†ã€‚
    
    ã€å‚æ•°ã€‘
    - config_id: é…ç½®IDï¼ˆ0-4ï¼‰
    - model_path: åŸºåº§æ¨¡å‹è·¯å¾„
    - device: è®­ç»ƒè®¾å¤‡ï¼ˆNone åˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰
    
    ã€è¿”å›ã€‘
    - checkpoint_path: å¾®è°ƒåçš„ checkpoint è·¯å¾„
    - loss_curve_path: Loss æ›²çº¿å›¾ç‰‡è·¯å¾„
    """
    if config_id not in PRESET_CONFIGS:
        print(f"âŒ é”™è¯¯ï¼šæ— æ•ˆçš„é…ç½®ID {config_id}ï¼Œè¯·é€‰æ‹© 0-4 ä¹‹é—´çš„æ•´æ•°")
        return None, None
    
    config = PRESET_CONFIGS[config_id]
    
    print("\n" + "ğŸ“ " + "="*76 + " ğŸ“")
    print("    æœ¬åœ°æ¨¡å‹ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰")
    print(f"    é…ç½®ID: {config_id} - {config['name']}")
    print("ğŸ“ " + "="*76 + " ğŸ“")
    
    if not os.path.exists(config['train_file']):
        print(f"âŒ é”™è¯¯ï¼šè®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨äº {config['train_file']}")
        return None, None
    
    # æ£€æµ‹è®¾å¤‡
    if device is None:
        device, device_name = detect_device()
    else:
        if device.type == "mps":
            device_name = "MPS (Apple Silicon GPU)"
        elif device.type == "cuda":
            device_name = f"CUDA ({torch.cuda.get_device_name(0)})"
        else:
            device_name = "CPU"
    
    # ä¸¥æ ¼ä½¿ç”¨é…ç½®ä¸­çš„ batch_size å’Œ gradient_accumulation_stepsï¼Œä¸åšåŠ¨æ€è°ƒæ•´
    batch_size = config['batch_size']
    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 4)  # é»˜è®¤å€¼4
    effective_batch_size = batch_size * gradient_accumulation_steps
    
    print(f"\nğŸ“ é…ç½®è¯´æ˜: {config['description']}")
    print(f"\nğŸ“Š è®­ç»ƒå‚æ•°:")
    print(f"   - é…ç½®ID: {config_id}")
    print(f"   - é…ç½®åç§°: {config['name']}")
    print(f"   - è®¾å¤‡: {device_name}")
    print(f"   - å­¦ä¹ ç‡: {config['learning_rate']}")
    print(f"   - LoRA Rank: {config['lora_rank']}")
    print(f"   - è®­ç»ƒè½®æ•°: {config['num_train_epochs']}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
    print(f"   - æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size} (batch Ã— accumulation)")
    print(f"   - æœ€å¤§é•¿åº¦: {config['max_length']}")
    print(f"   - ä¿å­˜é—´éš”: {config['save_steps']} steps")
    print(f"   - è®­ç»ƒæ•°æ®: {config['train_file']}")
    print(f"   - å†…å­˜ä¼˜åŒ–: å·²å¯ç”¨ Gradient Checkpointing + ç¦ç”¨è®­ç»ƒä¸­è¯„ä¼°")
    
    # è®¾å¤‡ç›¸å…³æç¤º
    if device.type == "cpu":
        print("\nâš ï¸  æ³¨æ„ï¼šCPU ç¯å¢ƒä¸‹è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢")
        if batch_size > 4:
            print(f"ğŸ’¡ æç¤ºï¼šå½“å‰ batch_size={batch_size} å¯èƒ½å¯¼è‡´å†…å­˜ä¸è¶³")
            print(f"   å»ºè®®ï¼šé€‰æ‹©é…ç½®0æˆ–1ï¼ˆbatch_size=8ï¼‰æˆ–åœ¨å‡ºç°å†…å­˜é”™è¯¯æ—¶é™ä½é…ç½®")
    elif device.type == "mps":
        print("\nâœ… ä½¿ç”¨ Apple Silicon GPU (MPS) åŠ é€Ÿ")
        print(f"   LoRAå†…å­˜å ç”¨ï¼š~{50 if config['lora_rank'] == 4 else 100}MBï¼Œå®Œå…¨å¯ç”¨")
        if batch_size > 8:
            print(f"ğŸ’¡ æç¤ºï¼šå½“å‰ batch_size={batch_size}ï¼Œé¢„è®¡æ€»å†…å­˜å ç”¨~5-6GB")
    else:
        print("\nâœ… ä½¿ç”¨ NVIDIA GPU åŠ é€Ÿ")
    
    # é…ç½®è¾“å‡ºç›®å½•
    config_dir = f"./output/config_{config_id}"
    output_dir = os.path.join(config_dir, "checkpoints")
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {config_dir}")
    print("="*80)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    print("="*80)
    
    try:
        # ========== æ­¥éª¤1ï¼šåŠ è½½æ¨¡å‹å’Œtokenizer ==========
        print("\n[1/5] åŠ è½½æ¨¡å‹å’Œtokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        # è®¾ç½®pad_tokenï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if device.type in ["cuda", "mps"] else torch.float32,
            device_map={"": device},  # ç›´æ¥åŠ è½½åˆ°æŒ‡å®šè®¾å¤‡
            trust_remote_code=True,
        )
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ˆè®¾å¤‡: {device}ï¼‰")
        
        # ğŸ”¥ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰ä»¥èŠ‚çœå†…å­˜
        # åŸç†ï¼šä¸ä¿å­˜æ‰€æœ‰ä¸­é—´æ¿€æ´»å€¼ï¼Œåå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—
        # æ•ˆæœï¼šèŠ‚çœ ~70% æ¿€æ´»å€¼å†…å­˜ï¼Œè®­ç»ƒé€Ÿåº¦ä¸‹é™ 10-20%
        # ä¸å½±å“è®­ç»ƒæ•ˆæœï¼Œåªæ˜¯ç”¨æ—¶é—´æ¢ç©ºé—´
        model.gradient_checkpointing_enable()
        print("âš¡ å·²å¯ç”¨ Gradient Checkpointingï¼ˆå†…å­˜ä¼˜åŒ–ï¼Œä¸å½±å“è®­ç»ƒæ•ˆæœï¼‰")
        
        # ========== æ­¥éª¤2ï¼šé…ç½®LoRA ==========
        print("\n[2/5] é…ç½®LoRA...")
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=config['lora_rank'],  # LoRAç§©
            lora_alpha=config['lora_rank'] * 2,  # é€šå¸¸è®¾ç½®ä¸ºrankçš„2å€
            lora_dropout=0.05,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            bias="none",
        )
        
        # åº”ç”¨LoRA
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()  # æ‰“å°å¯è®­ç»ƒå‚æ•°æ•°é‡
        
        # ========== æ­¥éª¤3ï¼šåŠ è½½å’Œå‡†å¤‡æ•°æ®é›† ==========
        print("\n[3/5] åŠ è½½æ•°æ®é›†...")
        train_dataset, eval_dataset = load_and_prepare_dataset(
            config['train_file'],
            split_ratio=0  # ä¸åˆ†å‰²éªŒè¯é›†ï¼ˆç¦ç”¨è®­ç»ƒä¸­è¯„ä¼°ä»¥èŠ‚çœå†…å­˜ï¼‰
        )
        
        print(f"âœ… è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        if eval_dataset:
            print(f"âœ… éªŒè¯æ ·æœ¬: {len(eval_dataset)}")
        else:
            print(f"â„¹ï¸  æ— éªŒè¯é›†ï¼ˆå·²ç¦ç”¨è®­ç»ƒä¸­è¯„ä¼°ä»¥èŠ‚çœå†…å­˜ï¼‰")
        
        # æ•°æ®é¢„å¤„ç†
        print("   é¢„å¤„ç†æ•°æ®...")
        tokenize_fn = lambda examples: tokenize_function(examples, tokenizer, config['max_length'])
        
        train_dataset = train_dataset.map(
            tokenize_fn,
            batched=True,
            remove_columns=train_dataset.column_names,
            desc="Tokenizing train dataset"
        )
        
        if eval_dataset:
            eval_dataset = eval_dataset.map(
                tokenize_fn,
                batched=True,
                remove_columns=eval_dataset.column_names,
                desc="Tokenizing eval dataset"
            )
        
        # ========== æ­¥éª¤4ï¼šé…ç½®Trainer ==========
        print("\n[4/5] é…ç½®è®­ç»ƒå™¨...")
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=config['num_train_epochs'],
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=config['learning_rate'],
            warmup_ratio=0.1,
            weight_decay=0.01,
            logging_steps=config['save_steps'],  # ä¸ save_steps ä¿æŒä¸€è‡´ï¼Œç¡®ä¿è®°å½•æ›´å¤šè®­ç»ƒ loss
            save_steps=config['save_steps'],
            save_strategy="steps",
            save_total_limit=1,  # åªä¿ç•™æœ€ç»ˆcheckpointï¼ŒèŠ‚çœç£ç›˜å’Œå†…å­˜
            # ğŸ”¥ å†…å­˜ä¼˜åŒ–ï¼šç¦ç”¨è®­ç»ƒä¸­è¯„ä¼°ï¼ˆèŠ‚çœ 2-3 GB å³°å€¼å†…å­˜ï¼‰
            eval_strategy="no",               # è®­ç»ƒæ—¶ä¸è¯„ä¼°ï¼ˆè®­ç»ƒåç”¨ eval.py å•ç‹¬è¯„ä¼°ï¼‰
            load_best_model_at_end=False,     # ä¸ä¿å­˜æœ€ä½³æ¨¡å‹å‰¯æœ¬
            fp16=device.type == "cuda",  # CUDAä½¿ç”¨fp16
            # MPSæš‚ä¸æ”¯æŒfp16è®­ç»ƒï¼Œä½¿ç”¨fp32
            report_to=[],  # ä¸ä¸ŠæŠ¥åˆ°wandbç­‰
            remove_unused_columns=False,
            # å†…å­˜ä¼˜åŒ–å‚æ•°ï¼ˆä¸å½±å“è®­ç»ƒæ•ˆæœï¼‰
            dataloader_num_workers=0,  # ä¸ä½¿ç”¨å¤šè¿›ç¨‹åŠ è½½æ•°æ®ï¼ŒèŠ‚çœå†…å­˜
            dataloader_pin_memory=False,  # ä¸å›ºå®šå†…å­˜ï¼Œå‡å°‘RAMå ç”¨
            gradient_checkpointing=True,  # é…åˆæ¨¡å‹çš„gradient_checkpointing_enable
        )
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=tokenizer,
            model=model,
            padding=True,
        )
        
        # åˆ›å»ºTrainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
        )
        
        # ========== æ­¥éª¤5ï¼šå¼€å§‹è®­ç»ƒ ==========
        print("\n[5/5] å¼€å§‹è®­ç»ƒ...")
        print("="*80)
        
        # è®­ç»ƒå‰æ¸…ç†ç¼“å­˜ï¼ˆMPS/CUDAï¼‰
        if device.type == "mps":
            torch.mps.empty_cache()
            print("   â†’ å·²æ¸…ç† MPS ç¼“å­˜")
        elif device.type == "cuda":
            torch.cuda.empty_cache()
            print("   â†’ å·²æ¸…ç† CUDA ç¼“å­˜")
        
        train_result = trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        trainer.save_model()
        trainer.save_state()
        
        print("\nâœ… å¾®è°ƒå®Œæˆï¼")
        
        # æŸ¥æ‰¾æœ€ä½³checkpoint
        checkpoint_dirs = glob.glob(os.path.join(output_dir, "checkpoint-*"))
        if checkpoint_dirs:
            latest_checkpoint = max(checkpoint_dirs, key=os.path.getmtime)
            checkpoint_path = latest_checkpoint
        else:
            checkpoint_path = output_dir
        
        print(f"   Checkpoint: {checkpoint_path}")
        
        # ç»˜åˆ¶ loss æ›²çº¿
        loss_curve_path = f"./output/config_{config_id}_training_loss.png"
        plot_training_curves(checkpoint_path, output_dir="./output", 
                            output_filename=f"config_{config_id}_training_loss.png")
        print(f"   Loss æ›²çº¿: {loss_curve_path}")
        
        return checkpoint_path, loss_curve_path
            
    except Exception as e:
        print(f"\nâŒ å¾®è°ƒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None


def main():
    """
    ä¸»å‡½æ•°ï¼šè§£æå‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œå¾®è°ƒã€‚
    """
    parser = argparse.ArgumentParser(
        description="æœ¬åœ°æ¨¡å‹ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰- åŸºäº ms-swift + LoRA",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
é¢„è®¾é…ç½®è¯´æ˜ï¼š
  ID 0: è¿‡å¤§å­¦ä¹ ç‡æµ‹è¯• - è§‚å¯Ÿè®­ç»ƒä¸ç¨³å®š/å‘æ•£ç°è±¡
  ID 1: å¿«é€ŸéªŒè¯ï¼ˆæ¨èï¼‰- é¦–æ¬¡è¿è¡Œï¼Œ5-10åˆ†é’Ÿå®Œæˆ
  ID 2: å°æ•°æ®é›†é•¿è®­ç»ƒ - è§‚å¯Ÿè¿‡æ‹Ÿåˆç°è±¡
  ID 3: å¤§æ•°æ®é›†æ ‡å‡†è®­ç»ƒ - æ€§èƒ½/æ—¶é—´å¹³è¡¡ï¼Œæ¨è
  ID 4: å¤§æ•°æ®é›†é•¿è®­ç»ƒ - è¿½æ±‚æœ€ä½³æ€§èƒ½ï¼Œè€—æ—¶è¾ƒé•¿

ä½¿ç”¨ç¤ºä¾‹ï¼š
  # é…ç½®1ï¼šå¿«é€ŸéªŒè¯ï¼ˆæ¨èé¦–æ¬¡è¿è¡Œï¼‰
  python 5_model_1_sft.py --config-id 1

  # é…ç½®3ï¼šå¤§æ•°æ®é›†æ ‡å‡†è®­ç»ƒ
  python 5_model_1_sft.py --config-id 3
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument("--config-id", type=int, required=True,
                       help="é…ç½®IDï¼ˆ0-4ï¼‰ï¼Œæ¯ä¸ªIDå¯¹åº”ä¸€ç»„é¢„è®¾çš„è®­ç»ƒå‚æ•°")
    
    # å¯é€‰å‚æ•°
    parser.add_argument("--model-path", type=str, default="./model",
                       help="åŸºåº§æ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤: ./modelï¼‰")
    parser.add_argument("--force-cpu", action="store_true",
                       help="å¼ºåˆ¶ä½¿ç”¨CPUè®­ç»ƒï¼ˆå½“MPSå†…å­˜ä¸è¶³æ—¶ä½¿ç”¨ï¼‰")
    
    args = parser.parse_args()
    
    # éªŒè¯é…ç½®ID
    if args.config_id not in PRESET_CONFIGS:
        print(f"\nâŒ é”™è¯¯ï¼šæ— æ•ˆçš„é…ç½®ID {args.config_id}")
        print("è¯·é€‰æ‹© 0-4 ä¹‹é—´çš„æ•´æ•°ï¼š")
        for cid, cfg in PRESET_CONFIGS.items():
            print(f"  {cid}: {cfg['name']}")
        return
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.model_path):
        print(f"\nâŒ é”™è¯¯ï¼šæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨äº {args.model_path}")
        print("è¯·å…ˆä¸‹è½½æ¨¡å‹ï¼š")
        print(f"  mkdir -p {args.model_path}")
        print(f"  modelscope download --model qwen/Qwen2.5-1.5B-Instruct --local_dir '{args.model_path}'")
        return
    
    # æ£€æµ‹æˆ–å¼ºåˆ¶æŒ‡å®šè®¾å¤‡
    if args.force_cpu:
        device = torch.device("cpu")
        print("\nâš ï¸  å¼ºåˆ¶ä½¿ç”¨ CPU è®­ç»ƒæ¨¡å¼")
        print("   ï¼ˆé€‚ç”¨äº MPS å†…å­˜ä¸è¶³çš„åœºæ™¯ï¼‰")
        disable_mps_if_needed()  # å®Œå…¨ç¦ç”¨MPS
    else:
        device = None  # è‡ªåŠ¨æ£€æµ‹
    
    # è¿è¡Œå¾®è°ƒ
    checkpoint_path, loss_curve_path = run_fine_tune(
        config_id=args.config_id,
        model_path=args.model_path,
        device=device
    )
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "ğŸ† " + "="*76 + " ğŸ†")
    print("    è®­ç»ƒå®Œæˆ")
    print("ğŸ† " + "="*76 + " ğŸ†")
    
    if checkpoint_path:
        print(f"\nâœ… é…ç½® {args.config_id} è®­ç»ƒæˆåŠŸï¼")
        print(f"ğŸ“ LoRA Checkpoint: {checkpoint_path}")
        if loss_curve_path:
            print(f"ğŸ“ˆ Loss æ›²çº¿: {loss_curve_path}")
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œï¼š")
        print(f"   # è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹")
        print(f"   python 5_model_2_eval.py --config-id {args.config_id}")
        print(f"\n   # åˆå¹¶ LoRA æƒé‡åˆ°åŸºåº§æ¨¡å‹")
        print(f"   python 5_model_3_merge.py --config-id {args.config_id}")
    else:
        print(f"\nâŒ é…ç½® {args.config_id} è®­ç»ƒå¤±è´¥")
    
    print("\n" + "="*80)
    print("ğŸ‰ æµç¨‹ç»“æŸ")
    print("="*80)


if __name__ == "__main__":
    main()
