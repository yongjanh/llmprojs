# ==============================================================================
# è¯´æ˜ï¼šSFTï¼ˆç›‘ç£å¾®è°ƒï¼‰å…±ç”¨å·¥å…·å‡½æ•°
# ==============================================================================
# æœ¬æ¨¡å—æä¾›å¾®è°ƒå’Œè¯„æµ‹è¿‡ç¨‹ä¸­å…±ç”¨çš„å·¥å…·å‡½æ•°ï¼ŒåŒ…æ‹¬ï¼š
# - è®¾å¤‡æ£€æµ‹ï¼ˆMPS/CUDA/CPUï¼‰
# - æ¨¡å‹åˆå§‹åŒ–
# - å•æ¬¡æ¨ç†
# - åŸºå‡†æµ‹è¯•
# - è®­ç»ƒæ›²çº¿å¯è§†åŒ–
# ==============================================================================

import os
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # æ— ç•Œé¢æ¨¡å¼ï¼Œé€‚åˆæœåŠ¡å™¨å’Œè„šæœ¬è¿è¡Œ


def detect_device():
    """
    è‡ªåŠ¨æ£€æµ‹æœ€ä½³å¯ç”¨è®¾å¤‡ã€‚
    
    ã€åŠŸèƒ½ã€‘
    æŒ‰ä¼˜å…ˆçº§æ£€æµ‹ï¼šMPS (Mac GPU) > CUDA (NVIDIA GPU) > CPU
    
    ã€è¿”å›ã€‘
    - device: torch.device å¯¹è±¡
    - device_name: è®¾å¤‡åç§°å­—ç¬¦ä¸²ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
    """
    if torch.cuda.is_available():
        device = torch.device("cuda")
        device_name = f"CUDA (NVIDIA GPU: {torch.cuda.get_device_name(0)})"
    elif torch.backends.mps.is_available():
        # Apple Silicon (M1/M2/M3) çš„ Metal Performance Shaders
        device = torch.device("mps")
        device_name = "MPS (Apple Silicon GPU)"
    else:
        device = torch.device("cpu")
        device_name = "CPU"
    
    return device, device_name


def initialize_model(model_path="./model"):
    """
    åˆå§‹åŒ– Qwen2.5-1.5B-Instruct åŸºåº§æ¨¡å‹ã€‚
    
    ã€åŠŸèƒ½ã€‘
    åŠ è½½æœ¬åœ°æ¨¡å‹å’Œ tokenizerï¼Œè‡ªåŠ¨æ£€æµ‹å¹¶é…ç½®æœ€ä½³è®¾å¤‡ï¼ˆMPS/CUDA/CPUï¼‰ã€‚
    åŸºäº transformers åº“ï¼Œé€‚é… PEFT æ¡†æ¶ã€‚
    
    ã€å‚æ•°ã€‘
    - model_path: æœ¬åœ°æ¨¡å‹æ–‡ä»¶è·¯å¾„
    
    ã€è¿”å›ã€‘
    - llm: åŠ è½½çš„è¯­è¨€æ¨¡å‹
    - tokenizer: å¯¹åº”çš„ tokenizer
    - template: Noneï¼ˆä¸ºäº†å…¼å®¹æ€§ä¿ç•™ï¼ŒPEFT ä¸éœ€è¦ templateï¼‰
    - device: ä½¿ç”¨çš„è®¾å¤‡
    """
    print("\n" + "="*80)
    print("ğŸ“¦ æ¨¡å‹åˆå§‹åŒ–")
    print("="*80)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯ï¼šæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨äº {model_path}")
        print("è¯·å…ˆä¸‹è½½æ¨¡å‹ï¼š")
        print(f"  mkdir -p {model_path}")
        print(f"  modelscope download --model qwen/Qwen2.5-1.5B-Instruct --local_dir '{model_path}'")
        return None, None, None, None
    
    # æ£€æµ‹æœ€ä½³è®¾å¤‡
    device, device_name = detect_device()
    print(f"ğŸ” æ£€æµ‹åˆ°è®¾å¤‡: {device_name}")
    
    print(f"ğŸ” æ­£åœ¨ä» {model_path} åŠ è½½æ¨¡å‹...")
    
    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹
    llm = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16 if device.type in ["cuda", "mps"] else torch.float32,
        device_map={"": device},
        trust_remote_code=True,
    )
    llm.eval()  # è¯„ä¼°æ¨¡å¼
    
    # é…ç½®ç”Ÿæˆå‚æ•°
    llm.generation_config.max_new_tokens = 256
    llm.generation_config.do_sample = False  # è´ªå¿ƒè§£ç ï¼Œç»“æœç¡®å®šæ€§
    
    print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    print(f"   - æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"   - è®¾å¤‡: {device_name}")
    print(f"   - æœ€å¤§ç”Ÿæˆé•¿åº¦: {llm.generation_config.max_new_tokens} tokens")
    
    if device.type == "mps":
        print(f"   - ğŸ’¡ æç¤º: ä½¿ç”¨ Apple Silicon GPU åŠ é€Ÿï¼Œæ¨ç†é€Ÿåº¦æ¯” CPU å¿« 5-10 å€")
    
    return llm, tokenizer, None, device  # template è¿”å› Noneï¼ˆPEFT ä¸éœ€è¦ï¼‰


def run_single_query(llm, tokenizer, template, query):
    """
    è¿è¡Œå•ä¸ªæŸ¥è¯¢çš„æ¨ç†ã€‚
    
    ã€åŠŸèƒ½ã€‘
    ä½¿ç”¨æ¨¡å‹å¯¹å•ä¸ªé—®é¢˜è¿›è¡Œæ¨ç†ï¼Œå¹¶è¿”å›å›ç­”ã€‚
    ä½¿ç”¨ transformers æ ‡å‡† APIï¼Œå…¼å®¹ PEFT æ¡†æ¶ã€‚
    
    ã€å‚æ•°ã€‘
    - llm: è¯­è¨€æ¨¡å‹
    - tokenizer: tokenizer
    - template: æ ¼å¼åŒ–æ¨¡æ¿ï¼ˆå…¼å®¹æ€§å‚æ•°ï¼Œå®é™…ä¸ä½¿ç”¨ï¼‰
    - query: ç”¨æˆ·é—®é¢˜
    
    ã€è¿”å›ã€‘
    - response: æ¨¡å‹çš„å›ç­”æ–‡æœ¬
    """
    # æ„å»º chat æ ¼å¼çš„è¾“å…¥
    messages = [{'role': 'user', 'content': query}]
    
    # ä½¿ç”¨ tokenizer.apply_chat_template æ„å»ºè¾“å…¥ï¼ˆQwen2.5 åŸç”Ÿæ”¯æŒï¼‰
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # Tokenize
    inputs = tokenizer(text, return_tensors="pt")
    
    # æ¨ç†
    with torch.no_grad():
        output_ids = llm.generate(
            input_ids=inputs['input_ids'].to(llm.device),
            attention_mask=inputs['attention_mask'].to(llm.device),
            max_new_tokens=llm.generation_config.max_new_tokens,
            do_sample=False,  # è´ªå¿ƒè§£ç ï¼Œç»“æœæ›´ç¨³å®š
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # è§£ç è¾“å‡ºï¼ˆåªå–ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
    input_len = inputs['input_ids'].shape[1]  # æ³¨æ„æ˜¯ shape[1]ï¼ˆåºåˆ—é•¿åº¦ï¼‰
    response_ids = output_ids[0][input_len:]
    response = tokenizer.decode(response_ids, skip_special_tokens=True)
    
    return response


def score_answer(response, ans, ans_core, support_partial=True):
    """
    ç»Ÿä¸€çš„ç­”æ¡ˆè¯„åˆ†å‡½æ•°ã€‚
    
    ã€åŠŸèƒ½ã€‘
    å¯¹æ¨¡å‹å›ç­”è¿›è¡Œè¯„åˆ†ï¼Œæ”¯æŒå®Œå…¨æ­£ç¡®ã€éƒ¨åˆ†æ­£ç¡®ã€é”™è¯¯ä¸‰çº§è¯„åˆ†ã€‚
    
    ã€è¯„åˆ†è§„åˆ™ã€‘
    - å®Œå…¨æ­£ç¡®ï¼šå®Œæ•´ç­”æ¡ˆï¼ˆå«æ ¼å¼ï¼‰åœ¨å›ç­”ä¸­ â†’ 1.0 åˆ†
    - éƒ¨åˆ†æ­£ç¡®ï¼šæ ¸å¿ƒç­”æ¡ˆåœ¨å›ç­”ä¸­ä½†æ ¼å¼ä¸å¯¹ â†’ 0.5 åˆ†ï¼ˆå¦‚æœæ”¯æŒï¼‰
    - é”™è¯¯ï¼šç­”æ¡ˆä¸åœ¨å›ç­”ä¸­ â†’ 0.0 åˆ†
    
    ã€å‚æ•°ã€‘
    - response: æ¨¡å‹çš„å›ç­”æ–‡æœ¬
    - ans: å®Œæ•´æ ‡å‡†ç­”æ¡ˆï¼ˆå«æ ¼å¼ï¼Œå¦‚ "{ans: \"42\"}"ï¼‰
    - ans_core: æ ¸å¿ƒç­”æ¡ˆï¼ˆä¸å«æ ¼å¼ï¼Œå¦‚ "42"ï¼‰
    - support_partial: æ˜¯å¦æ”¯æŒéƒ¨åˆ†æ­£ç¡®è¯„åˆ†ï¼ˆé»˜è®¤Trueï¼‰
    
    ã€è¿”å›ã€‘
    - score: åˆ†æ•°ï¼ˆ1.0 / 0.5 / 0.0ï¼‰
    - label: è¯„åˆ†æ ‡ç­¾ï¼ˆ"å®Œå…¨æ­£ç¡®" / "éƒ¨åˆ†æ­£ç¡®" / "é”™è¯¯"ï¼‰
    """
    if ans in response:
        return 1.0, "âœ… å®Œå…¨æ­£ç¡®"
    elif support_partial and ans_core in response:
        return 0.5, "âš ï¸  éƒ¨åˆ†æ­£ç¡®ï¼ˆæ ¼å¼æœ‰è¯¯ï¼‰"
    else:
        return 0.0, "âŒ é”™è¯¯"


def run_model_evaluation(
    model, 
    tokenizer, 
    template, 
    test_file="./resources/test.jsonl",
    max_samples=None,
    support_partial=True,
    model_name="æ¨¡å‹"
):
    """
    é€šç”¨çš„æ¨¡å‹è¯„ä¼°å‡½æ•°ï¼Œé€‚ç”¨äºåŸºåº§æ¨¡å‹ã€LoRAæ¨¡å‹ã€åˆå¹¶æ¨¡å‹ã€‚
    
    ã€åŠŸèƒ½ã€‘
    ç»Ÿä¸€å¤„ç†æ‰€æœ‰ç±»å‹æ¨¡å‹çš„è¯„ä¼°ï¼Œç¡®ä¿è¯„åˆ†é€»è¾‘å®Œå…¨ä¸€è‡´ã€‚
    
    ã€æµ‹è¯•æ•°æ®æ ¼å¼ã€‘
    æ¯è¡Œæ˜¯ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ï¼š
    - messages[1].content: ç”¨æˆ·é—®é¢˜ï¼ˆæ ¼å¼ï¼š#æ•°å­¦é¢˜#\n{å…·ä½“é—®é¢˜}ï¼‰
    - messages[2].content: æ ‡å‡†ç­”æ¡ˆï¼ˆæ ¼å¼ï¼š...{ans: "ç­”æ¡ˆ"}...ï¼‰
    
    ã€è¯„åˆ†è§„åˆ™ã€‘
    ä½¿ç”¨ç»Ÿä¸€çš„ score_answer() å‡½æ•°ï¼š
    - å®Œå…¨æ­£ç¡®ï¼š+1.0 åˆ†
    - éƒ¨åˆ†æ­£ç¡®ï¼š+0.5 åˆ†ï¼ˆå¦‚æœ support_partial=Trueï¼‰
    - é”™è¯¯ï¼š0 åˆ†
    
    ã€å‚æ•°ã€‘
    - model: ä»»æ„æ¨¡å‹ï¼ˆåŸºåº§/LoRA/åˆå¹¶ï¼‰
    - tokenizer: tokenizer
    - template: æ ¼å¼åŒ–æ¨¡æ¿
    - test_file: æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„
    - max_samples: æœ€å¤šæµ‹è¯•çš„æ ·æœ¬æ•°ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰
    - support_partial: æ˜¯å¦æ”¯æŒéƒ¨åˆ†æ­£ç¡®è¯„åˆ†ï¼ˆé»˜è®¤Trueï¼‰
    - model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
    
    ã€è¿”å›ã€‘
    - accuracy: åŠ æƒå‡†ç¡®ç‡ï¼ˆ0-100ï¼‰
    - results: è¯¦ç»†ç»“æœåˆ—è¡¨ [{"question": str, "answer": str, "response": str, 
                                "score": float, "label": str}]
    """
    print("\n" + "="*80)
    print(f"ğŸ“Š {model_name}è¯„ä¼°")
    print("="*80)
    
    if not os.path.exists(test_file):
        print(f"âŒ é”™è¯¯ï¼šæµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨äº {test_file}")
        return 0.0, []
    
    print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file}")
    if max_samples:
        print(f"ğŸ“ æœ€å¤šæµ‹è¯•æ ·æœ¬æ•°: {max_samples}")
    
    total_score = 0.0
    total_count = 0
    results = []
    
    # ç»Ÿè®¡æ•°é‡
    full_correct = 0  # å®Œå…¨æ­£ç¡®æ•°
    partial_correct = 0  # éƒ¨åˆ†æ­£ç¡®æ•°
    
    with open(test_file, 'r', encoding='utf-8') as f:
        for line in f:
            if max_samples and total_count >= max_samples:
                break
                
            # è§£ææµ‹è¯•æ ·æœ¬
            math_question = json.loads(line)
            query = math_question["messages"][1]["content"]
            
            # æå–é—®é¢˜æ–‡æœ¬ï¼ˆå»æ‰ #æ•°å­¦é¢˜# æ ‡è®°ï¼‰
            if "#æ•°å­¦é¢˜#\n" in query:
                question_text = query.split("#æ•°å­¦é¢˜#\n")[1]
            else:
                question_text = query
            
            # æ¨¡å‹æ¨ç†
            response = run_single_query(model, tokenizer, template, query)
            
            # æå–æ ‡å‡†ç­”æ¡ˆ
            ans_full = math_question["messages"][2]["content"]
            pos = ans_full.find("ans")
            if pos != -1:
                end_pos = ans_full[pos:].find('}}')
                ans = ans_full[pos - 2: end_pos + pos + 2]  # æå– {ans: "xxx"} æ ¼å¼
                ans_core = ans[6:-2]  # æå– xxx éƒ¨åˆ†
            else:
                ans = ans_full
                ans_core = ans_full
            
            # ç»Ÿä¸€è¯„åˆ†
            score, label = score_answer(response, ans, ans_core, support_partial)
            total_score += score
            total_count += 1
            
            # ç»Ÿè®¡
            if score == 1.0:
                full_correct += 1
            elif score == 0.5:
                partial_correct += 1
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            results.append({
                "question": question_text,
                "answer": ans_core,
                "response": response,
                "score": score,
                "label": label
            })
            
            # æ‰“å°è¯¦ç»†ç»“æœ
            print(f"\n{'='*80}")
            print(f"é—®é¢˜ {total_count}/{max_samples if max_samples else '?'}: {question_text}")
            print(f"æ ‡å‡†ç­”æ¡ˆ: {ans_core}")
            print(f"----- {model_name}å›ç­” -----")
            print(response)
            print(f"----- è¯„åˆ† -----")
            print(label)
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = 100.0 * total_score / total_count if total_count > 0 else 0.0
    
    print("\n" + "="*80)
    print(f"ğŸ¯ {model_name}æµ‹è¯•ç»“æœï¼š")
    print(f"   - å®Œå…¨æ­£ç¡®: {full_correct}/{total_count} ({100.0 * full_correct / total_count:.1f}%)")
    if support_partial:
        print(f"   - éƒ¨åˆ†æ­£ç¡®: {partial_correct}/{total_count} ({100.0 * partial_correct / total_count:.1f}%)")
        print(f"   - åŠ æƒå‡†ç¡®ç‡: {accuracy:.1f}%")
    else:
        print(f"   - å‡†ç¡®ç‡: {accuracy:.1f}%")
    print("="*80)
    
    return accuracy, results


def run_benchmark_test(llm, tokenizer, template, test_file="./resources/test.jsonl", max_samples=None):
    """
    åŸºå‡†æµ‹è¯•ï¼ˆå…¼å®¹æ€§åŒ…è£…å‡½æ•°ï¼‰ã€‚
    
    ã€åŠŸèƒ½ã€‘
    ä¸ºäº†ä¿æŒå‘åå…¼å®¹ï¼ŒåŒ…è£… run_model_evaluation å‡½æ•°ã€‚
    
    ã€æ³¨æ„ã€‘
    å»ºè®®ç›´æ¥ä½¿ç”¨ run_model_evaluation() ä»¥è·å¾—æ›´è¯¦ç»†çš„ç»“æœã€‚
    
    ã€å‚æ•°ã€‘
    - llm: è¯­è¨€æ¨¡å‹
    - tokenizer: tokenizer
    - template: æ ¼å¼åŒ–æ¨¡æ¿
    - test_file: æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„
    - max_samples: æœ€å¤šæµ‹è¯•çš„æ ·æœ¬æ•°ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰
    
    ã€è¿”å›ã€‘
    - accuracy: å‡†ç¡®ç‡ï¼ˆ0-100ï¼‰
    """
    accuracy, _ = run_model_evaluation(
        llm, tokenizer, template,
        test_file=test_file,
        max_samples=max_samples,
        support_partial=True,
        model_name="åŸºåº§æ¨¡å‹"
    )
    return accuracy


def plot_training_curves(checkpoint_path, output_dir="./output", output_filename="training_loss_curve.png"):
    """
    ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„ loss æ›²çº¿ã€‚
    
    ã€åŠŸèƒ½ã€‘
    ä» checkpoint ç›®å½•ä¸­è¯»å–è®­ç»ƒæ—¥å¿—ï¼Œç»˜åˆ¶è®­ç»ƒ loss å’Œè¯„ä¼° loss çš„å˜åŒ–è¶‹åŠ¿ã€‚
    
    ã€å‚æ•°ã€‘
    - checkpoint_path: checkpoint è·¯å¾„
    - output_dir: å›¾è¡¨ä¿å­˜ç›®å½•
    - output_filename: è¾“å‡ºæ–‡ä»¶åï¼ˆé»˜è®¤: training_loss_curve.pngï¼‰
    
    ã€è¿”å›ã€‘
    - success: æ˜¯å¦æˆåŠŸç»˜åˆ¶
    """
    print("\n" + "="*80)
    print("ğŸ“ˆ ç”Ÿæˆè®­ç»ƒ Loss æ›²çº¿")
    print("="*80)
    
    try:
        # æŸ¥æ‰¾ trainer_state.json æ–‡ä»¶
        trainer_state_path = os.path.join(checkpoint_path, "trainer_state.json")
        if not os.path.exists(trainer_state_path):
            # å°è¯•åœ¨çˆ¶ç›®å½•æŸ¥æ‰¾
            parent_dir = os.path.dirname(checkpoint_path)
            trainer_state_path = os.path.join(parent_dir, "trainer_state.json")
            
        if not os.path.exists(trainer_state_path):
            print(f"âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒæ—¥å¿—æ–‡ä»¶: trainer_state.json")
            return False
        
        # è¯»å–è®­ç»ƒæ—¥å¿—
        with open(trainer_state_path, 'r') as f:
            trainer_state = json.load(f)
        
        # æå– loss æ•°æ®
        log_history = trainer_state.get('log_history', [])
        if not log_history:
            print(f"âš ï¸  è®­ç»ƒæ—¥å¿—ä¸ºç©º")
            return False
        
        # åˆ†ç¦»è®­ç»ƒå’Œè¯„ä¼°æ•°æ®
        train_steps = []
        train_losses = []
        eval_steps = []
        eval_losses = []
        final_train_loss = None
        final_step = 0
        
        for entry in log_history:
            # ä¸­é—´æ­¥éª¤çš„è®­ç»ƒ lossï¼ˆç”± logging_steps æ§åˆ¶ï¼‰
            if 'loss' in entry and 'train_loss' not in entry:
                train_steps.append(entry.get('step', 0))
                train_losses.append(entry['loss'])
            # è¯„ä¼° loss
            if 'eval_loss' in entry:
                eval_steps.append(entry.get('step', 0))
                eval_losses.append(entry['eval_loss'])
            # æœ€ç»ˆè®­ç»ƒ lossï¼ˆè®­ç»ƒç»“æŸæ—¶çš„å¹³å‡å€¼ï¼‰
            if 'train_loss' in entry:
                final_train_loss = entry['train_loss']
                final_step = entry.get('step', 0)
        
        if not train_losses and not eval_losses:
            print(f"âš ï¸  æœªæ‰¾åˆ° loss æ•°æ®")
            return False
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆMac ä½¿ç”¨ Arial Unicode MSï¼‰
        if os.uname().sysname == 'Darwin':  # macOS
            plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
        else:
            plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå›¾è¡¨
        fig, ax = plt.subplots(figsize=(12, 6))
        
        # ç»˜åˆ¶è®­ç»ƒ loss
        if train_losses:
            ax.plot(train_steps, train_losses, label='è®­ç»ƒ Loss (ä¸­é—´æ­¥éª¤)', 
                   color='#1f77b4', linewidth=2, marker='o', markersize=6)
        
        # æ·»åŠ æœ€ç»ˆè®­ç»ƒ loss çš„æ ‡æ³¨ï¼ˆæ•´ä½“å¹³å‡å€¼ï¼‰
        if final_train_loss is not None:
            ax.scatter([final_step], [final_train_loss], 
                      color='#d62728', marker='*', s=200, zorder=5,
                      label=f'æœ€ç»ˆè®­ç»ƒ Loss (å¹³å‡)', edgecolors='black', linewidths=1)
        
        # ç»˜åˆ¶è¯„ä¼° loss
        if eval_losses:
            ax.plot(eval_steps, eval_losses, label='è¯„ä¼° Loss', 
                   color='#ff7f0e', linewidth=2, marker='s', markersize=4)
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        ax.set_xlabel('è®­ç»ƒæ­¥æ•° (Steps)', fontsize=12)
        ax.set_ylabel('Loss', fontsize=12)
        ax.set_title('è®­ç»ƒè¿‡ç¨‹ Loss å˜åŒ–æ›²çº¿', fontsize=14, fontweight='bold')
        ax.legend(fontsize=11)
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ ä¿¡æ¯æ–‡æœ¬
        info_text_lines = []
        if final_train_loss is not None:
            info_text_lines.append(f'æœ€ç»ˆè®­ç»ƒ Loss (å¹³å‡): {final_train_loss:.4f}')
        if eval_losses:
            info_text_lines.append(f'æœ€ç»ˆè¯„ä¼° Loss: {eval_losses[-1]:.4f}')
        
        if info_text_lines:
            info_text = '\n'.join(info_text_lines)
            ax.text(0.02, 0.98, info_text,
                   transform=ax.transAxes, fontsize=10,
                   verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        os.makedirs(output_dir, exist_ok=True)
        plot_filename = os.path.join(output_dir, output_filename)
        plt.savefig(plot_filename, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… Loss æ›²çº¿å·²ä¿å­˜è‡³: {os.path.abspath(plot_filename)}")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
        if train_losses:
            print(f"   - è®­ç»ƒ Loss (ä¸­é—´): {len(train_losses)} ä¸ªè®°å½•ç‚¹")
            print(f"     ç¬¬ä¸€ä¸ª: {train_losses[0]:.4f} (step {train_steps[0]})")
            print(f"     æœ€åä¸€ä¸ª: {train_losses[-1]:.4f} (step {train_steps[-1]})")
        if final_train_loss is not None:
            print(f"   - è®­ç»ƒ Loss (æœ€ç»ˆå¹³å‡): {final_train_loss:.4f}")
        if eval_losses:
            print(f"   - è¯„ä¼° Loss: {eval_losses[0]:.4f} â†’ {eval_losses[-1]:.4f} "
                  f"({len(eval_losses)} ä¸ªè®°å½•ç‚¹, ä¸‹é™ {(eval_losses[0] - eval_losses[-1]) / eval_losses[0] * 100:.1f}%)")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç»˜åˆ¶ Loss æ›²çº¿å¤±è´¥: {str(e)}")
        return False

