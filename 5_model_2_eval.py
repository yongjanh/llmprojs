# ==============================================================================
# è¯´æ˜ï¼šæ¨¡å‹è¯„æµ‹è„šæœ¬ - è¯„ä¼°åŸºåº§æ¨¡å‹å’Œå¾®è°ƒåæ¨¡å‹çš„æ€§èƒ½
# ==============================================================================
# æœ¬è„šæœ¬ç”¨äºè¯„ä¼°ä¸åŒæ¨¡å‹åœ¨æ•°å­¦é—®ç­”ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ï¼Œæ”¯æŒï¼š
# 1. åŸºåº§æ¨¡å‹è¯„ä¼°ï¼šæµ‹è¯•åŸå§‹ Qwen2.5-1.5B-Instruct æ¨¡å‹çš„æ€§èƒ½
# 2. LoRA æ¨¡å‹è¯„ä¼°ï¼šåŠ è½½ LoRA checkpointï¼Œæµ‹è¯•å¾®è°ƒåçš„æ€§èƒ½
# 3. åˆå¹¶æ¨¡å‹è¯„ä¼°ï¼šæµ‹è¯•åˆå¹¶äº† LoRA æƒé‡çš„å®Œæ•´æ¨¡å‹
# 4. å¯¹æ¯”åˆ†æï¼šå¯¹æ¯”å¾®è°ƒå‰åçš„æ€§èƒ½æå‡
#
# ã€è¯„æµ‹æµç¨‹ã€‘
# 1. åŠ è½½åŸºåº§æ¨¡å‹
# 2. åœ¨æµ‹è¯•é›†ä¸Šè¿è¡ŒåŸºåº§æ¨¡å‹æ¨ç†ï¼Œè®¡ç®—åŸºå‡†å‡†ç¡®ç‡
# 3. åŠ è½½å¾®è°ƒåæ¨¡å‹ï¼ˆLoRA æˆ–åˆå¹¶æ¨¡å‹ï¼‰
# 4. åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œå¾®è°ƒåæ¨¡å‹æ¨ç†ï¼Œè®¡ç®—å¾®è°ƒåå‡†ç¡®ç‡
# 5. å¯¹æ¯”åˆ†ææ€§èƒ½æå‡
#
# ã€è¯„åˆ†è§„åˆ™ã€‘
# - å®Œå…¨æ­£ç¡®ï¼šç­”æ¡ˆå®Œæ•´åŒ¹é…ï¼ˆåŒ…æ‹¬æ ¼å¼ï¼‰ï¼Œ+1 åˆ†
# - éƒ¨åˆ†æ­£ç¡®ï¼šæ ¸å¿ƒæ•°å­—åŒ¹é…ä½†æ ¼å¼æœ‰è¯¯ï¼Œ+0.5 åˆ†
# - é”™è¯¯ï¼šç­”æ¡ˆä¸åŒ¹é…ï¼Œ0 åˆ†
#
# ã€é…ç½®IDè¯´æ˜ã€‘
# æœ¬è„šæœ¬æ”¯æŒè¯„ä¼°5ç§é¢„è®¾é…ç½®å¯¹åº”çš„å¾®è°ƒæ¨¡å‹ï¼ˆå·²é’ˆå¯¹A10 GPUä¼˜åŒ–ï¼‰ï¼š
#
# â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ ID â”‚ åç§°                â”‚ å­¦ä¹ ç‡  â”‚ Rank â”‚ Epoch â”‚ æ•°æ®é›†      â”‚ Batch â”‚ ç´¯ç§¯æ­¥æ•°â”‚æœ‰æ•ˆBatchâ”‚ éªŒè¯é›†  â”‚ è¯´æ˜                    â”‚
# â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ 0  â”‚ è¿‡å¤§å­¦ä¹ ç‡æµ‹è¯•      â”‚ 0.1     â”‚ 4    â”‚ 1     â”‚ train_100   â”‚ 8     â”‚ 2      â”‚ 16      â”‚ 10%     â”‚ è§‚å¯Ÿè®­ç»ƒä¸ç¨³å®š/å‘æ•£ç°è±¡ â”‚
# â”‚ 1  â”‚ å¿«é€ŸéªŒè¯ï¼ˆæ¨èï¼‰âœ¨  â”‚ 5e-5    â”‚ 4    â”‚ 1     â”‚ train_100   â”‚ 8     â”‚ 2      â”‚ 16      â”‚ 10%     â”‚ å¿«é€ŸéªŒè¯æµç¨‹ï¼Œ1åˆ†é’Ÿå®Œæˆ â”‚
# â”‚ 2  â”‚ å°æ•°æ®é›†é•¿è®­ç»ƒ      â”‚ 5e-5    â”‚ 4    â”‚ 50    â”‚ train_100   â”‚ 8     â”‚ 2      â”‚ 16      â”‚ 10%     â”‚ è§‚å¯Ÿè¿‡æ‹Ÿåˆç°è±¡          â”‚
# â”‚ 3  â”‚ å¤§æ•°æ®é›†æ ‡å‡†è®­ç»ƒ    â”‚ 5e-5    â”‚ 8    â”‚ 3     â”‚ train_1k    â”‚ 8     â”‚ 2      â”‚ 16      â”‚ 5%      â”‚ æ€§èƒ½/æ—¶é—´å¹³è¡¡ï¼Œæ¨è     â”‚
# â”‚ 4  â”‚ å¤§æ•°æ®é›†é•¿è®­ç»ƒ      â”‚ 5e-5    â”‚ 8    â”‚ 15    â”‚ train_1k    â”‚ 8     â”‚ 2      â”‚ 16      â”‚ 5%      â”‚ è¿½æ±‚æœ€ä½³æ€§èƒ½ï¼Œè€—æ—¶è¾ƒé•¿  â”‚
# â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# ã€è¯´æ˜ã€‘
# - æœ‰æ•ˆBatch = Batch Ã— ç´¯ç§¯æ­¥æ•°ï¼šæ‰€æœ‰é…ç½®ç»Ÿä¸€ä¸º 8Ã—2=16ï¼Œç¡®ä¿è®­ç»ƒç¨³å®šæ€§
# - éªŒè¯é›†ï¼šConfig 0-2 ä½¿ç”¨10%ï¼ŒConfig 3-4 ä½¿ç”¨5%ï¼ˆæ ·æœ¬é‡å……è¶³ï¼‰
# - GPUä¼˜åŒ–ï¼šfp16è®­ç»ƒ + Gradient Checkpointing + å¤šè¿›ç¨‹æ•°æ®åŠ è½½
# - Warmupç­–ç•¥ï¼šå›ºå®š100æ­¥ï¼ˆå¯¹é½åŸå§‹swifté…ç½®ï¼ŒConfig 3é¢„çƒ­å æ¯”å¤§â†’æ¬ æ‹Ÿåˆï¼‰
# - å­¦ä¹ ç‡è¡°å‡ï¼šCosineä½™å¼¦è¡°å‡ï¼ˆæ›´å¹³æ»‘ä¼˜é›…ï¼Œé˜²æ­¢åæœŸå­¦ä¹ ç‡è¿‡å¿«é™ä¸º0ï¼‰
# - æ­£åˆ™åŒ–ï¼šWeight Decay = 0.1ï¼ˆå¢å¼ºç‰ˆï¼Œé˜²æ­¢å°æ•°æ®é›†è¿‡æ‹Ÿåˆï¼‰
# - Checkpointä¿å­˜ï¼š
#   * best_model/ - éªŒè¯Lossæœ€ä½çš„æœ€ä½³æ¨¡å‹ï¼ˆè¯„ä¼°ä¼˜å…ˆä½¿ç”¨ï¼‰
#   * checkpoints/ - è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ‰€æœ‰checkpointï¼ˆ1è½®è®­ç»ƒå…¨ä¿ç•™ï¼Œå¤šè½®ä»…ä¿ç•™æœ€æ–°1ä¸ªï¼‰
# - è®­ç»ƒä¸­è¯„ä¼°ï¼šæ‰€æœ‰é…ç½®å¯ç”¨éªŒè¯Lossç›‘æ§ï¼Œè§‚å¯Ÿè¿‡æ‹Ÿåˆè¶‹åŠ¿
#
# ã€ä½¿ç”¨æ–¹æ³•ã€‘
# ```bash
# # è¯„ä¼°é…ç½®1çš„LoRAæ¨¡å‹ï¼ˆè‡ªåŠ¨æŸ¥æ‰¾checkpointï¼‰
# python 5_model_2_eval.py --config-id 1
#
# # è¯„ä¼°é…ç½®3çš„LoRAæ¨¡å‹
# python 5_model_2_eval.py --config-id 3
#
# # è¯„ä¼°åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
# python 5_model_2_eval.py --merged-model ./output/config_3/merged_model
#
# # æ‰‹åŠ¨æŒ‡å®šLoRA checkpointè·¯å¾„
# python 5_model_2_eval.py --checkpoint ./output/config_1/checkpoint
#
# # åªè¯„ä¼°åŸºåº§æ¨¡å‹ï¼ˆä¸åŠ è½½å¾®è°ƒcheckpointï¼‰
# python 5_model_2_eval.py --baseline-only
#
# # é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°
# python 5_model_2_eval.py --config-id 1 --max-samples 10
# ```
#
# ã€ç›¸å…³è„šæœ¬ã€‘
# - `5_model_1_sft.py`: è®­ç»ƒå’Œå¾®è°ƒè„šæœ¬
# - `chatbot/sft_utils.py`: å…±ç”¨çš„å·¥å…·å‡½æ•°æ¨¡å—
# ==============================================================================

import os
import json
import argparse
import torch
import glob
from peft import PeftModel
from chatbot.sft_utils import (
    initialize_model,
    run_model_evaluation
)


def evaluate_finetuned_model(
    base_llm, 
    tokenizer, 
    template, 
    checkpoint_path, 
    test_file="./resources/test.jsonl",
    max_samples=None
):
    """
    åŠ è½½å¾®è°ƒåçš„LoRAæ¨¡å‹å¹¶åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ã€‚
    
    ã€åŠŸèƒ½ã€‘
    1. ä» checkpoint åŠ è½½ LoRA æƒé‡
    2. å°† LoRA æƒé‡åº”ç”¨åˆ°åŸºåº§æ¨¡å‹
    3. ä½¿ç”¨ç»Ÿä¸€çš„è¯„ä¼°å‡½æ•°è®¡ç®—å‡†ç¡®ç‡
    
    ã€è¯„åˆ†è§„åˆ™ã€‘
    ä½¿ç”¨ chatbot.sft_utils.run_model_evaluation çš„ç»Ÿä¸€è¯„åˆ†ï¼š
    - å®Œå…¨æ­£ç¡®ï¼š+1.0 åˆ†
    - éƒ¨åˆ†æ­£ç¡®ï¼š+0.5 åˆ†
    - é”™è¯¯ï¼š0 åˆ†
    
    ã€å‚æ•°ã€‘
    - base_llm: åŸºåº§æ¨¡å‹
    - tokenizer: Tokenizer
    - template: æ ¼å¼åŒ–æ¨¡æ¿
    - checkpoint_path: LoRA checkpoint è·¯å¾„
    - test_file: æµ‹è¯•é›†æ–‡ä»¶
    - max_samples: æœ€å¤šæµ‹è¯•çš„æ ·æœ¬æ•°
    
    ã€è¿”å›ã€‘
    - accuracy: å‡†ç¡®ç‡ï¼ˆ0-100ï¼‰
    - finetuned_model: åŠ è½½äº† LoRA æƒé‡çš„æ¨¡å‹
    """
    print("\n" + "="*80)
    print("ğŸ“Š åŠ è½½ LoRA æ¨¡å‹")
    print("="*80)
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ é”™è¯¯ï¼šCheckpoint ä¸å­˜åœ¨äº {checkpoint_path}")
        return 0.0, None
    
    print(f"ğŸ” æ­£åœ¨åŠ è½½ LoRA æƒé‡: {checkpoint_path}")
    
    # åŠ è½½ LoRA æƒé‡
    try:
        finetuned_model = PeftModel.from_pretrained(base_llm, checkpoint_path)
        finetuned_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        print("âœ… LoRA æƒé‡åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {str(e)}")
        return 0.0, None
    
    # ä½¿ç”¨ç»Ÿä¸€çš„è¯„ä¼°å‡½æ•°
    accuracy, _ = run_model_evaluation(
        finetuned_model,
        tokenizer,
        template,
        test_file=test_file,
        max_samples=max_samples,
        support_partial=True,
        model_name="LoRAå¾®è°ƒæ¨¡å‹"
    )
    
    return accuracy, finetuned_model


def evaluate_merged_model(
    merged_model_path,
    test_file="./resources/test.jsonl",
    max_samples=None
):
    """
    è¯„ä¼°åˆå¹¶åçš„å®Œæ•´æ¨¡å‹ã€‚
    
    ã€åŠŸèƒ½ã€‘
    åŠ è½½åˆå¹¶äº† LoRA æƒé‡çš„å®Œæ•´æ¨¡å‹ï¼Œä½¿ç”¨ç»Ÿä¸€çš„è¯„ä¼°å‡½æ•°è¿›è¡Œè¯„ä¼°ã€‚
    
    ã€è¯„åˆ†è§„åˆ™ã€‘
    ä½¿ç”¨ chatbot.sft_utils.run_model_evaluation çš„ç»Ÿä¸€è¯„åˆ†ï¼š
    - å®Œå…¨æ­£ç¡®ï¼š+1.0 åˆ†
    - éƒ¨åˆ†æ­£ç¡®ï¼š+0.5 åˆ†
    - é”™è¯¯ï¼š0 åˆ†
    
    ã€å‚æ•°ã€‘
    - merged_model_path: åˆå¹¶åæ¨¡å‹çš„è·¯å¾„
    - test_file: æµ‹è¯•é›†æ–‡ä»¶
    - max_samples: æœ€å¤šæµ‹è¯•çš„æ ·æœ¬æ•°
    
    ã€è¿”å›ã€‘
    - accuracy: å‡†ç¡®ç‡ï¼ˆ0-100ï¼‰
    """
    print("\n" + "="*80)
    print("ğŸ“Š åŠ è½½åˆå¹¶æ¨¡å‹")
    print("="*80)
    
    if not os.path.exists(merged_model_path):
        print(f"âŒ é”™è¯¯ï¼šåˆå¹¶æ¨¡å‹ä¸å­˜åœ¨äº {merged_model_path}")
        return 0.0
    
    print(f"ğŸ” æ­£åœ¨åŠ è½½åˆå¹¶åçš„æ¨¡å‹: {merged_model_path}")
    
    # åŠ è½½åˆå¹¶åçš„æ¨¡å‹
    try:
        llm, tokenizer, template, device = initialize_model(model_path=merged_model_path)
        if llm is None:
            return 0.0
        print("âœ… åˆå¹¶æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {str(e)}")
        return 0.0
    
    # ä½¿ç”¨ç»Ÿä¸€çš„è¯„ä¼°å‡½æ•°
    accuracy, _ = run_model_evaluation(
        llm,
        tokenizer,
        template,
        test_file=test_file,
        max_samples=max_samples,
        support_partial=True,
        model_name="åˆå¹¶æ¨¡å‹"
    )
    
    return accuracy


def compare_results(baseline_accuracy, finetuned_accuracy, model_type="LoRA", config_id=None):
    """
    å¯¹æ¯”å¾®è°ƒå‰åçš„æ€§èƒ½ã€‚
    
    ã€å‚æ•°ã€‘
    - baseline_accuracy: åŸºåº§æ¨¡å‹å‡†ç¡®ç‡
    - finetuned_accuracy: å¾®è°ƒåæ¨¡å‹å‡†ç¡®ç‡
    - model_type: å¾®è°ƒæ¨¡å‹ç±»å‹ï¼ˆ"LoRA" æˆ– "Merged"ï¼‰
    - config_id: é…ç½®IDï¼ˆç”¨äºç‰¹æ®Šé…ç½®çš„è§£è¯»ï¼‰
    """
    print("\n" + "ğŸ† " + "="*76 + " ğŸ†")
    print("    å¾®è°ƒæ•ˆæœå¯¹æ¯”åˆ†æ")
    print("ğŸ† " + "="*76 + " ğŸ†")
    
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print(f"   - å¾®è°ƒå‰ï¼ˆåŸºåº§æ¨¡å‹ï¼‰å‡†ç¡®ç‡: {baseline_accuracy:.1f}%")
    print(f"   - å¾®è°ƒåï¼ˆ{model_type}æ¨¡å‹ï¼‰å‡†ç¡®ç‡: {finetuned_accuracy:.1f}%")
    
    if finetuned_accuracy > baseline_accuracy:
        improvement = finetuned_accuracy - baseline_accuracy
        improvement_rate = (improvement / baseline_accuracy * 100) if baseline_accuracy > 0 else 0
        print(f"   - âœ… æ€§èƒ½æå‡: +{improvement:.1f}% (ç›¸å¯¹æå‡ {improvement_rate:.1f}%)")
    elif finetuned_accuracy < baseline_accuracy:
        degradation = baseline_accuracy - finetuned_accuracy
        print(f"   - âš ï¸  æ€§èƒ½ä¸‹é™: -{degradation:.1f}%")
    else:
        print(f"   - â– æ€§èƒ½æŒå¹³")
    
    print("\nğŸ’¡ åˆ†æä¸å»ºè®®:")
    
    # ç‰¹æ®Šé…ç½®çš„è§£è¯»
    if config_id == 0:
        print("   ğŸ“– Config 0ï¼ˆè¿‡å¤§å­¦ä¹ ç‡æµ‹è¯•ï¼‰è§£è¯»ï¼š")
        if finetuned_accuracy < baseline_accuracy:
            print("      âœ… é¢„æœŸè¡Œä¸ºï¼šå­¦ä¹ ç‡è¿‡å¤§ï¼ˆ0.1ï¼‰å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œæ€§èƒ½ä¸‹é™æ­£å¸¸")
            print("      ğŸ’¡ å­¦ä¹ è¦ç‚¹ï¼šè§‚å¯Ÿè®­ç»ƒLossæ›²çº¿ï¼Œåº”è¯¥çœ‹åˆ°éœ‡è¡æˆ–å‘æ•£ç°è±¡")
            print("      ğŸ” æ£€æŸ¥Lossæ›²çº¿ï¼š./output/config_0_training_loss.png")
        else:
            print("      âš ï¸  éé¢„æœŸï¼šå­¦ä¹ ç‡è¿‡å¤§ä½†æ€§èƒ½æœªä¸‹é™ï¼Œå¯èƒ½åŸå› ï¼š")
            print("         - è®­ç»ƒæ­¥æ•°å¤ªå°‘ï¼Œè¿˜æœªå……åˆ†å±•ç°ä¸ç¨³å®šæ€§")
            print("         - æ•°æ®é›†è¿‡äºç®€å•")
        return
    elif config_id == 2:
        print("   ğŸ“– Config 2ï¼ˆå°æ•°æ®é›†é•¿è®­ç»ƒï¼‰è§£è¯»ï¼š")
        print("      ğŸ’¡ å­¦ä¹ è¦ç‚¹ï¼šè§‚å¯Ÿè®­ç»ƒLossä¸éªŒè¯Lossçš„åˆ†ç¦»ï¼Œç†è§£è¿‡æ‹Ÿåˆç°è±¡")
        print("      ğŸ” æ£€æŸ¥Lossæ›²çº¿ï¼š./output/config_2_training_loss.png")
        if finetuned_accuracy > baseline_accuracy:
            print("      âœ… è®­ç»ƒæœ‰æ•ˆï¼šåœ¨è®­ç»ƒé›†ä¸Šå­¦ä¹ åˆ°äº†çŸ¥è¯†")
            if finetuned_accuracy < baseline_accuracy + 10:
                print("      âš ï¸  æ³›åŒ–ä¸€èˆ¬ï¼šå¯èƒ½å·²è¿‡æ‹Ÿåˆï¼ŒéªŒè¯Lossåº”è¯¥å¼€å§‹ä¸Šå‡")
        return
    
    # å¸¸è§„é…ç½®çš„åˆ†æ
    if finetuned_accuracy > baseline_accuracy:
        print("   âœ… å¾®è°ƒæˆåŠŸï¼æ¨¡å‹æ€§èƒ½å¾—åˆ°æå‡ã€‚")
        if finetuned_accuracy < 80:
            print("   ğŸ“ˆ è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®ï¼š")
            print("      - å¢åŠ è®­ç»ƒæ•°æ®é‡ï¼ˆä½¿ç”¨ train_1k.jsonlï¼‰")
            print("      - å¢å¤§ LoRA rankï¼ˆä» 4 å¢åŠ åˆ° 8 æˆ– 16ï¼‰")
            print("      - å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆnum_train_epochsï¼‰")
            print("      - è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå»ºè®®èŒƒå›´ 1e-5 åˆ° 1e-4ï¼‰")
    elif finetuned_accuracy < baseline_accuracy:
        print("   âš ï¸  å¾®è°ƒæ•ˆæœä¸ç†æƒ³ï¼Œå¯èƒ½åŸå› ï¼š")
        print("      1. è®­ç»ƒæ•°æ®é‡ä¸è¶³æˆ–è´¨é‡è¾ƒå·®")
        print("      2. å­¦ä¹ ç‡è¿‡å¤§å¯¼è‡´è¿‡æ‹Ÿåˆ")
        print("      3. LoRA rank è¿‡å°ï¼Œè¡¨è¾¾èƒ½åŠ›ä¸è¶³")
        print("      4. è®­ç»ƒè½®æ•°ä¸å¤Ÿæˆ–è¿‡å¤š")
        print("   ğŸ”§ è°ƒä¼˜å»ºè®®ï¼š")
        print("      - æ£€æŸ¥è®­ç»ƒæ•°æ®è´¨é‡å’Œæ•°é‡")
        print("      - é™ä½å­¦ä¹ ç‡ï¼ˆå¦‚ä» 5e-5 é™åˆ° 1e-5ï¼‰")
        print("      - å¢å¤§ LoRA rankï¼ˆå¦‚ä» 4 å¢åˆ° 8ï¼‰")
        print("      - è°ƒæ•´è®­ç»ƒè½®æ•°ï¼ˆè§‚å¯Ÿ Loss æ›²çº¿ç¡®å®šæœ€ä½³å€¼ï¼‰")
    else:
        print("   â– å¾®è°ƒæ•ˆæœä¸æ˜æ˜¾ï¼Œå»ºè®®ï¼š")
        print("      - å¢åŠ è®­ç»ƒæ•°æ®é‡")
        print("      - è°ƒæ•´è¶…å‚æ•°ï¼ˆå­¦ä¹ ç‡ã€rankã€epochï¼‰")
        print("      - æ£€æŸ¥æ•°æ®åˆ†å¸ƒæ˜¯å¦ä¸æµ‹è¯•é›†ä¸€è‡´")


def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„è¯„æµ‹æµç¨‹ã€‚
    """
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description="è¯„ä¼°åŸºåº§æ¨¡å‹å’Œå¾®è°ƒåæ¨¡å‹çš„æ€§èƒ½",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹ï¼š
  # è¯„ä¼°é…ç½®1çš„LoRAæ¨¡å‹
  python 5_model_2_eval.py --config-id 1

  # è¯„ä¼°åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
  python 5_model_2_eval.py --merged-model ./output/config_3/merged_model

  # æ‰‹åŠ¨æŒ‡å®šLoRA checkpoint
  python 5_model_2_eval.py --checkpoint ./output/config_1/checkpoint

  # åªè¯„ä¼°åŸºåº§æ¨¡å‹
  python 5_model_2_eval.py --baseline-only --max-samples 10
        """
    )
    parser.add_argument("--config-id", type=int, default=None,
                       help="é…ç½®IDï¼ˆè‡ªåŠ¨æŸ¥æ‰¾å¯¹åº”çš„ checkpointï¼‰")
    parser.add_argument("--checkpoint", type=str, default=None,
                       help="LoRA checkpoint è·¯å¾„ï¼ˆæ‰‹åŠ¨æŒ‡å®šï¼‰")
    parser.add_argument("--merged-model", type=str, default=None,
                       help="åˆå¹¶åçš„å®Œæ•´æ¨¡å‹è·¯å¾„")
    parser.add_argument("--baseline-only", action="store_true",
                       help="åªè¯„ä¼°åŸºåº§æ¨¡å‹ï¼Œä¸è¯„ä¼°å¾®è°ƒåæ¨¡å‹")
    parser.add_argument("--test-file", type=str, default="./resources/test.jsonl",
                       help="æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--max-samples", type=int, default=None,
                       help="æœ€å¤šæµ‹è¯•çš„æ ·æœ¬æ•°ï¼ˆé»˜è®¤å…¨éƒ¨ï¼‰")
    parser.add_argument("--model-path", type=str, default="./model",
                       help="åŸºåº§æ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤: ./modelï¼‰")
    args = parser.parse_args()
    
    print("\n" + "ğŸ“Š " + "="*76 + " ğŸ“Š")
    print("    æ¨¡å‹è¯„æµ‹æµç¨‹")
    print("    åŸºäº Qwen2.5-1.5B-Instruct + LoRA")
    print("ğŸ“Š " + "="*76 + " ğŸ“Š")
    
    # ========== æ­¥éª¤ 1ï¼šåˆå§‹åŒ–åŸºåº§æ¨¡å‹ï¼ˆå¦‚æœä¸æ˜¯åªè¯„ä¼°åˆå¹¶æ¨¡å‹ï¼‰==========
    if args.merged_model is None:
        llm, tokenizer, template, device = initialize_model(model_path=args.model_path)
        if llm is None:
            print("\nâŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return
        
        # ========== æ­¥éª¤ 2ï¼šåŸºåº§æ¨¡å‹è¯„ä¼° ==========
        print("\n" + "="*80)
        print("ğŸ“Š æ­¥éª¤ 1ï¼šåŸºåº§æ¨¡å‹è¯„ä¼°ï¼ˆå¾®è°ƒå‰ï¼‰")
        print("="*80)
        baseline_accuracy, _ = run_model_evaluation(
            llm,
            tokenizer,
            template,
            test_file=args.test_file,
            max_samples=args.max_samples,
            support_partial=True,
            model_name="åŸºåº§æ¨¡å‹"
        )
    else:
        # å¦‚æœåªè¯„ä¼°åˆå¹¶æ¨¡å‹ï¼Œè·³è¿‡åŸºåº§æ¨¡å‹è¯„ä¼°
        baseline_accuracy = 0.0
        llm, tokenizer, template = None, None, None
    
    # å¦‚æœåªè¯„ä¼°åŸºåº§æ¨¡å‹ï¼Œç›´æ¥è¿”å›
    if args.baseline_only:
        print("\n" + "="*80)
        print("ğŸ‰ è¯„æµ‹å®Œæ¯•ï¼")
        print("="*80)
        return
    
    # ========== æ­¥éª¤ 3ï¼šç¡®å®šè¯„ä¼°ç›®æ ‡ ==========
    if args.merged_model:
        # è¯„ä¼°åˆå¹¶åçš„æ¨¡å‹
        print("\n" + "="*80)
        print("ğŸ“Š æ­¥éª¤ 2ï¼šåˆå¹¶æ¨¡å‹è¯„ä¼°")
        print("="*80)
        finetuned_accuracy = evaluate_merged_model(
            merged_model_path=args.merged_model,
            test_file=args.test_file,
            max_samples=args.max_samples
        )
        model_type = "Merged"
        model_path_display = args.merged_model
    else:
        # è¯„ä¼° LoRA æ¨¡å‹
        checkpoint_path = args.checkpoint
        
        if checkpoint_path is None:
            if args.config_id is None:
                # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„ checkpointï¼ˆæ‰€æœ‰é…ç½®ï¼‰
                all_checkpoints = glob.glob("./output/config_*/checkpoints/checkpoint-*")
                if not all_checkpoints:
                    print("\nâš ï¸  æœªæ‰¾åˆ°ä»»ä½• checkpoint")
                    print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python 5_model_1_sft.py --config-id <id>")
                    return
                
                checkpoint_path = max(all_checkpoints, key=os.path.getmtime)
                print(f"\nğŸ” æœªæŒ‡å®šé…ç½®IDï¼Œè‡ªåŠ¨é€‰æ‹©æœ€æ–° checkpoint: {checkpoint_path}")
            else:
                # æ ¹æ®é…ç½®IDæŸ¥æ‰¾checkpoint
                # ä¼˜å…ˆæŸ¥æ‰¾best_modelï¼ˆè®­ç»ƒæ—¶ä¿å­˜çš„æœ€ä½³æ¨¡å‹ï¼‰
                best_model_path = f"./output/config_{args.config_id}/best_model"
                checkpoint_base = f"./output/config_{args.config_id}/checkpoints"
                
                if os.path.exists(best_model_path):
                    # æ‰¾åˆ°best_modelï¼Œä¼˜å…ˆä½¿ç”¨
                    checkpoint_path = best_model_path
                    print(f"\nğŸ” é…ç½® {args.config_id} ä½¿ç”¨æœ€ä½³æ¨¡å‹ï¼ˆéªŒè¯Lossæœ€ä½ï¼‰: {checkpoint_path}")
                elif os.path.exists(checkpoint_base):
                    # æ²¡æœ‰best_modelï¼ŒæŸ¥æ‰¾checkpointsç›®å½•ä¸­çš„æœ€æ–°checkpoint
                    checkpoints = glob.glob(f"{checkpoint_base}/checkpoint-*")
                    if not checkpoints:
                        print(f"\nâš ï¸  é…ç½® {args.config_id} æ²¡æœ‰ä»»ä½• checkpoint")
                        print(f"ğŸ’¡ è¯·å…ˆè¿è¡Œ: python 5_model_1_sft.py --config-id {args.config_id}")
                        return
                    checkpoint_path = max(checkpoints, key=os.path.getmtime)
                    print(f"\nğŸ” é…ç½® {args.config_id} ä½¿ç”¨æœ€æ–°checkpoint: {checkpoint_path}")
                else:
                    print(f"\nâš ï¸  æœªæ‰¾åˆ°é…ç½® {args.config_id} çš„ä»»ä½•æ¨¡å‹")
                    print(f"ğŸ’¡ è¯·å…ˆè¿è¡Œ: python 5_model_1_sft.py --config-id {args.config_id}")
                    return
        else:
            if not os.path.exists(checkpoint_path):
                print(f"\nâŒ é”™è¯¯ï¼šæŒ‡å®šçš„ checkpoint ä¸å­˜åœ¨: {checkpoint_path}")
                return
            print(f"\nğŸ” ä½¿ç”¨æŒ‡å®šçš„ checkpoint: {checkpoint_path}")
        
        # ========== æ­¥éª¤ 4ï¼šLoRA æ¨¡å‹è¯„ä¼° ==========
        print("\n" + "="*80)
        print("ğŸ“Š æ­¥éª¤ 2ï¼šå¾®è°ƒåæ¨¡å‹è¯„ä¼°")
        print("="*80)
        finetuned_accuracy, finetuned_model = evaluate_finetuned_model(
            llm,
            tokenizer,
            template,
            checkpoint_path,
            test_file=args.test_file,
            max_samples=args.max_samples
        )
        model_type = "LoRA"
        model_path_display = checkpoint_path
    
    # ========== æ­¥éª¤ 5ï¼šå¯¹æ¯”åˆ†æ ==========
    if finetuned_accuracy > 0:
        if args.merged_model is None:  # åªæœ‰è¯„ä¼°LoRAæ¨¡å‹æ—¶æ‰è¿›è¡Œå¯¹æ¯”
            compare_results(baseline_accuracy, finetuned_accuracy, model_type, config_id=args.config_id)
        else:
            print(f"\nâœ… åˆå¹¶æ¨¡å‹è¯„ä¼°å®Œæˆï¼Œå‡†ç¡®ç‡: {finetuned_accuracy:.1f}%")
    
    print("\n" + "="*80)
    print("ğŸ‰ è¯„æµ‹å®Œæ¯•ï¼")
    print("="*80)
    
    # æ‰“å°ç›¸å…³æ–‡ä»¶ä½ç½®
    print(f"\nğŸ“ ç›¸å…³æ–‡ä»¶:")
    print(f"   - è¯„ä¼°æ¨¡å‹: {model_path_display}")
    
    # æŸ¥æ‰¾å¯¹åº”çš„ Loss æ›²çº¿
    if args.config_id is not None:
        loss_curve_path = f"./output/config_{args.config_id}_training_loss.png"
        if os.path.exists(loss_curve_path):
            print(f"   - Loss æ›²çº¿: {loss_curve_path}")


if __name__ == "__main__":
    main()
